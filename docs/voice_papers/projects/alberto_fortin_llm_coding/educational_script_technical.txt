FACTS AND DATA  
• The title of the content is "After months of coding with LLMs, I'm going back to using my brain."  
• TLDR statement in Spanish: "LLMs funcionan adecuadamente para tareas de codificación, pero a gran escala producen códigos desorganizados."  
• The previous SaaS infrastructure used PHP+MySQL and did not meet requirements, prompting a change.  
• The new infrastructure uses Go and Clickhouse.  
• The author has 15 years of experience in software engineering and studied C++ and Java.  
• Each day of delay in system readiness represents lost revenue due to potential customers waiting for the implementation.  

CONCEPTS AND DEFINITIONS  
• Infrastructure: The transition from PHP+MySQL to a solution based on Go and Clickhouse is defined by the requirements of the SaaS.  
• LLM (Language Learning Model): A tool used to generate code based on inputs of messages and contexts.  
• Cursor Notepads: The tool utilized to feed prompts to the LLM.  
• Benchmark usage: A metric used for evaluating performance.  
• The term “agent” versus a “cron job” is discussed in the context of the tool’s functionality.  
• Vibe coding: Describes the use of AI tools by users with little programming knowledge, leading to difficulties handling errors and confusing code.  
• “Magic shovel”: A term used by some influencers to describe an AI tool.  

DECLARATIONS AND STATEMENTS  
• The author claims that LLMs work adequately for coding tasks but, when scaled up, produce disorganized code.  
• The previous PHP+MySQL combination no longer met the requirements for the SaaS, which is why the author switched to Go and Clickhouse.  
• The author acted initially as a product manager consulting with Claude on best practices, performing independent research, and generating a plan from multiple exchanges.  
• A markdown file was generated detailing the existing infrastructure, the desired infrastructure, the objectives, and the reasons for the change.  
• The author states that the methodology consisted of entering messages and context into the LLM (using Cursor Notepads), receiving generated code, and then building and testing the resulting code.  
• The author emphasizes that implementation speed was prioritized over code cleanliness because clients needed specific data and potential customers were waiting to purchase a plan upon launch.  
• The author claims that the generated code presented problems such as:  
  – Two service files in the same directory with similar names but different methods.  
  – Inconsistencies in method and property names (e.g., one file using “WebAPIprovider” and another “webApi” for the same parameter).  
  – Multiple declarations of the same method across different files.  
  – Inconsistencies in the method of calling and retrieving configuration files.  
• The situation is compared to having 10 junior-to-mid level developers working simultaneously without access to Git and without coordination.  
• Recurrent errors occurred: error messages, when re-entered into the LLM, produced corrections that in turn caused other errors; and the LLM’s capacity to solve detailed problems decreased with increased complexity.  
• The author states that a manual “coding review” was performed to understand the generated code and correct inconsistencies.  
• The process of learning about Go and Clickhouse involved reading documentation and articles, watching YouTube videos (for example, about Clickhouse), and asking detailed questions to Claude while challenging his answers.  
• The change in approach included taking time to review and rewrite code sections that caused discomfort, and using LLMs only for simple tasks such as renaming code parameters and converting pseudocode to Go.  
• The author now plans functions and features using paper and pencil before employing LLMs, reserving the LLM’s role to that of an assistant and verifier.  
• The author expresses concern that overdependence on LLMs may lead to a loss of mental agility and an inability to plan and write organized code.  
• The author claims that users without programming knowledge who use AI tools for coding (vibe coding) face challenges dealing with errors and confusing code.  
• Limitations of current LLMs are noted in generating complex queries. For example:  
  – They are unable to generate a Clickhouse query for tables with over 100 million rows on a server with limited RAM even when provided with the SQL schema and documentation.  
  – Specific models mentioned include Gemini, o4-mini-high, o3, and Sonnet 3.7; none are able to solve the problem without generating errors.  
• The performance of the LLM is noted to be inconsistent; even with a supposedly perfect workflow, outcomes are unstable and break with slight changes or different requirements.  
• Despite recommended workflows by experts and influencers, certain tasks remain beyond the capabilities of the AI.  
• There is a conflict noted between the speed offered by AI and the need for meticulous code review to achieve consistent, functional results.  
• The author references tools and techniques such as “Cursor rules” and a “15-step workflow” found on Reddit that, though tested, do not resolve all challenges.  
• The author states that a “middle ground” is being adopted: drastically reducing the use of AI for creative or complex code generation and instead using it for simple tasks and verification, relying on personal experience and skills to write and organize code.  
• The author expresses enthusiasm for technology but warns that over-reliance on AI can negatively impact learning and the development of programming skills.  
• The author emphasizes that despite the advanced and promising appearance of technology, it has clear limitations in consistency, performance, and handling complex problems.  
• Direct textual expression from the content: "I long enough you’ll be able to relate. One day it’s amazing, the next day it’s incredibly stupid. Are they throttling the GPUs? Are these tools just impossible to control? What the fuck is going on?" This statement exemplifies the variable perceptions of AI performance and frustration with control issues.  

EXAMPLES AND CASES  
• Two service files in the same directory have similar names but perform different methods.  
• Inconsistencies exist in naming conventions between files, such as one file using “WebAPIprovider” and another “webApi” for the same parameter.  
• The same method is declared in multiple files.  
• There are inconsistencies in how configuration files are called and retrieved.  
• The LLM failed to generate a complex Clickhouse query for a table with more than 100 million rows on a low-RAM server despite receiving the SQL schema and corresponding documentation.  
• Specific models referenced that failed to handle complex query generation include Gemini, o4-mini-high, o3, and Sonnet 3.7.  
• Reports from AI-related subreddits describe completely opposite experiences using the same model with the same prompt on the same day: one report describes coding with AI as "amazing" on some days, while another describes it as "incredibly ineffective" on other days.  
• A final comparison is made between two approaches: one is the traditional method of planning manually ("walking") and the other is using AI ("a spaceship") that is fast but yields confusing, unstable results.  
• In the additional textual block, influencers are mentioned as promoting benchmarks and describing a tool as a “magic shovel,” and a group of companies is said to advocate that the tool works as an “agent” rather than a cron job.  
• Questions raised include whether GPU resources are being throttled and whether the AI tools are controllable.  

PROCESSES AND METHODS  
• The methodology involved:  
  – Entering messages and contextual details into the LLM via Cursor Notepads.  
  – Receiving code generated by the LLM.  
  – Building and testing the resulting code.  
• Priority was given to quick implementation because customers required specific data and potential clients were waiting for the product launch.  
• The code generation process led to issues like disorganization, inconsistent naming, and multiple declarations of methods, which were then addressed through manual code reviews.  
• The process of self-learning involved:  
  – Reading documentation and articles related to Go and Clickhouse.  
  – Watching YouTube videos (e.g., about Clickhouse).  
  – Asking detailed questions to Claude and challenging the responses.  
• The workflow eventually shifted to using LLMs only for simple tasks (e.g., renaming parameters, converting pseudocode to Go) while the author handled more complex coding tasks manually.  
• Tools and techniques such as "Cursor rules" and a "15-step workflow" from Reddit were referenced as practices that, although validated, did not overcome all the challenges encountered.  

RESULTS AND FINDINGS  
• Large-scale code generation with LLMs resulted in disorganized and inconsistent code.  
• Specific issues in the generated code included:  
  – Presence of two files with similar names but different methods, causing ambiguity.  
  – Inconsistent naming conventions among methods and properties.  
  – Redundant declarations of the same method in multiple files.  
  – Inconsistencies in handling configuration files.  
• The situation was compared to having 10 junior-to-mid level developers working simultaneously without proper coordination or version control (i.e., without Git).  
• Repeated error messages and subsequent LLM-based corrections led to a cycle of new errors.  
• As coding complexity increased, the LLM’s ability to resolve detailed problems diminished.  
• Manual code reviews allowed the identification and correction of these inconsistencies and errors.  
• The LLM’s performance varied greatly; even with an apparently perfect workflow, results were not sustained and broke with minor changes in requirements.  
• User reports on Reddit noted that the same prompt on the same day could result in outcomes described as either “amazing” or “incredibly ineffective.”  
• The use of AI has the advantage of speed but creates a need for extensive manual review to ensure consistency and functionality.  

HISTORICAL INFORMATION  
• The SaaS infrastructure was initially based on PHP+MySQL, which ultimately proved inadequate, leading to the decision to shift to Go and Clickhouse.  
• The migration was initiated by the author's involvement as a product manager, consulting with Claude and researching best practices through multiple exchanges.  
• A detailed markdown file was generated to document the existing setup, the desired changes, objectives, and reasons for making the switch.  
• The evolution of the workflow included initial heavy use of the LLM for generating code, followed by a gradual decision to limit its use to simpler tasks due to observed limitations.  
• The narrative includes a final comparison between traditional coding practices (planning manually) and the rapid but unstable results provided by the AI-driven approach.  
• Influencers and benchmarks promoted the idea of using AI as a flawless tool, contrasting with the author’s real-world experience of frequent errors and complexities.  

ADDITIONAL POINTS  
• Benchmark usage is employed as a metric for evaluating performance.  
• Influencers promote a tool described as a “magic shovel.”  
• A group of companies attempts to persuade users that the tool functions as an “agent” instead of as a cron job.  
• On AI-related subreddits, users report highly variable outcomes when using the same model with the same prompt on the same day – sometimes the AI is “amazing” and other times “incredibly ineffective.”  
• Questions are raised about whether GPU resources are being throttled and whether the AI tools are truly controllable.  
• The direct quote "I long enough you’ll be able to relate. One day it’s amazing, the next day it’s incredibly stupid. Are they throttling the GPUs? Are these tools just impossible to control? What the fuck is going on?" exemplifies these variable experiences and frustrations.