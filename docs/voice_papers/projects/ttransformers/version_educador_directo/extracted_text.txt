Gail Weiss1 Yoav Goldberg2 3 Eran Yahav1
What is the computational model behind a Trans-
former? Where recurrent neural networks have
direct parallels in ﬁnite state machines, allow-
ing clear discussion and thought around archi-
tecture variants or trained models, Transformers
have no such familiar parallel. In this paper we
aim to change that, proposing a computational
model for the transformer-encoder in the form
of a programming language. We map the basic
components of a transformer-encoder attention
and feed-forward computation into simple prim-
itives, around which we form a programming lan-
guage: the Restricted Access Sequence Process-
ing Language (RASP). We show how RASP can
conceivably be learned by a Transformer, and how
a Transformer can be trained to mimic a RASP so-
lution. In particular, we provide RASP programs
for histograms, sorting, and Dyck-languages. We
further use our model to relate their difﬁculty in
terms of the number of required layers and atten-
tion heads: analyzing a RASP program implies a
to encode a task in a transformer. Finally, we see
used to explain phenomena seen in recent works.
1. Introduction
We present a computational model for the transformer ar-
RASP (Restricted Access Sequence Processing Language).
Much as the token-by-token processing of RNNs can be
conceptualized as ﬁnite state automata , our language captures the unique information-ﬂow
constraints under which a transformer operates as it pro-
cesses input sequences. Our model helps reason about how
1Technion, Haifa, Israel 2Bar Ilan University, Ramat Gan,
Israel 3Allen Institute for AI. Correspondence to: Gail Weiss
sgailw@cs. technion. ac. il.
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).
a transformer operates at a higher-level of abstraction, rea-
rather than neural network primitives.
We are inspired by the use of automata as an abstract compu-
tational model for recurrent neural networks (RNNs). Using
of work, including extraction of automata from RNNs ,
analysis of RNNs practical expressive power in terms of au-
tomata , and even augmentations based
on automata variants (Joulin & Mikolov, 2015). Previous
work on transformers explores their computational power,
but does not provide a computational model .
Thinking in terms of the RASP model can help derive com-
putational results. Bhattamishra et al. and Ebrahimi
et al. explore the ability of transformers to recognize
Dyck-k languages, with Bhattamishra et al. providing a
construction by which Transformer-encoders can recognize
a simpliﬁed variant of Dyck-k. Using RASP, we succinctly
express the construction of as a
short program, and further improve it to show, for the ﬁrst
time, that transformers can fully recognize Dyck-kfor all k.
Scaling up the complexity, Clark et al. showed em-
multi-step logical reasoning over ﬁrst order logical formulas
provided as input, resulting in soft theorem provers. For
this task, the mechanism of the computation remained elu-
sive: how does a transformer perform even non-soft theorem
proving? As the famous saying by Richard Feynman goes,
what I cannot create, I do not understand: usingRASP, we
inferences over input expressions, and then compile it to
the transformer hardware, deﬁning a sequence of attention
and multi-layer perceptron (MLP) operations.
Considering computation problems and their implementa-
in favor of symbolic programs. Recognizing that a task
is representable in a transformer is as simple as ﬁnding a
RASP program for it, and communicating this solution
previously done by presenting a hand-crafted transformer
arXiv:2106.06981v2 [cs. LG] 19 Jul 2021
same_tok = select ( tokens, tokens,==);
hist = selector_width (
same_tok,
assume_bos = True );
first = not has_prev ( tokens );
same_count = select (hist, hist,==);
same_count_reprs = same_count and
select ( first, True,==);
hist2 = selector_width (
same_count_reprs,
assume_bos = True );
Figure: We consider double-histogram, the task of counting for each input token how many unique input tokens have the
same frequency as itself (e. g.: hist2(" aaabbccdef")=). (a) shows a RASP program for this
task, (b) shows the selection patterns of that same program, compiled to a transformer architecture and applied to the input
sequence aaabbccdef, (c) shows the corresponding attention heatmaps, for the same input sequence, in a 2-layer 2-head
transformer trained on double-histogram. This particular transformer was trained using both target and attention supervision,
i. e.: in addition to the standard cross entropy loss on the target output, the model was given an MSE-loss on the difference
between its attention heatmaps and those expected by the RASP solution. The transformer reached test accuracy of 99.9%
on the task, and comparing the selection patterns in (b) with the heatmaps in (c) suggests that it has also successfully learned
to replicate the solution described in (a).
for the task is now possible through a few lines of code.
recent empirical observation of transformer variants , and to ﬁnd concrete limitations of efﬁcient
transformers with restricted attention .
In Section, we show how a compiled RASP program can
indeed be realised in a neural transformer (as in Figure),
trained on the task using gradient descent (Figs 5 and 4).
Code We provide a RASP read-evaluate-print-loop (REPL)
in http: github. com tech-srl RASP, along with a
RASP cheat sheet and link to replication code for our work.
2. Overview
We begin with an informal overview ofRASP, with exam-
ples. The formal introduction is given in Section
Intuitively, transformers computations are applied to their
entire input in parallel, using attention to draw on and com-
their calculations . The iterative process of a trans-
rather the depth of the computation: the number of layers it
applies to its input as it works towards its ﬁnal result.
The computational model. Conceptually, a RASP com-
putation over length-ninput involves manipulation of se-
quences of length n, and matrices of size n n. There
computation. The abstract computation model is as follows:
The input of a RASP computation is two sequences, tokens
and indices. The ﬁrst contains the user-provided input, and
the second contains the range 0,1,..., n 1. The output of
a RASP computation is a sequence, and the consumer of the
output can choose to look only at speciﬁc output locations.
element-wise operations. For example, for the sequences
s1 = and s2 = , we can derive s1 + s2 =
, s1 +2 = , pow(s1,2) = , s1 2 =
[F, F, T ], pairwise_mul(s1, s2) = , and so on.
aggregate operations (Figure). Select operations take two
sequences k, q and a boolean predicatepover pairs of values,
and return a selection matrix Ssuch that for every i, j [n],
S[i][j] = p(k[i], q[j]). Aggregate operations take a matrix S
and a numeric sequence v, and return a sequence sin which
each position s[i] combines the values in vaccording to row
iin S(see full deﬁnition in Section).
Aggregate operations (over select matrices) are the only way
to combine values from different sequence positions, or to
s = select(,,==) res=aggregate(s, )
T F F 4 6 8 = 4 = 
Figure: Visualizing the select and aggregate operations.
On the left, a selection matrixs is computed by select, which
==. On the right, aggregate uses s as a ﬁlter over its input
values, averaging only the selected values at each position
in order to create its output, res. Where no values have been
selected, aggregate substitutes 0 in its output.
move values from one position to another. For example, to
perform the python computation: x = for _ in a],
we must ﬁrst use S= select(indices,0,=) to select the ﬁrst
position, and then x= aggregate(S, a) to broadcast it across
a new sequence of the same length.
RASP programs are lazy functional, and thus operate on
functions rather than sequences. That is, instead of a se-
quence indices= , we have a function indices that
returns on inputs of length 3. Similarly, s3=s1+s2
is a function, that when applied to an input xwill produce
the value s3(x), which will be computed as s1(x)+s2(x).
We call these functions s-ops (sequence operators). The
same is true for the selection matrices, whose functions we
refer to as selectors, and the RASP language is deﬁned in
terms of s-ops and selectors, not sequences and matrices.
However, the conceptual model to bear in mind is that of
operations over sequences and selection matrices.
Example: Double Histograms The RASP program
in Figure solves double-histogram, the task of count-
in the sequence have the same frequency as its own:
hist2(" aabcd")=. The program begins
by creating the the selector same_tok, in which each input
token as its own, and then applies the RASP operation
selector_width to it in order to obtain the s-op hist,
which computes the frequency of each token in the in-
put: hist("hello")=. Next, the program
uses the function has_prev1 to create the s-op first,
which marks the ﬁrst appearance of each token in a se-
quence: first("hello")=[T, T, T, F, T]. Finally, apply-
ing selector_width to the selector same_count_reprs,
which focuses each position on all ﬁrst tokens with the
same frequency as its own, provides hist2 as desired.
1Presented in Figure in Appendix.
def frac_prevs (sop, val )
prevs = select ( indices, indices, =);
return aggregate ( prevs,
indicator ( sop==val ));
def pair_balance (open, close )
opens = frac_prevs ( tokens, open );
closes = frac_prevs ( tokens, close );
return opens - closes;
bal1 = pair_balance ("(",") ");
bal2 = pair_balance (" "," ");
negative = bal1 0 or bal2 0;
had_neg = aggregate ( select_all,
indicator ( negative )) 0;
select_last = select ( indices, length -1, ==);
end_0 = aggregate ( select_last,
bal1==0 and bal2==0 );
shuffle_dyck2 = end_0 and not had_neg;
Figure: RASP program for the task shufﬂe-dyck-2 (bal-
ance 2 parenthesis pairs, independently of each other), cap-
turing a higher level representation of the hand-crafted trans-
former presented by Bhattamishra et al. .
Example: Shufﬂe-Dyck in RASP As an example of the
kind of tasks that are natural to encode usingRASP, consider
the Shufﬂe-Dyck language, in which multiple parentheses
types must be balanced but do not have to satisfy any or-
der with relation to each other. (For example, "([)]" is
considered balanced). In their work on transformer expres-
siveness, Bhattamishra et al. present a hand-crafted
transformer for this language, including the details of which
dimension represents which partial computation. RASP
can concisely describe the same solution, showing the high-
arrangement into an actual transformer architecture.
We present this solution in Figure: the code compiles to
a transformer architecture using 2 layers and a total of 3
heads, exactly as in the construction of Bhattamishra et al..
These numbers are inferred by the RASP compiler: the
programmer does not have to think about such details.
A pair of parentheses is balanced in a sequence if their run-
ning balance is never negative, and additionally is equal to
exactly 0 at the ﬁnal input token. Lines 13 23 check this
deﬁnition: lines 13 and 14 use pair_balance to compute
the running balances of each parenthesis pair, and 17 checks
whether these balances were negative anywhere in the se-
quence. The snippet in 21 (bal1==0 and bal2==0) creates
an s-op checking at each location whether both pairs are
balanced, with the aggregation of line 20 loading the value
of this s-op from the last position. From there, a boolean
composition of end_0 and had_neg deﬁnes shufﬂe-dyck-2.
Compilation and Abstraction The high-level operations
in RASP can be compiled down to execute on a transformer:
for example, the code presented in Figure compiles to a
two-layer, 3-head (total) architecture, whose attention pat-
terns when applied to the input sequence " aaabbccdef"
are presented in Figure(b). (The full compiled compu-
tation ﬂow for this program showing how its component
s-ops interact is presented in Appendix).
RASP abstracts away low-level operations into simple prim-
itives, allowing a programmer to explore the full potential
of how these are realized in practice. At the same time,
RASP enforces the information-ﬂow constraints of trans-
formers, preventing anyone from writing a program more
powerful than they can express. One example of this is the
lack of input-dependent loops in the s-ops, reﬂecting the fact
that transformers cannot arbitrarily repeat operations2. An-
other is in the selectors: for each two positions, the decision
whether one selects ( attends to ) the other is pairwise.
We ﬁnd RASP a natural tool for conveying transformer
solutions to given tasks. It is modular and compositional,
allowing us to focus on arbitrarily high-level computations
when writing programs. Of course, we are restricted to
tasks for which a human can encode a solution: we do not
expect any researcher to implement, e. g., a strong language
model or machine-translation system in RASP these are
not realizable in any programming language. Rather, we
encode in traditional programming languages, and the
way they relate to the expressive power of the transformer.
In Section, we will show empirically that RASP solutions
can indeed translate to real transformers. One example is
given in Figure: having written a RASP program (left)
for the double-histograms task, we analyse it to obtain the
mimic our solution, and then train a transformer with super-
a neural version of our solution (right). We ﬁnd that the
and use them to reach a high accuracy on the target task.
2Though work exploring such transformer variants exists: De-
hghani et al. devise a transformer architecture with a control
unit, which can repeat its sublayers arbitrarily many times.
3. The RASP language
functions referred to as s-ops (sequence operators), func-
sequence of the same length. Excluding some atomic values,
and the convenience of lists and dictionaries, everything in
RASP is a function. Hence, to simplify presentation, we of-
ten demonstrate RASP values with one or more input-output
pairs: for example, identity("hi")="hi"3.
RASP has a small set of built-in s-ops, and the goal of
programming in RASP is to compose these into a ﬁnal
s-op computing the target task. For these compositions,
the functions select (creating selection matrices called se-
lectors), aggregate (collapsing selectors and s-ops into a
new s-ops), and selector_width (creating an s-op from
a selector) are provided, along with several elementwise
operators reﬂecting the feed-forward sublayers of a trans-
former. As noted in Section, while all s-ops and selectors
are in fact functions, we will prefer to talk in terms of the
sequences and matrices that they create. Constant values
in RASP (e. g., 2, T, h) are treated as s-ops with a single
value broadcast at all positions, and all symbolic values are
which is the value being manipulated in practice.
The built-in s-ops The simplest s-op is the identity, given
in RASP under the name tokens: tokens("hi")="hi".
The other built-in s-ops are indices and length,
processing input sequences as their names suggest:
indices("hi")=, and length("hi")=.
s-ops can be combined with constants (numbers, booleans,
or tokens) or each other to create new s-ops, in either an
elementwise or more complicated fashion.
Elementwise combination of s-ops is done by
the common operators for the values they con-
tain, for example: (indices+1)("hi")=, and
((indices+1)==length)("hi")=[F, T]. This includes
also a ternary operator: (tokens if (indices%2==0)
else "-")("hello")="h-l-o". When the condition of
the operator is an s-op itself, the result is an s-op that is
dependent on all 3 of the terms in the operator creating it.
Select and Aggregate operations are used to combine in-
formation from different sequence positions. A selector
takes two lists, representing keys and queries respectively,
and a predicate p, and computes from these a selection ma-
trix describing for each key, query pair (k, q) whether the
condition p(k, q) holds.
3We use strings as shorthand for a sequence of characters.
For example:
list, and averages for each row of the matrix the values of
the list in its selected columns. For example,
Intuitively, a select-aggregate pair can be thought of as a
two-dimensional map-reduce operation. The selector can be
viewed as performing ﬁltering, and aggregate as performing
a reduce operation over the ﬁltered elements (see Figure).
In RASP, the selection operation is provided through
the function select, which takes two s-ops k and q
and a comparison operator and returns the composi-
tion of sel (,, ) with k and q, with this sequence-to-
matrix function referred to as a selector. For exam-
ple: a=select(indices, indices, ) is a selector, and
a("hey")=[[F, F, F],[T, F, F],[T, T, F]]. Similarly, the
aggregation operation is provided through aggregate,
which takes one selector and one s-op and returns
the composition of agg with these. For example:
aggregate(a, indices+1)("hey")=.4
Simple select-aggregate examples To create the s-
op that reverses any input sequence, we build a se-
lector that requests for each query position the to-
ken at the opposite end of the sequence, and then
aggregate that selector with the original input to-
kens: flip=select(indices, length-indices-1,==),
and reverse=aggregate(flip, tokens). For example:
flip("hey") =
reverse("hey") = "yeh"
"a" in our input, we build a selector that gathers infor-
mation from all input positions, and then aggregate it
4For convenience and efﬁciency, when averaging the ﬁl-
tered values in an aggregation, for every position where only
a single value has been selected, RASP passes that value di-
rectly to the output without attempting to average it. This
of tokens between locations: for example, using the selector
load1=select(indices,1,==), we may directly create the s-
op aggregate(load1, tokens)("hey")="eee". Additionally, in
positions when no values are selected, the aggregation simply re-
turns a default value for the output (in Figure, we see this with
default value 0), this value may be set as one of the inputs to the
aggregate function.
with a sequence broadcasting 1 wherever the input to-
ken is "a", and 0 everywhere else. This is expressed
as select_all=select(1,1,==), and then frac_as =
aggregate(select_all,1 if tokens=="a" else 0).
elementwise using boolean logic. For example, for
load1=select(indices,1,==) and flip from above:
(load1 or flip)("hey") =
selector width The ﬁnal operation in RASP is the powerful
selector_width, which takes as input a single selector
and returns a new s-op that computes, for each output
position, the number of input values which that selector has
chosen for it. This is best understood by example: using
the selector same_token=select(tokens, tokens,==)
that ﬁlters for each query position the keys with
the same token as its own, we can compute its
width to obtain a histogram of our input sequence:
selector_width(same_token)("hello")=.
Additional operations: While the above operations are
together sufﬁcient to represent any RASP program, RASP
further provides a library of primitives for common op-
erations, such as in either of a value within a se-
quence: ("i" in tokens)("hi")=[T, T], or of each
value in a sequence within some static list: tokens in
["a","b","c"])("hat")=[F, T, F]. RASP also provides
functions such as count, or sort.
3.1. Relation to a Transformer
information ﬂow of a transformer architecture, suggesting
how many heads and layers are needed to solve a task.
The built-in s-ops indices and tokens reﬂect
the initial input embeddings of a transformer,
while length is computed in RASP: length=
aggregate(select_all, indicator(indices==0)),
where select_all=select(1,1,==).
Elementwise Operations reﬂect the feed-forward sub-
layers of a transformer. These have overall not been re-
stricted in any meaningful way: as famously shown by
Hornik et al. , MLPs such as those present in the
feed-forward transformer sub-layers can approximate with
arbitrary accuracy any borel-measurable function, provided
sufﬁciently large input and hidden dimensions.
matrices, deﬁning for each input the selection (attention) pat-
weighted averages, and aggregation reﬂects this ﬁnal averag-
ing operation. The uniform weights dictated by our selectors
reﬂect an attention pattern in which unselected pairs are
all given strongly negative scores, while the selected pairs
all have higher, similar, scores. Such attention patterns are
supported by the ﬁndings of .
selectors to be reused in multiple aggregations, abstract-
attention heads in the compiled architecture. Making se-
lectors ﬁrst class citizens also enables functions such as
selector_width, which take selectors as parameters.
Additional abstractions All other operations, including
the powerful selector_width operation, are implemented
in terms of the above primitives. selector_width in par-
one or two selectors, depending on whether or not one can
assume a beginning-of-sequence token is added to the input
sequence. Its implementation is given in Appendix.
Compilation Converting an s-op to a transformer architec-
ture is as simple as tracing its computation ﬂow out from
the base s-ops. Each aggregation is an attention head, which
must be placed at a layer later than all of its inputs. El-
ementwise operations are feedforward operations, and sit
in the earliest layer containing all of their dependencies.
Some optimisations are possible: for example, aggregations
merged into the same attention head. A full" compilation
to concrete transformer weights requires to e. g. derive
MLP weights for the elementwise operations, and is beyond
the scope of this work. RASP provides a method to visualize
this compiled ﬂow for any s-op and input pair: Figures 4
and 5 were rendered using draw(reverse,"abcde") and
draw(hist," aabbaabb").
4. Implications and insights
Restricted-Attention Transformers Multiple works pro-
efﬁcient transformers, reducing the time complexity of each
layer from O(n2) to O(nlog(n)) or even O(n) with respect
to the input sequence length n for a
survey of such approaches). Several of these do so using
sparse attention, in which the attention is masked using
interact ).
cannot perform. In particular, these variants of transformers
all impose restrictions on the selectors, permanently forcing
some of the n2 index pairs in every selector to False. But
does this necessarily weaken these transformers?
In Appendix we present a sorting algorithm in RASP, ap-
plicable to input sequences with arbitrary length and alpha-
bet size5. This problem is known to require at Ω(nlog(n))
transformer can take full advantage of Ω(nlog(n)) of the
n2 operations it performs in every attention head. It fol-
o(nlog(n)) operations incur a real loss in expressive power.
Sandwich Transformers Recently, Press et al. 
showed that reordering the attention and feed-forward sub-
modeling tasks. In particular, they showed that: 1. pushing
feed-forward sublayers towards the bottom of a transformer
weakened it; and 2. pushing attention sublayers to the bot-
tom and feed-forward sublayers to the top strengthened it,
provided there was still some interleaving in the middle.
The base operations ofRASP help us understand the observa-
tions of Press et al.. Any arrangement of a transformer s sub-
layers into a ﬁxed architecture imposes a restriction on the
in a program compilable to that architecture. For example,
an architecture in which all feed-forward sublayers appear
before the attention sublayers, imposes that no elementwise
operations may be applied to the result of any aggregation.
In RASP, there is little value to repeated elementwise op-
erations before the ﬁrst aggregate: each position has only
its initial input, and cannot generate new information. This
explains the ﬁrst observation of Press et al.. In contrast, an
architecture beginning with several attention sublayers i. e.,
multiple select-aggregate pairs will be able to gather
the computation, even if only by simple rules6. More com-
generating new selectors, explaining the second observation.
Recognising Dyck-kLanguages The Dyck-klanguages
the languages of sequences of correctly balanced parenthe-
ses, with k parenthesis types have been heavily used in
considering the expressive power of RNNs .
Such investigations motivate similar questions for trans-
formers, and several works approach the task. Hahn 
proves that transformer-encoders with hard attention can-
not recognise Dyck-2. Bhattamishra et al. and Yao
et al. provide transformer-encoder constructions
5Of course, realizing this solution in real transformers requires
sufﬁciently stable word and positional embeddings a practical
limitation that applies to all transformer variants.
6While the attention sublayer of a transformer does do some
vectors, it does not contain the powerful MLP with hidden layer as
is present in the feed-forward sublayer.
for recognizing simpliﬁed variants of Dyck-k, though the
simpliﬁcations are such that no conclusion can be drawn
for unbounded depth Dyck-kwith k 1. Optimistically,
Ebrahimi et al. train a transformer-encoder with
causal attention masking to process Dyck-klanguages with
reasonable accuracy for several k 1, ﬁnding that it learns
a stack-like behaviour to complete the task.
We consider Dyck- k using RASP, speciﬁcally deﬁning
Dyck-k-PTF as the task of classifying for every preﬁx of
a sequence whether it is legal, but not yet balanced ( P),
balanced (T), or illegal (F). We show that RASP can solve
this task in a ﬁxed number of heads and layers for any k,
presenting our solution in Appendix.
Symbolic Reasoning in Transformers Clark et al. 
show that transformers are able to emulate symbolic reason-
ing: they train a transformer which, given the facts Ben
is a bird" and birds can ﬂy", correctly validates that Ben
can ﬂy". Moreover, they show that transformers are able to
perform several logical steps: given also the fact that only
winged animals can ﬂy, their transformer conﬁrms that Ben
has wings. This ﬁnding however does not shed any light on
how the transformer is achieving such a feat.
RASP empowers us to approach the problem on a high level.
We write a RASP program for the related but simpliﬁed
problem of containment and inference over sets of elements,
sets, and logical symbols, in which the example is written
as b B, x B x F, b F? (implementation available in
our repository). The main idea is to store at the position of
in that set, and at each element symbol the sets it is and
is not contained in. Logical inferences are computed by
passing information between symbols in the same fact,
symbols, which share their stored information.
Use of Separator Tokens Clark et al. observe that
many attention heads in BERT (some-
times) focus on separator tokens, speculating that these are
used for no-ops" in the computation. ﬁnd that transformers more successfully learn Dyck-
beginning-of-sequence (BOS) token, with the trained mod-
parentheses. Our RASP programs suggest an additional
role that such separators may be playing: by providing a
ﬁxed signal from a neutral position, separators facilitate
conditioned counting in transformers, that use the diffusion
attending to. Without such neutral positions, counting re-
quires an additional head, such that an agreed-upon position
7We note that RASP does not suggest the embedding width
needed to encode this solution in an actual transformer.
may artiﬁcially be treated as neutral in one head and then
independently accounted for in the other.
A simple example of this is seen in Figure. There,
selector_width is applied with a BOS token, creating
ﬁrst input position (the BOS location) from all query
positions, in addition to the actual positions selected
by select(tokens, tokens,==). A full description of
selector_width is given in Appendix.
5. Experiments
fronts: 1. its ability to upper bound the number of heads and
layers required to solve a task, 2. the tightness of that bound,
3. its feasibility in a transformer, i. e., whether a sufﬁciently
large transformer can encode a given RASP solution., train-
ing several transformers. We relegate the exact details of
the transformers and their training to Appendix.
For this section, we consider the following tasks:
1. Reverse, e. g.: reverse("abc")="cba".
2. Histograms, with a unique beginning-of-sequence
(BOS) token (e. g., hist_bos(" aba")=)
and without it (e. g., hist_nobos("aba")=).
3. Double-Histograms, with BOS: for each token, the
as itself. E. g.: hist2(" abbc")=.
4. Sort, with BOS: ordering the input tokens lexicograph-
ically. e. g.: sort(" cba")=" abc".
5. Most-Freq, with BOS: returning the unique input to-
kens in order of decreasing frequency, with original
position as a tie-breaker and the BOS token for padding.
E. g.: most_freq(" abbccddd")=" dbca ".
6. Dyck-i PTF, for i = 1,2: the task of returning,
at each output position, whether the input preﬁx up
to and including that position is a legal Dyck- i se-
quence (T), and if not, whether it can ( P) or cannot
(F) be continued into a legal Dyck- i sequence. E. g:
Dyck1_ptf("()())")="PTPTF".
We refer to double-histogram as 2-hist, and to each Dyck-i
PTF problem simply as Dyck-i. The full RASP programs for
these tasks, and the computation ﬂows they compile down
to, are presented in Appendix. The size of the transformer
architecture each task compiles to is presented in Table.
Upper bounding the difﬁculty of a task Given a RASP
program for a task, e. g. double-histogram as described in
Figure, we can compile it down to a transformer architec-
8The actual optimal solution for Dyck-2 PTF cannot be realised
in RASP as is, as it requires the addition of a select-best operator
to the language reﬂecting the power afforded by softmax in the
transformer s self-attention. In this paper, we always refer to our
analysis of Dyck-2 with respect to this additional operation.
Language Layers Heads Test Acc. Matches?
Reverse 2 1 99.99%
Hist BOS 1 1 100%
Hist no BOS 1 2 99.97%
Double Hist 2 2 99.58%
Sort 2 1 99.96%
Most Freq 3 2 95.99%
Dyck-1 PTF 2 1 99.67%
Dyck-2 PTF 8 3 1 99.85%
Table: Does a RASP program correctly upper bound the
solve a task? In the left columns, we show the compilation
size of our RASP programs for each considered task, and
in the right columns we show the best (of 4) accuracies
of transformers trained on these same tasks, and evaluate
whether their attention mechanisms appear to match (using a
for partially similar patterns: see Figure for an example).
per layer, we report the maximum of these.
ture, effectively predicting the maximum number of layers
and layer width (number of heads in a layer) needed to solve
that task in a transformer. To evaluate whether this bound is
truly sufﬁcient for the transformer, we train 4 transformers
of the prescribed sizes on each of the tasks.
each task in Table. Most of these transformers reached
accuracies of 99.5% and over, suggesting that the upper
bounds obtained by our programs are indeed sufﬁcient for
solving these tasks in transformers. For some of the tasks,
we even ﬁnd that the RASP program is the same as or very
similar to the natural solution found by the trained trans-
former. In particular, Figures 4 and 5 show a strong simi-
for the tasks Reverse and Histogram-BOS, though the trans-
mechanism for computing length than that given in RASP.
predicted by our compilation, and observing the drop-off in
test accuracy. Speciﬁcally, we repeat our above experiments,
but this time we also train each task on up to 4 different
sizes. In particular, denoting L, H the number of layers and
heads predicted by our compiled RASP programs, we train
for each task transformers with sizes (L, H), (L 1, H),
(L, H 1), and (L 1,2H) (where possible) 9.
9The transformers of size (L 1, 2H) are used to validate that
layers, as opposed to the reduction in total heads that this entails.
However, doubling H means the embedding dimension will be
divided over twice as many heads. To counteract any negative
effect this may have, we also double the embedding dimension for
opp_index = length - indices - 1;
flip = select ( indices, opp_index,==);
reverse = aggregate (flip, tokens );
Figure: Top: RASP code for computing reverse
(e. g., reverse("abc")="cba"). Below, its compila-
tion to a transformer architecture (left, obtained through
draw(reverse,"abcde") in the RASP REPL), and the at-
(right), both visualised on the same input. Visually, the atten-
the program. The head in the ﬁrst layer, however, appears
to have learned a different solution from our own: instead
of focusing uniformly on the entire sequence (as is done
in the computation of length in RASP ), this head shows a
preference for the last position in the sequence.
these architectures in Table. For most of the tasks, the
transformers fail completely to learn their target languages.
The main exception to this is sort, which appears unaffected
by the removal of one layer, and even achieves its best results
in this case. Drawing the attention pattern for the single-
patterns. It appears that the transformer has learned to take
advantage of the bounded input alphabet size, effectively
these transformers.
same_tok = select (tokens, tokens, ==);
hist = selector_width ( same_tok,
assume_bos = True );
Figure: The RASP program for computing with-BOS histograms (left), alongside its compilation to a transformer
architecture (cream boxes) and the attention head (center bottom) of a transformer trained on the same task, without attention
supervision. The compiled architecture and the trained head are both presented on the same input sequence, " aabbaabb".
The transformer architecture was generated in the RASP REPL using draw(hist," aabbaabb").
Language RASP Average test accuracy (%) with...
L, H L, H H 1 L 1 L 1, 2H
Reverse 2, 1 99.9 - 23.1 41.2
Most Freq 3, 2 93.9 92.1 84.0 90.2
Table: Accuracy dropoff in transformers when reducing
RASP solutions for the same tasks. The transformers trained
on the size predicted by RASP have very high accuracy, and
in most cases there is a clear drop as that size is reduced.
Cases creating an impossible architecture (H or Lzero) are
marked with -. Histogram with BOS uses only 1 layer and
head, and so is not included. As in Table, Dyck- 2 is
considered with the addition of select_best to RASP.
implementing bucket sort for its task. This is because a
single full-attention head is sufﬁcient to compute for every
token its total appearances in the input, from which the
correct output can be computed locally at every position.
RASP program can indeed be represented in a transformer.
For this, we return to the tougher tasks above, and this time
train the transformer with an additional loss component en-
compiled solution (i. e., we supervise the attention patterns
in addition to the target output). In particular, we consider
the tasks double-histogram, sort, and most-freq, all with
the assumption of a BOS token in the input. After train-
ing each transformer for 250 epochs with both target and
attention supervision, they all obtain high test accuracies on
the task (99+%), and appear to encode attention patterns
similar to those compiled from our solutions. We present
the obtained patterns for double-histogram, alongside the
compiled RASP solution, in Figure. We present its full
computation ﬂow, as well as the learned attention patterns
and full ﬂow of sort and most-freq, in Appendix.
6. Conclusions
We abstract the computation model of the transformer-
encoder as a simple sequence processing language, RASP,
that captures the unique constraints on information ﬂow
present in a transformer. Considering computation prob-
details of a neural network in favor of symbolic programs.
to realise it in a transformer. We show several examples
of programs written in the RASP language, showing how
operations can be implemented by a transformer, and train
several transformers on these tasks, ﬁnding that RASP helps
to solve them. Additionally, we use RASP to shed light on
an empirical observation over transformer variants, and ﬁnd
concrete limitations for some efﬁcient transformers.
Acknowledgments We thank Uri Alon, Omri Gilad, Daniel
Filan, and the reviewers for their constructive comments.
Council under European Union s Horizon 2020 research &
innovation programme, agreement #802774 (iEXTRACT).