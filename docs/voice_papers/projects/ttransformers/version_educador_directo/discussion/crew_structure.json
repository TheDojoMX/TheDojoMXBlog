{
  "project_name": "version_educador_directo",
  "paper_title": "Think Transformers",
  "language": "Spanish",
  "agents": [
    {
      "role": "Coordinator",
      "goal": "Coordinate the discussion and ensure all perspectives are heard",
      "backstory": "You are an experienced moderator who ensures productive discussions"
    },
    {
      "role": "Scientific Reviewer",
      "goal": "Verify the soundness and methodology of the paper",
      "backstory": "You are a rigorous scientist who evaluates research methodology and conclusions"
    },
    {
      "role": "Critical Thinker",
      "goal": "Question assumptions and challenge ideas presented",
      "backstory": "You are a skeptical academic who questions everything and looks for flaws"
    },
    {
      "role": "Educational Writer",
      "goal": "Create engaging educational content in the style of popular science educators",
      "backstory": "You are a skilled science communicator who explains complex topics in an accessible, engaging way like 3Blue1Brown or other popular educators"
    },
    {
      "role": "Voice Director",
      "goal": "Transform content into perfect voice-ready script for publication",
      "backstory": "You are a master voice coach and script editor who specializes in creating flawless, publication-ready scripts that voice actors can read naturally. You ensure every word flows perfectly when spoken aloud."
    },
    {
      "role": "AI Researcher",
      "goal": "Provide technical insights on AI methodology and implications",
      "backstory": "You are an AI researcher with deep technical knowledge"
    },
    {
      "role": "AI Philosopher",
      "goal": "Discuss philosophical implications of AI research",
      "backstory": "You are a philosopher specializing in AI ethics and implications"
    },
    {
      "role": "AI Doomer",
      "goal": "Raise concerns about potential risks and negative consequences",
      "backstory": "You are concerned about AI safety and potential existential risks"
    },
    {
      "role": "AI Enthusiast",
      "goal": "Highlight positive potential and applications",
      "backstory": "You are optimistic about AI's potential to solve problems"
    },
    {
      "role": "AI Newcomer",
      "goal": "Ask basic questions that others can answer",
      "backstory": "You know little about AI but are curious and ask good questions"
    }
  ],
  "tasks": [
    {
      "description": "\n            Analyze the paper titled \"Think Transformers\" and provide your perspective.\n            \n            Paper content:\n            Gail Weiss1 Yoav Goldberg2 3 Eran Yahav1\nWhat is the computational model behind a Trans-\nformer? Where recurrent neural networks have\ndirect parallels in ﬁnite state machines, allow-\ning clear discussion and thought around archi-\ntecture variants or trained models, Transformers\nhave no such familiar parallel. In this paper we\naim to change that, proposing a computational\nmodel for the transformer-encoder in the form\nof a programming language. We map the basic\ncomponents of a transformer-encoder attention\nand feed-forward computation into simple prim-\nitives, around which we form a programming lan-\nguage: the Restricted Access Sequence Process-\ning Language (RASP). We show how RASP can\nconceivably be learned by a Transformer, and how\na Transformer can be trained to mimic a RASP so-\nlution. In particular, we provide RASP programs\nfor histograms, sorting, and Dyck-languages. We\nfurther use our model to relate their difﬁculty in\nterms of the number of required layers and atten-\ntion heads: analyzing a RASP program implies a\nto encode a task in a transformer. Finally, we see\nused to explain phenomena seen in recent works.\n1. Introduction\nWe present a computational model for the transformer ar-\nRASP (Restricted Access Sequence Processing Language).\nMuch as the token-by-token processing of RNNs can be\nconceptualized as ﬁnite state automata , our language captures the unique information-ﬂow\nconstraints under which a transformer operates as it pro-\ncesses input sequences. Our model helps reason about how\n1Technion, Haifa, Israel 2Bar Ilan University, Ramat Gan,\nIsrael 3Allen Institute for AI. Correspondence to: Gail Weiss\nsgailw@cs. technion. ac. il.\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\na transformer operates at a higher-level of abstraction, rea-\nrather than neural network primitives.\nWe are inspired by the use of automata as an abstract compu-\ntational model for recurrent neural networks (RNNs). Using\nof work, including extraction of automata from RNNs ,\nanalysis of RNNs practical expressive power in terms of au-\ntomata , and even augmentations based\non automata variants (Joulin & Mikolov, 2015). Previous\nwork on transformers explores their computational power,\nbut does not provide a computational model .\nThinking in terms of the RASP model can help derive com-\nputational results. Bhattamishra et al. and Ebrahimi\net al. explore the ability of transformers to recognize\nDyck-k languages, with Bhattamishra et al. providing a\nconstruction by which Transformer-encoders can recognize\na simpliﬁed variant of Dyck-k. Using RASP, we succinctly\nexpress the construction of as a\nshort program, and further improve it to show, for the ﬁrst\ntime, that transformers can fully recognize Dyck-kfor all k.\nScaling up the complexity, Clark et al. showed em-\nmulti-step logical reasoning over ﬁrst order logical formulas\nprovided as input, resulting in soft theorem provers. For\nthis task, the mechanism of the computation remained elu-\nsive: how does a transformer perform even non-soft theorem\nproving? As the famous saying by Richard Feynman goes,\nwhat I cannot create, I do not understand: usingRASP, we\ninferences over input expressions, and then compile it to\nthe transformer hardware, deﬁning a sequence of attention\nand multi-layer perceptron (MLP) operations.\nConsidering computation problems and their implementa-\nin favor of symbolic programs. Recognizing that a task\nis representable in a transformer is as simple as ﬁnding a\nRASP program for it, and communicating this solution\npreviously done by presenting a hand-crafted transformer\narXiv:2106.06981v2 [cs. LG] 19 Jul 2021\nsame_tok = select ( tokens, tokens,==);\nhist = selector_width (\nsame_tok,\nassume_bos = True );\nfirst = not has_prev ( tokens );\nsame_count = select (hist, hist,==);\nsame_count_reprs = same_count and\nselect ( first, True,==);\nhist2 = selector_width (\nsame_count_reprs,\nassume_bos = True );\nFigure: We consider double-histogram, the task of counting for each input token how many unique input tokens have the\nsame frequency as itself (e. g.: hist2(\" aaabbccdef\")=). (a) shows a RASP program for this\ntask, (b) shows the selection patterns of that same program, compiled to a transformer architecture and applied to the input\nsequence aaabbccdef, (c) shows the corresponding attention heatmaps, for the same input sequence, in a 2-layer 2-head\ntransformer trained on double-histogram. This particular transformer was trained using both target and attention supervision,\ni. e.: in addition to the standard cross entropy loss on the target output, the model was given an MSE-loss on the difference\nbetween its attention heatmaps and those expected by the RASP solution. The transformer reached test accuracy of 99.9%\non the task, and comparing the selection patterns in (b) with the heatmaps in (c) suggests that it has also successfully learned\nto replicate the solution described in (a).\nfor the task is now possible through a few lines of code.\nrecent empirical observation of transformer variants , and to ﬁnd concrete limitations of efﬁcient\ntransformers with restricted attention .\nIn Section, we show how a compiled RASP program can\nindeed be realised in a neural transformer (as in Figure),\ntrained on the task using gradient descent (Figs 5 and 4).\nCode We provide a RASP read-evaluate-print-loop (REPL)\nin http: github. com tech-srl RASP, along with a\nRASP cheat sheet and link to replication code for our work.\n2. Overview\nWe begin with an informal overview ofRASP, with exam-\nples. The formal introduction is given in Section\nIntuitively, transformers computations are applied to their\nentire input in parallel, using attention to draw on and com-\ntheir calculations . The iterative process of a trans-\nrather the depth of the computation: the number of layers it\napplies to its input as it works towards its ﬁnal result.\nThe computational model. Conceptually, a RASP com-\nputation over length-ninput involves manipulation of se-\nquences of length n, and matrices of size n n. There\ncomputation. The abstract computation model is as follows:\nThe input of a RASP computation is two sequences, tokens\nand indices. The ﬁrst contains the user-provided input, and\nthe second contains the range 0,1,..., n 1. The output of\na RASP computation is a sequence, and the consumer of the\noutput can choose to look only at speciﬁc output locations.\nelement-wise operations. For example, for the sequences\ns1 = and s2 = , we can derive s1 + s2 =\n, s1 +2 = , pow(s1,2) = , s1 2 =\n[F, F, T ], pairwise_mul(s1, s2) = , and so on.\naggregate operations (Figure). Select operations take two\nsequences k, q and a boolean predicatepover pairs of values,\nand return a selection matrix Ssuch that for every i, j [n],\nS[i][j] = p(k[i], q[j]). Aggregate operations take a matrix S\nand a numeric sequence v, and return a sequence sin which\neach position s[i] combines the values in vaccording to row\niin S(see full deﬁnition in Section).\nAggregate operations (over select matrices) are the only way\nto combine values from different sequence positions, or to\ns = select(,,==) res=aggregate(s, )\nT F F 4 6 8 = 4 = \nFigure: Visualizing the select and aggregate operations.\nOn the left, a selection matrixs is computed by select, which\n==. On the right, aggregate uses s as a ﬁlter over its input\nvalues, averaging only the selected values at each position\nin order to create its output, res. Where no values have been\nselected, aggregate substitutes 0 in its output.\nmove values from one position to another. For example, to\nperform the python computation: x = for _ in a],\nwe must ﬁrst use S= select(indices,0,=) to select the ﬁrst\nposition, and then x= aggregate(S, a) to broadcast it across\na new sequence of the same length.\nRASP programs are lazy functional, and thus operate on\nfunctions rather than sequences. That is, instead of a se-\nquence indices= , we have a function indices that\nreturns on inputs of length 3. Similarly, s3=s1+s2\nis a function, that when applied to an input xwill produce\nthe value s3(x), which will be computed as s1(x)+s2(x).\nWe call these functions s-ops (sequence operators). The\nsame is true for the selection matrices, whose functions we\nrefer to as selectors, and the RASP language is deﬁned in\nterms of s-ops and selectors, not sequences and matrices.\nHowever, the conceptual model to bear in mind is that of\noperations over sequences and selection matrices.\nExample: Double Histograms The RASP program\nin Figure solves double-histogram, the task of count-\nin the sequence have the same frequency as its own:\nhist2(\" aabcd\")=. The program begins\nby creating the the selector same_tok, in which each input\ntoken as its own, and then applies the RASP operation\nselector_width to it in order to obtain the s-op hist,\nwhich computes the frequency of each token in the in-\nput: hist(\"hello\")=. Next, the program\nuses the function has_prev1 to create the s-op first,\nwhich marks the ﬁrst appearance of each token in a se-\nquence: first(\"hello\")=[T, T, T, F, T]. Finally, apply-\ning selector_width to the selector same_count_reprs,\nwhich focuses each position on all ﬁrst tokens with the\nsame frequency as its own, provides hist2 as desired.\n1Presented in Figure in Appendix.\ndef frac_prevs (sop, val )\nprevs = select ( indices, indices, =);\nreturn aggregate ( prevs,\nindicator ( sop==val ));\ndef pair_balance (open, close )\nopens = frac_prevs ( tokens, open );\ncloses = frac_prevs ( tokens, close );\nreturn opens - closes;\nbal1 = pair_balance (\"(\",\") \");\nbal2 = pair_balance (\" \",\" \");\nnegative = bal1 0 or bal2 0;\nhad_neg = aggregate ( select_all,\nindicator ( negative )) 0;\nselect_last = select ( indices, length -1, ==);\nend_0 = aggregate ( select_last,\nbal1==0 and bal2==0 );\nshuffle_dyck2 = end_0 and not had_neg;\nFigure: RASP program for the task shufﬂe-dyck-2 (bal-\nance 2 parenthesis pairs, independently of each other), cap-\nturing a higher level representation of the hand-crafted trans-\nformer presented by Bhattamishra et al. .\nExample: Shufﬂe-Dyck in RASP As an example of the\nkind of tasks that are natural to encode usingRASP, consider\nthe Shufﬂe-Dyck language, in which multiple parentheses\ntypes must be balanced but do not have to satisfy any or-\nder with relation to each other. (For example, \"([)]\" is\nconsidered balanced). In their work on transformer expres-\nsiveness, Bhattamishra et al. present a hand-crafted\ntransformer for this language, including the details of which\ndimension represents which partial computation. RASP\ncan concisely describe the same solution, showing the high-\narrangement into an actual transformer architecture.\nWe present this solution in Figure: the code compiles to\na transformer architecture using 2 layers and a total of 3\nheads, exactly as in the construction of Bhattamishra et al..\nThese numbers are inferred by the RASP compiler: the\nprogrammer does not have to think about such details.\nA pair of parentheses is balanced in a sequence if their run-\nning balance is never negative, and additionally is equal to\nexactly 0 at the ﬁnal input token. Lines 13 23 check this\ndeﬁnition: lines 13 and 14 use pair_balance to compute\nthe running balances of each parenthesis pair, and 17 checks\nwhether these balances were negative anywhere in the se-\nquence. The snippet in 21 (bal1==0 and bal2==0) creates\nan s-op checking at each location whether both pairs are\nbalanced, with the aggregation of line 20 loading the value\nof this s-op from the last position. From there, a boolean\ncomposition of end_0 and had_neg deﬁnes shufﬂe-dyck-2.\nCompilation and Abstraction The high-level operations\nin RASP can be compiled down to execute on a transformer:\nfor example, the code presented in Figure compiles to a\ntwo-layer, 3-head (total) architecture, whose attention pat-\nterns when applied to the input sequence \" aaabbccdef\"\nare presented in Figure(b). (The full compiled compu-\ntation ﬂow for this program showing how its component\ns-ops interact is presented in Appendix).\nRASP abstracts away low-level operations into simple prim-\nitives, allowing a programmer to explore the full potential\nof how these are realized in practice. At the same time,\nRASP enforces the information-ﬂow constraints of trans-\nformers, preventing anyone from writing a program more\npowerful than they can express. One example of this is the\nlack of input-dependent loops in the s-ops, reﬂecting the fact\nthat transformers cannot arbitrarily repeat operations2. An-\nother is in the selectors: for each two positions, the decision\nwhether one selects ( attends to ) the other is pairwise.\nWe ﬁnd RASP a natural tool for conveying transformer\nsolutions to given tasks. It is modular and compositional,\nallowing us to focus on arbitrarily high-level computations\nwhen writing programs. Of course, we are restricted to\ntasks for which a human can encode a solution: we do not\nexpect any researcher to implement, e. g., a strong language\nmodel or machine-translation system in RASP these are\nnot realizable in any programming language. Rather, we\nencode in traditional programming languages, and the\nway they relate to the expressive power of the transformer.\nIn Section, we will show empirically that RASP solutions\ncan indeed translate to real transformers. One example is\ngiven in Figure: having written a RASP program (left)\nfor the double-histograms task, we analyse it to obtain the\nmimic our solution, and then train a transformer with super-\na neural version of our solution (right). We ﬁnd that the\nand use them to reach a high accuracy on the target task.\n2Though work exploring such transformer variants exists: De-\nhghani et al. devise a transformer architecture with a control\nunit, which can repeat its sublayers arbitrarily many times.\n3. The RASP language\nfunctions referred to as s-ops (sequence operators), func-\nsequence of the same length. Excluding some atomic values,\nand the convenience of lists and dictionaries, everything in\nRASP is a function. Hence, to simplify presentation, we of-\nten demonstrate RASP values with one or more input-output\npairs: for example, identity(\"hi\")=\"hi\"3.\nRASP has a small set of built-in s-ops, and the goal of\nprogramming in RASP is to compose these into a ﬁnal\ns-op computing the target task. For these compositions,\nthe functions select (creating selection matrices called se-\nlectors), aggregate (collapsing selectors and s-ops into a\nnew s-ops), and selector_width (creating an s-op from\na selector) are provided, along with several elementwise\noperators reﬂecting the feed-forward sublayers of a trans-\nformer. As noted in Section, while all s-ops and selectors\nare in fact functions, we will prefer to talk in terms of the\nsequences and matrices that they create. Constant values\nin RASP (e. g., 2, T, h) are treated as s-ops with a single\nvalue broadcast at all positions, and all symbolic values are\nwhich is the value being manipulated in practice.\nThe built-in s-ops The simplest s-op is the identity, given\nin RASP under the name tokens: tokens(\"hi\")=\"hi\".\nThe other built-in s-ops are indices and length,\nprocessing input sequences as their names suggest:\nindices(\"hi\")=, and length(\"hi\")=.\ns-ops can be combined with constants (numbers, booleans,\nor tokens) or each other to create new s-ops, in either an\nelementwise or more complicated fashion.\nElementwise combination of s-ops is done by\nthe common operators for the values they con-\ntain, for example: (indices+1)(\"hi\")=, and\n((indices+1)==length)(\"hi\")=[F, T]. This includes\nalso a ternary operator: (tokens if (indices%2==0)\nelse \"-\")(\"hello\")=\"h-l-o\". When the condition of\nthe operator is an s-op itself, the result is an s-op that is\ndependent on all 3 of the terms in the operator creating it.\nSelect and Aggregate operations are used to combine in-\nformation from different sequence positions. A selector\ntakes two lists, representing keys and queries respectively,\nand a predicate p, and computes from these a selection ma-\ntrix describing for each key, query pair (k, q) whether the\ncondition p(k, q) holds.\n3We use strings as shorthand for a sequence of characters.\nFor example:\nlist, and averages for each row of the matrix the values of\nthe list in its selected columns. For example,\nIntuitively, a select-aggregate pair can be thought of as a\ntwo-dimensional map-reduce operation. The selector can be\nviewed as performing ﬁltering, and aggregate as performing\na reduce operation over the ﬁltered elements (see Figure).\nIn RASP, the selection operation is provided through\nthe function select, which takes two s-ops k and q\nand a comparison operator and returns the composi-\ntion of sel (,, ) with k and q, with this sequence-to-\nmatrix function referred to as a selector. For exam-\nple: a=select(indices, indices, ) is a selector, and\na(\"hey\")=[[F, F, F],[T, F, F],[T, T, F]]. Similarly, the\naggregation operation is provided through aggregate,\nwhich takes one selector and one s-op and returns\nthe composition of agg with these. For example:\naggregate(a, indices+1)(\"hey\")=.4\nSimple select-aggregate examples To create the s-\nop that reverses any input sequence, we build a se-\nlector that requests for each query position the to-\nken at the opposite end of the sequence, and then\naggregate that selector with the original input to-\nkens: flip=select(indices, length-indices-1,==),\nand reverse=aggregate(flip, tokens). For example:\nflip(\"hey\") =\nreverse(\"hey\") = \"yeh\"\n\"a\" in our input, we build a selector that gathers infor-\nmation from all input positions, and then aggregate it\n4For convenience and efﬁciency, when averaging the ﬁl-\ntered values in an aggregation, for every position where only\na single value has been selected, RASP passes that value di-\nrectly to the output without attempting to average it. This\nof tokens between locations: for example, using the selector\nload1=select(indices,1,==), we may directly create the s-\nop aggregate(load1, tokens)(\"hey\")=\"eee\". Additionally, in\npositions when no values are selected, the aggregation simply re-\nturns a default value for the output (in Figure, we see this with\ndefault value 0), this value may be set as one of the inputs to the\naggregate function.\nwith a sequence broadcasting 1 wherever the input to-\nken is \"a\", and 0 everywhere else. This is expressed\nas select_all=select(1,1,==), and then frac_as =\naggregate(select_all,1 if tokens==\"a\" else 0).\nelementwise using boolean logic. For example, for\nload1=select(indices,1,==) and flip from above:\n(load1 or flip)(\"hey\") =\nselector width The ﬁnal operation in RASP is the powerful\nselector_width, which takes as input a single selector\nand returns a new s-op that computes, for each output\nposition, the number of input values which that selector has\nchosen for it. This is best understood by example: using\nthe selector same_token=select(tokens, tokens,==)\nthat ﬁlters for each query position the keys with\nthe same token as its own, we can compute its\nwidth to obtain a histogram of our input sequence:\nselector_width(same_token)(\"hello\")=.\nAdditional operations: While the above operations are\ntogether sufﬁcient to represent any RASP program, RASP\nfurther provides a library of primitives for common op-\nerations, such as in either of a value within a se-\nquence: (\"i\" in tokens)(\"hi\")=[T, T], or of each\nvalue in a sequence within some static list: tokens in\n[\"a\",\"b\",\"c\"])(\"hat\")=[F, T, F]. RASP also provides\nfunctions such as count, or sort.\n3.1. Relation to a Transformer\ninformation ﬂow of a transformer architecture, suggesting\nhow many heads and layers are needed to solve a task.\nThe built-in s-ops indices and tokens reﬂect\nthe initial input embeddings of a transformer,\nwhile length is computed in RASP: length=\naggregate(select_all, indicator(indices==0)),\nwhere select_all=select(1,1,==).\nElementwise Operations reﬂect the feed-forward sub-\nlayers of a transformer. These have overall not been re-\nstricted in any meaningful way: as famously shown by\nHornik et al. , MLPs such as those present in the\nfeed-forward transformer sub-layers can approximate with\narbitrary accuracy any borel-measurable function, provided\nsufﬁciently large input and hidden dimensions.\nmatrices, deﬁning for each input the selection (attention) pat-\nweighted averages, and aggregation reﬂects this ﬁnal averag-\ning operation. The uniform weights dictated by our selectors\nreﬂect an attention pattern in which unselected pairs are\nall given strongly negative scores, while the selected pairs\nall have higher, similar, scores. Such attention patterns are\nsupported by the ﬁndings of .\nselectors to be reused in multiple aggregations, abstract-\nattention heads in the compiled architecture. Making se-\nlectors ﬁrst class citizens also enables functions such as\nselector_width, which take selectors as parameters.\nAdditional abstractions All other operations, including\nthe powerful selector_width operation, are implemented\nin terms of the above primitives. selector_width in par-\none or two selectors, depending on whether or not one can\nassume a beginning-of-sequence token is added to the input\nsequence. Its implementation is given in Appendix.\nCompilation Converting an s-op to a transformer architec-\nture is as simple as tracing its computation ﬂow out from\nthe base s-ops. Each aggregation is an attention head, which\nmust be placed at a layer later than all of its inputs. El-\nementwise operations are feedforward operations, and sit\nin the earliest layer containing all of their dependencies.\nSome optimisations are possible: for example, aggregations\nmerged into the same attention head. A full\" compilation\nto concrete transformer weights requires to e. g. derive\nMLP weights for the elementwise operations, and is beyond\nthe scope of this work. RASP provides a method to visualize\nthis compiled ﬂow for any s-op and input pair: Figures 4\nand 5 were rendered using draw(reverse,\"abcde\") and\ndraw(hist,\" aabbaabb\").\n4. Implications and insights\nRestricted-Attention Transformers Multiple works pro-\nefﬁcient transformers, reducing the time complexity of each\nlayer from O(n2) to O(nlog(n)) or even O(n) with respect\nto the input sequence length n for a\nsurvey of such approaches). Several of these do so using\nsparse attention, in which the attention is masked using\ninteract ).\ncannot perform. In particular, these variants of transformers\nall impose restrictions on the selectors, permanently forcing\nsome of the n2 index pairs in every selector to False. But\ndoes this necessarily weaken these transformers?\nIn Appendix we present a sorting algorithm in RASP, ap-\nplicable to input sequences with arbitrary length and alpha-\nbet size5. This problem is known to require at Ω(nlog(n))\ntransformer can take full advantage of Ω(nlog(n)) of the\nn2 operations it performs in every attention head. It fol-\no(nlog(n)) operations incur a real loss in expressive power.\nSandwich Transformers Recently, Press et al. \nshowed that reordering the attention and feed-forward sub-\nmodeling tasks. In particular, they showed that: 1. pushing\nfeed-forward sublayers towards the bottom of a transformer\nweakened it; and 2. pushing attention sublayers to the bot-\ntom and feed-forward sublayers to the top strengthened it,\nprovided there was still some interleaving in the middle.\nThe base operations ofRASP help us understand the observa-\ntions of Press et al.. Any arrangement of a transformer s sub-\nlayers into a ﬁxed architecture imposes a restriction on the\nin a program compilable to that architecture. For example,\nan architecture in which all feed-forward sublayers appear\nbefore the attention sublayers, imposes that no elementwise\noperations may be applied to the result of any aggregation.\nIn RASP, there is little value to repeated elementwise op-\nerations before the ﬁrst aggregate: each position has only\nits initial input, and cannot generate new information. This\nexplains the ﬁrst observation of Press et al.. In contrast, an\narchitecture beginning with several attention sublayers i. e.,\nmultiple select-aggregate pairs will be able to gather\nthe computation, even if only by simple rules6. More com-\ngenerating new selectors, explaining the second observation.\nRecognising Dyck-kLanguages The Dyck-klanguages\nthe languages of sequences of correctly balanced parenthe-\nses, with k parenthesis types have been heavily used in\nconsidering the expressive power of RNNs .\nSuch investigations motivate similar questions for trans-\nformers, and several works approach the task. Hahn \nproves that transformer-encoders with hard attention can-\nnot recognise Dyck-2. Bhattamishra et al. and Yao\net al. provide transformer-encoder constructions\n5Of course, realizing this solution in real transformers requires\nsufﬁciently stable word and positional embeddings a practical\nlimitation that applies to all transformer variants.\n6While the attention sublayer of a transformer does do some\nvectors, it does not contain the powerful MLP with hidden layer as\nis present in the feed-forward sublayer.\nfor recognizing simpliﬁed variants of Dyck-k, though the\nsimpliﬁcations are such that no conclusion can be drawn\nfor unbounded depth Dyck-kwith k 1. Optimistically,\nEbrahimi et al. train a transformer-encoder with\ncausal attention masking to process Dyck-klanguages with\nreasonable accuracy for several k 1, ﬁnding that it learns\na stack-like behaviour to complete the task.\nWe consider Dyck- k using RASP, speciﬁcally deﬁning\nDyck-k-PTF as the task of classifying for every preﬁx of\na sequence whether it is legal, but not yet balanced ( P),\nbalanced (T), or illegal (F). We show that RASP can solve\nthis task in a ﬁxed number of heads and layers for any k,\npresenting our solution in Appendix.\nSymbolic Reasoning in Transformers Clark et al. \nshow that transformers are able to emulate symbolic reason-\ning: they train a transformer which, given the facts Ben\nis a bird\" and birds can ﬂy\", correctly validates that Ben\ncan ﬂy\". Moreover, they show that transformers are able to\nperform several logical steps: given also the fact that only\nwinged animals can ﬂy, their transformer conﬁrms that Ben\nhas wings. This ﬁnding however does not shed any light on\nhow the transformer is achieving such a feat.\nRASP empowers us to approach the problem on a high level.\nWe write a RASP program for the related but simpliﬁed\nproblem of containment and inference over sets of elements,\nsets, and logical symbols, in which the example is written\nas b B, x B x F, b F? (implementation available in\nour repository). The main idea is to store at the position of\nin that set, and at each element symbol the sets it is and\nis not contained in. Logical inferences are computed by\npassing information between symbols in the same fact,\nsymbols, which share their stored information.\nUse of Separator Tokens Clark et al. observe that\nmany attention heads in BERT (some-\ntimes) focus on separator tokens, speculating that these are\nused for no-ops\" in the computation. ﬁnd that transformers more successfully learn Dyck-\nbeginning-of-sequence (BOS) token, with the trained mod-\nparentheses. Our RASP programs suggest an additional\nrole that such separators may be playing: by providing a\nﬁxed signal from a neutral position, separators facilitate\nconditioned counting in transformers, that use the diffusion\nattending to. Without such neutral positions, counting re-\nquires an additional head, such that an agreed-upon position\n7We note that RASP does not suggest the embedding width\nneeded to encode this solution in an actual transformer.\nmay artiﬁcially be treated as neutral in one head and then\nindependently accounted for in the other.\nA simple example of this is seen in Figure. There,\nselector_width is applied with a BOS token, creating\nﬁrst input position (the BOS location) from all query\npositions, in addition to the actual positions selected\nby select(tokens, tokens,==). A full description of\nselector_width is given in Appendix.\n5. Experiments\nfronts: 1. its ability to upper bound the number of heads and\nlayers required to solve a task, 2. the tightness of that bound,\n3. its feasibility in a transformer, i. e., whether a sufﬁciently\nlarge transformer can encode a given RASP solution., train-\ning several transformers. We relegate the exact details of\nthe transformers and their training to Appendix.\nFor this section, we consider the following tasks:\n1. Reverse, e. g.: reverse(\"abc\")=\"cba\".\n2. Histograms, with a unique beginning-of-sequence\n(BOS) token (e. g., hist_bos(\" aba\")=)\nand without it (e. g., hist_nobos(\"aba\")=).\n3. Double-Histograms, with BOS: for each token, the\nas itself. E. g.: hist2(\" abbc\")=.\n4. Sort, with BOS: ordering the input tokens lexicograph-\nically. e. g.: sort(\" cba\")=\" abc\".\n5. Most-Freq, with BOS: returning the unique input to-\nkens in order of decreasing frequency, with original\nposition as a tie-breaker and the BOS token for padding.\nE. g.: most_freq(\" abbccddd\")=\" dbca \".\n6. Dyck-i PTF, for i = 1,2: the task of returning,\nat each output position, whether the input preﬁx up\nto and including that position is a legal Dyck- i se-\nquence (T), and if not, whether it can ( P) or cannot\n(F) be continued into a legal Dyck- i sequence. E. g:\nDyck1_ptf(\"()())\")=\"PTPTF\".\nWe refer to double-histogram as 2-hist, and to each Dyck-i\nPTF problem simply as Dyck-i. The full RASP programs for\nthese tasks, and the computation ﬂows they compile down\nto, are presented in Appendix. The size of the transformer\narchitecture each task compiles to is presented in Table.\nUpper bounding the difﬁculty of a task Given a RASP\nprogram for a task, e. g. double-histogram as described in\nFigure, we can compile it down to a transformer architec-\n8The actual optimal solution for Dyck-2 PTF cannot be realised\nin RASP as is, as it requires the addition of a select-best operator\nto the language reﬂecting the power afforded by softmax in the\ntransformer s self-attention. In this paper, we always refer to our\nanalysis of Dyck-2 with respect to this additional operation.\nLanguage Layers Heads Test Acc. Matches?\nReverse 2 1 99.99%\nHist BOS 1 1 100%\nHist no BOS 1 2 99.97%\nDouble Hist 2 2 99.58%\nSort 2 1 99.96%\nMost Freq 3 2 95.99%\nDyck-1 PTF 2 1 99.67%\nDyck-2 PTF 8 3 1 99.85%\nTable: Does a RASP program correctly upper bound the\nsolve a task? In the left columns, we show the compilation\nsize of our RASP programs for each considered task, and\nin the right columns we show the best (of 4) accuracies\nof transformers trained on these same tasks, and evaluate\nwhether their attention mechanisms appear to match (using a\nfor partially similar patterns: see Figure for an example).\nper layer, we report the maximum of these.\nture, effectively predicting the maximum number of layers\nand layer width (number of heads in a layer) needed to solve\nthat task in a transformer. To evaluate whether this bound is\ntruly sufﬁcient for the transformer, we train 4 transformers\nof the prescribed sizes on each of the tasks.\neach task in Table. Most of these transformers reached\naccuracies of 99.5% and over, suggesting that the upper\nbounds obtained by our programs are indeed sufﬁcient for\nsolving these tasks in transformers. For some of the tasks,\nwe even ﬁnd that the RASP program is the same as or very\nsimilar to the natural solution found by the trained trans-\nformer. In particular, Figures 4 and 5 show a strong simi-\nfor the tasks Reverse and Histogram-BOS, though the trans-\nmechanism for computing length than that given in RASP.\npredicted by our compilation, and observing the drop-off in\ntest accuracy. Speciﬁcally, we repeat our above experiments,\nbut this time we also train each task on up to 4 different\nsizes. In particular, denoting L, H the number of layers and\nheads predicted by our compiled RASP programs, we train\nfor each task transformers with sizes (L, H), (L 1, H),\n(L, H 1), and (L 1,2H) (where possible) 9.\n9The transformers of size (L 1, 2H) are used to validate that\nlayers, as opposed to the reduction in total heads that this entails.\nHowever, doubling H means the embedding dimension will be\ndivided over twice as many heads. To counteract any negative\neffect this may have, we also double the embedding dimension for\nopp_index = length - indices - 1;\nflip = select ( indices, opp_index,==);\nreverse = aggregate (flip, tokens );\nFigure: Top: RASP code for computing reverse\n(e. g., reverse(\"abc\")=\"cba\"). Below, its compila-\ntion to a transformer architecture (left, obtained through\ndraw(reverse,\"abcde\") in the RASP REPL), and the at-\n(right), both visualised on the same input. Visually, the atten-\nthe program. The head in the ﬁrst layer, however, appears\nto have learned a different solution from our own: instead\nof focusing uniformly on the entire sequence (as is done\nin the computation of length in RASP ), this head shows a\npreference for the last position in the sequence.\nthese architectures in Table. For most of the tasks, the\ntransformers fail completely to learn their target languages.\nThe main exception to this is sort, which appears unaffected\nby the removal of one layer, and even achieves its best results\nin this case. Drawing the attention pattern for the single-\npatterns. It appears that the transformer has learned to take\nadvantage of the bounded input alphabet size, effectively\nthese transformers.\nsame_tok = select (tokens, tokens, ==);\nhist = selector_width ( same_tok,\nassume_bos = True );\nFigure: The RASP program for computing with-BOS histograms (left), alongside its compilation to a transformer\narchitecture (cream boxes) and the attention head (center bottom) of a transformer trained on the same task, without attention\nsupervision. The compiled architecture and the trained head are both presented on the same input sequence, \" aabbaabb\".\nThe transformer architecture was generated in the RASP REPL using draw(hist,\" aabbaabb\").\nLanguage RASP Average test accuracy (%) with...\nL, H L, H H 1 L 1 L 1, 2H\nReverse 2, 1 99.9 - 23.1 41.2\nMost Freq 3, 2 93.9 92.1 84.0 90.2\nTable: Accuracy dropoff in transformers when reducing\nRASP solutions for the same tasks. The transformers trained\non the size predicted by RASP have very high accuracy, and\nin most cases there is a clear drop as that size is reduced.\nCases creating an impossible architecture (H or Lzero) are\nmarked with -. Histogram with BOS uses only 1 layer and\nhead, and so is not included. As in Table, Dyck- 2 is\nconsidered with the addition of select_best to RASP.\nimplementing bucket sort for its task. This is because a\nsingle full-attention head is sufﬁcient to compute for every\ntoken its total appearances in the input, from which the\ncorrect output can be computed locally at every position.\nRASP program can indeed be represented in a transformer.\nFor this, we return to the tougher tasks above, and this time\ntrain the transformer with an additional loss component en-\ncompiled solution (i. e., we supervise the attention patterns\nin addition to the target output). In particular, we consider\nthe tasks double-histogram, sort, and most-freq, all with\nthe assumption of a BOS token in the input. After train-\ning each transformer for 250 epochs with both target and\nattention supervision, they all obtain high test accuracies on\nthe task (99+%), and appear to encode attention patterns\nsimilar to those compiled from our solutions. We present\nthe obtained patterns for double-histogram, alongside the\ncompiled RASP solution, in Figure. We present its full\ncomputation ﬂow, as well as the learned attention patterns\nand full ﬂow of sort and most-freq, in Appendix.\n6. Conclusions\nWe abstract the computation model of the transformer-\nencoder as a simple sequence processing language, RASP,\nthat captures the unique constraints on information ﬂow\npresent in a transformer. Considering computation prob-\ndetails of a neural network in favor of symbolic programs.\nto realise it in a transformer. We show several examples\nof programs written in the RASP language, showing how\noperations can be implemented by a transformer, and train\nseveral transformers on these tasks, ﬁnding that RASP helps\nto solve them. Additionally, we use RASP to shed light on\nan empirical observation over transformer variants, and ﬁnd\nconcrete limitations for some efﬁcient transformers.\nAcknowledgments We thank Uri Alon, Omri Gilad, Daniel\nFilan, and the reviewers for their constructive comments.\nCouncil under European Union s Horizon 2020 research &\ninnovation programme, agreement #802774 (iEXTRACT).\n            \n            Each agent should:\n            1. Read and understand the paper from your role's perspective\n            2. Identify key points relevant to your expertise\n            3. Prepare questions or concerns to discuss\n            4. Consider the implications from your viewpoint\n            \n            Language: Spanish\n            ",
      "expected_output": "Initial analysis and key points from each role's perspective",
      "agent_role": "Coordinator"
    },
    {
      "description": "\n            Based on the initial analysis, conduct a thorough discussion of the paper.\n            \n            The discussion should:\n            1. Cover all major points of the paper\n            2. Include different perspectives from each role\n            3. Address potential concerns and criticisms\n            4. Explore implications and applications\n            5. Be engaging and conversational\n            \n            Create a rich dialogue that would be interesting for a podcast audience.\n            Language: Spanish\n            ",
      "expected_output": "Comprehensive discussion transcript with multiple perspectives",
      "agent_role": "Critical Thinker"
    },
    {
      "description": "\n            Transform the discussion into a relaxed educational lecture text.\n            \n            The script should be in the style of popular science educators like 3Blue1Brown:\n            1. Written as a SINGLE EDUCATOR speaking directly to the listener (use \"tú\"/\"usted\")\n            2. Use analogies and accessible explanations\n            3. Include all key insights from the discussion\n            4. Be engaging and educational, not just informative\n            5. Flow naturally from concept to concept with smooth transitions\n            6. Include moments of wonder and intellectual curiosity\n            7. Break down complex ideas into digestible parts\n            8. Use a teaching tone that makes the listener feel they're learning something fascinating\n            9. Write as continuous text ready to be read by a voice actor\n            10. NO section headers, NO subheaders, NO formatting marks\n            11. Don't address the public with greetings or goodbyes, but make questions\n            12. Always end up with questions for the reader and practical implications\n            13. Write as plain text that flows naturally for voice reading\n            14. NO [PAUSES], NO [MUSIC], NO stage directions - just the educational content\n            15. CRITICAL: Address the listener directly - \"puedes imaginar\", \"si consideras\", \"te darás cuenta\"\n            16. DO NOT write as if summarizing a discussion - write as if YOU are the teacher\n            17. Avoid phrases like \"los expertos discutieron\" or \"el equipo concluyó\"\n            \n            \n            ACCESSIBLE LEVEL REQUIREMENTS:\n            15. Focus on core concepts and main findings rather than technical details\n            16. Use everyday analogies to explain complex ideas\n            17. Emphasize practical implications and real-world applications\n            18. Keep technical jargon to a minimum, always explaining when used\n            19. Focus on the \"why this matters\" rather than the \"how they did it\"\n            20. Make connections to things the audience already understands\n            \n            \n            \n            DURATION REQUIREMENT: EXACTLY 20 minutes of content (3000-3200 words) - THIS IS ABSOLUTELY MANDATORY\n            - You MUST produce 4X more content than a 5-minute version\n            - This should be a COMPREHENSIVE, IN-DEPTH analysis\n            - MANDATORY SECTIONS TO INCLUDE:\n              * Detailed background and context (300-400 words)\n              * Comprehensive methodology explanation (500-600 words)\n              * Thorough results analysis with specific numbers (800-900 words)\n              * Extensive discussion of implications (500-600 words)\n              * Detailed limitations and future research (400-500 words)\n              * Practical applications and broader significance (500-600 words)\n            - Each finding must be explored in detail with multiple perspectives\n            - Include extensive background context and theoretical framework\n            - Discuss methodology, limitations, and alternative interpretations thoroughly\n            - Provide multiple examples, analogies, and real-world applications\n            - Include detailed discussion of implications and future directions\n            - Allow for deeper exploration of related concepts and broader significance\n            - This should feel like a comprehensive academic lecture, not a summary\n            \n            \n            Language: Spanish\n            ",
      "expected_output": "Clean educational text ready for voice actor reading",
      "agent_role": "Educational Writer"
    },
    {
      "description": "\n            FINAL STEP: Transform the educational content into a PERFECT voice-ready script.\n            \n            CRITICAL: Verify the content meets the 20-minute target (3000-3200 words). If it's too short, EXPAND it significantly.\n            CRITICAL: Ensure technical level is accessible - keep accessible but thorough.\n            \n            MANDATORY VOICE OPTIMIZATION REQUIREMENTS:\n            1. Create a SINGLE, CONTINUOUS text ready for a voice actor to read\n            2. NO markdown formatting, NO headers, NO bullet points, NO lists\n            3. Convert ALL content into natural, flowing sentences\n            4. Replace any remaining bullet points with complete sentences\n            5. Ensure PERFECT flow from sentence to sentence\n            6. Remove ANY formatting marks: *, #, -, •, etc.\n            7. Make sure sentences are not too long or complex for voice delivery\n            8. Use natural speech patterns and rhythm\n            9. Include natural transitions between concepts (\"Ahora consideremos...\", \"Lo que resulta particularmente interesante es...\")\n            10. NO stage directions, NO [PAUSES], NO [MUSIC], NO technical annotations\n            11. NO greetings or goodbyes - start directly with content\n            12. End with thought-provoking questions and practical implications\n            13. If content is too short, SIGNIFICANTLY EXPAND with more detail and depth\n            14. This must be PUBLICATION-READY text that a voice actor can read smoothly\n            15. Every word should sound natural when spoken aloud\n            16. CRITICAL: Write in SECOND PERSON (tú/usted) - address the listener directly\n            17. DO NOT refer to \"we researchers\", \"our discussion\", \"the team analyzed\" - this is NOT a research report\n            18. Instead use: \"si consideras\", \"puedes ver que\", \"imagina que\", \"te preguntarás\", etc.\n            19. Write as if a SINGLE EDUCATOR is teaching directly to the listener\n            20. Remove any references to \"coordinators\", \"discussions between experts\", or \"our analysis\"\n            21. This should sound like ONE VOICE teaching, not a summary of multiple voices\n            \n            CRITICAL: This is the FINAL version that will be published. Make it PERFECT.\n            \n            Language: Spanish\n            ",
      "expected_output": "FINAL publication-ready voice script (3000-3200 words)",
      "agent_role": "Voice Director"
    }
  ]
}