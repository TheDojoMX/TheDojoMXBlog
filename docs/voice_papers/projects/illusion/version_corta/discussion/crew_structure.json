{
  "project_name": "version_corta",
  "paper_title": "The Illusion Of Thinking",
  "language": "Spanish",
  "agents": [
    {
      "role": "Coordinator",
      "goal": "Coordinate the discussion and ensure all perspectives are heard",
      "backstory": "You are an experienced moderator who ensures productive discussions"
    },
    {
      "role": "Scientific Reviewer",
      "goal": "Verify the soundness and methodology of the paper",
      "backstory": "You are a rigorous scientist who evaluates research methodology and conclusions"
    },
    {
      "role": "Critical Thinker",
      "goal": "Question assumptions and challenge ideas presented",
      "backstory": "You are a skeptical academic who questions everything and looks for flaws"
    },
    {
      "role": "Educational Writer",
      "goal": "Create engaging educational content in the style of popular science educators",
      "backstory": "You are a skilled science communicator who explains complex topics in an accessible, engaging way like 3Blue1Brown or other popular educators"
    },
    {
      "role": "Voice Director",
      "goal": "Transform content into perfect voice-ready script for publication",
      "backstory": "You are a master voice coach and script editor who specializes in creating flawless, publication-ready scripts that voice actors can read naturally. You ensure every word flows perfectly when spoken aloud."
    },
    {
      "role": "AI Researcher",
      "goal": "Provide technical insights on AI methodology and implications",
      "backstory": "You are an AI researcher with deep technical knowledge"
    },
    {
      "role": "AI Philosopher",
      "goal": "Discuss philosophical implications of AI research",
      "backstory": "You are a philosopher specializing in AI ethics and implications"
    },
    {
      "role": "AI Doomer",
      "goal": "Raise concerns about potential risks and negative consequences",
      "backstory": "You are concerned about AI safety and potential existential risks"
    },
    {
      "role": "AI Enthusiast",
      "goal": "Highlight positive potential and applications",
      "backstory": "You are optimistic about AI's potential to solve problems"
    },
    {
      "role": "AI Newcomer",
      "goal": "Ask basic questions that others can answer",
      "backstory": "You know little about AI but are curious and ask good questions"
    }
  ],
  "tasks": [
    {
      "description": "\n            Analyze the paper titled \"The Illusion Of Thinking\" and provide your perspective.\n            \n            Paper content:\n            The Illusion of Thinking:\n(LRMs) that generate detailed thinking processes before providing answers. While these models\ndemonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scal-\ning properties, and limitations remain insufficiently understood. Current evaluations primarily fo-\ncus on established mathematical and coding benchmarks, emphasizing final answer accuracy. How-\never, this evaluation paradigm often suffers from data contamination and does not provide insights\ninto the reasoning traces structure and quality. In this work, we systematically investigate these\ngaps with the help of controllable puzzle environments that allow precise manipulation of composi-\ntional complexity while maintaining consistent logical structures. This setup enables the analysis\nof not only final answers but also the internal reasoning traces, offering insights into how LRMs\nthink. Through extensive experimentation across diverse puzzles, we show that frontier LRMs\nface a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counter-\nintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then\ndeclines despite having an adequate token budget. By comparing LRMs with their standard LLM\ncounterparts under equivalent inference compute, we identify three performance regimes: (1) low-\ncomplexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity\ntasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks\nwhere both models experience complete collapse. We found that LRMs have limitations in exact\ncomputation: they fail to use explicit algorithms and reason inconsistently across puzzles. We\nalso investigate the reasoning traces in more depth, studying the patterns of explored solutions\nand analyzing the models computational behavior, shedding light on their strengths, limitations,\nand ultimately raising crucial questions about their true reasoning capabilities.\nLarge Language Models (LLMs) have recently evolved to include specialized variants explicitly\ndesigned for reasoning tasks Large Reasoning Models (LRMs) such as OpenAI s o1 o3 ,\nDeepSeek-R1 , Claude 3.7 Sonnet Thinking , and Gemini Thinking . These models are new\nartifacts, characterized by their thinking mechanisms such as long Chain-of-Thought (CoT) with\nself-reflection, and have demonstrated promising results across various reasoning benchmarks. Their\nEqual contribution.\nWork done during an internship at Apple.\np_shojaee, imirzadeh, kalizadehvahid, mchorton, bengio, farajtabar @apple. com\nP eg 1 P eg 2\n3T ar get Stat e\nMove disk 1 from peg 0 to peg 2...\nLet me double-check this...\nanswer the final answer is moves=...\n(f or analysis)\n(f or measuring accur acy)\nComplexity (number of disks)\n100Accuracy (%)\nClaude 3.7\n(+thinking)\nClaude 3.7\nComplexity (number of disks)\n20,000Response Length (Tokens)\nClaude 3.7\n(+thinking)\nClaude 3.7\nComplexity (number of disks)\nFigure: Top: Our setup enables verification of both final answers and intermediate reasoning traces,\nallowing detailed analysis of model thinking behavior. Bottom left & middle: At low complexity,\nnon-thinking models are more accurate and token-efficient. As complexity increases, reasoning models\noutperform but require more tokens until both collapse beyond a critical threshold, with shorter\ntraces. Bottom right: For correctly solved cases, Claude 3.7 Thinking tends to find answers early\nat low complexity and later at higher complexity. In failed cases, it often fixates on an early wrong\nanswer, wasting the remaining token budget. Both cases reveal inefficiencies in the reasoning process.\nand problem-solving tasks, with some researchers proposing them as significant steps toward more\ngeneral artificial intelligence capabilities.\nDespite these claims and performance advancements, the fundamental benefits and limitations of\nLRMs remain insufficiently understood. Critical questions still persist: Are these models capable\nof generalizable reasoning, or are they leveraging different forms of pattern matching ? How\ndoes their performance scale with increasing problem complexity? How do they compare to their\nnon-thinking standard LLM counterparts when provided with the same inference token compute?\nMost importantly, what are the inherent limitations of current reasoning approaches, and what\nimprovements might be necessary to advance toward more robust reasoning capabilities?\ncurrent evaluation paradigms. Existing evaluations predominantly focus on established mathematical\nand coding benchmarks, which, while valuable, often suffer from data contamination issues and do\nnot allow for controlled experimental conditions across different settings and complexities. Moreover,\nthese evaluations do not provide insights into the structure and quality of reasoning traces. To\nunderstand the reasoning behavior of these models more rigorously, we need environments that\nenable controlled experimentation.\nIn this study, we probe the reasoning mechanisms of frontier LRMs through the lens of problem\ncomplexity. Rather than standard benchmarks (e. g., math problems), we adopt controllable puzzle en-\nthe core logic and inspect both solutions and internal reasoning (Fig. 1, top). These puzzles: (1) of-\nfer fine-grained control over complexity; (2) avoid contamination common in established benchmarks;\n(3) require only the explicitly provided rules, emphasizing algorithmic reasoning; and (4) support\nrigorous, simulator-based evaluation, enabling precise solution checks and detailed failure analyses.\n(LRMs): First, despite their sophisticated self-reflection mechanisms learned through reinforcement\nlearning, these models fail to develop generalizable problem-solving capabilities for planning tasks,\nwith performance collapsing to zero beyond a certain complexity threshold. Second, our comparison\nbetween LRMs and standard LLMs under equivalent inference compute reveals three distinct reason-\ning regimes (Fig. 1, bottom). For simpler, low-compositional problems, standard LLMs demonstrate\ngreater efficiency and accuracy. As problem complexity moderately increases, thinking models gain\nan advantage. However, when problems reach high complexity with longer compositional depth,\nboth model types experience complete performance collapse (Fig. 1, bottom left). Notably, near\nthis collapse point, LRMs begin reducing their reasoning effort (measured by inference-time tokens)\nas problem complexity increases, despite operating well below generation length limits (Fig. 1,\nbottom middle). This suggests a fundamental inference time scaling limitation in LRMs reasoning\ncapabilities relative to problem complexity. Finally, our analysis of intermediate reasoning traces or\nthoughts reveals complexity-dependent patterns: In simpler problems, reasoning models often identify\nphenomenon. At moderate complexity, correct solutions emerge only after extensive exploration\nof incorrect paths. Beyond a certain complexity threshold, models completely fail to find correct\nsolutions (Fig. 1, bottom right). This indicates LRMs possess limited self-correction capabilities\nthat, while valuable, reveal fundamental inefficiencies and clear scaling limitations.\nThese findings highlight both the strengths and limitations of existing LRMs, raising questions\ndeployment. Our key contributions are:\ncontrollable experimentation with respect to problem complexity.\nWe show that state-of-the-art LRMs (e. g., o3-mini, DeepSeek-R1, Claude-3.7-Sonnet-Thinking)\nstill fail to develop generalizable problem-solving capabilities, with accuracy ultimately collapsing\nto zero beyond certain complexities across different environments.\ncomplexity, evidenced by the counterintuitive decreasing trend in the thinking tokens after a\ncomplexity point.\nto intermediate solutions of thinking traces with the help of deterministic puzzle simulators. Our\nanalysis reveals that as problem complexity increases, correct solutions systematically emerge at\nlater positions in thinking compared to incorrect ones, providing quantitative insights into the\nself-correction mechanisms within LRMs.\nWe uncover surprising limitations in LRMs ability to perform exact computation, including their\nfailure to benefit from explicit algorithms and their inconsistent reasoning across puzzle types.\nReasoning in Language Models. Large Language Models (LLMs) undergo multiple costly\ntraining phases using vast amounts of training data. While these LLMs demonstrate promising\nlanguage understanding with strong compression capabilities, their intelligence and reasoning abilities\nremain a critical topic of scientific debate . Earlier iterations of LLMs exhibited\npoor performance on reasoning benchmarks . To address these shortcomings, several\ndata and test-time computation. For instance, generating a Chain of Thought (CoT) \nand incorporating self-verification prior to the final answer have been shown to improve\nmodel performance. However, obtaining high-quality and scalable CoT data is quite expensive\ndue to its scarcity. Another line of research focuses on compensating for the lack of supervised\nlearning . A notable open-source example of these improvements is Deepseek-\nR1 , which demonstrated that applying RL with verifiable rewards can significantly enhance model\nperformance, matching that of closed models like OpenAI s o1 , leading to a new generation of\nlanguage models referred to as Large Reasoning Models (LRMs) such as Gemini flash thinking ,\nClaude 3.7 Sonnet thinking , etc.\nUnderstanding Large Reasoning Models. Recent studies have explored various aspects of\nreasoning behavior: Large Reasoning Models have shown emergent behaviors such as discrepancy\nbetween thought traces and final answers as well as efficiency concerns through what\nresearchers term the overthinking phenomenon , where models produce verbose,\nredundant outputs, even after finding the solution, creating significant inference computational\noverhead. In this work, we systematically analyze how much model thinks w. r. t task complexity.\nRecently, Ballon et al. demonstrated that in newer LRMs accuracy generally declines when\nthinking increases in math problems, in contrast we observe when in controlled puzzle environment\ntask complexity only happens up to some threshold. Yue et al. questioned whether reinforcement\nlearning truly elicits novel reasoning patterns and shows pass@k of reasoning vs non-reasoning models\nconverge to the same point. We also observe that in MATH-500 pass@k is close for reasoning versus\nnon-reasoning models but we observed different patterns under medium and high complexity of\npuzzles, which is not easily observable on established math benchmarks used in common evaluations.\nControllable Evaluation Environments. Unlike earlier studies that focused on mathematical\nproblems to evaluate the reasoning capabilities of language models, this work introduces controllable\npuzzle environments. These environments allow for precise manipulation of problem complexity while\nmaintaining consistent logical processes, enabling a more rigorous analysis of reasoning patterns and\nlimitations. Controllable environments are not uncommon in the literature . However,\nour primary aim is not to propose a new benchmark; instead, we use these benchmarks as tools\nfor designing experiments to understand the reasoning capabilities of language models. A closely\nrelated study by Valmeekam et al. demonstrated that o1-models show significant performance\nimprovements compared to previous models. Our work offers additional insights, such as examining\npairs of thinking non-thinking models (e. g., DeepSeek-R1 V3, Claude 3.7 Sonnet thinking non-\nthinking). Furthermore, we study the reasoning traces of the LRMs in more depth, revealing different\nbehaviors across various complexity levels.\nOverall, the promising results from recent LRMs raise a critical question: how much have the\npreviously reported limitations of LLMs been improved? In this work, we move beyond merely\nmeasuring the performance of these LRMs. We analyze how well these LRMs tackle problems of\nvarying complexities and examine the properties of their reasoning processes.\nInference Compute Budget (Tokens)\nclaude-3-7-sonnet-thinking\nclaude-3-7-sonnet-no-thinking\nInference Compute Budget (Tokens)\nclaude-3-7-sonnet-thinking\nclaude-3-7-sonnet-no-thinking\nInference Compute Budget (Tokens)\nclaude-3-7-sonnet-thinking\nclaude-3-7-sonnet-no-thinking\nInference Compute Budget (Tokens)\nDeepSeek-R1\nDeepSeek-V3\nInference Compute Budget (Tokens)\nDeepSeek-R1\nDeepSeek-V3\nInference Compute Budget (Tokens)\nDeepSeek-R1\nDeepSeek-V3\nFigure: Comparative analysis of thinking versus non-thinking models across math benchmarks\nreveals inconsistent performance patterns. While results on the MATH-500 dataset show comparable\nperformance between both model types, the thinking models demonstrate superior performance\non AIME24 and AIME25 benchmarks. Additionally, the observed performance degradation from\nAIME24 to AIME25 highlights the vulnerability of these benchmarks to data contamination issues.\nCurrently, it is not clear whether the performance enhancements observed in recent RL-based\ndata, to the significantly greater inference compute allocated to thinking tokens, or to reasoning\ncapabilities developed by RL-based training? Recent studies have explored this question\nwith established math benchmarks by comparing the upper-bound capabilities (pass@k) of RL-based\nthinking models with their non-thinking standard LLM counterparts. They have shown that under\nequivalent inference token budgets, non-thinking LLMs can eventually reach performance comparable\nto thinking models on benchmarks like MATH500 and AIME24 . We also conducted our\ncomparative analysis of frontier LRMs likeClaude-3.7-Sonnet (with vs. without thinking) and\nDeepSeek (R1 vs. V3). Our results (shown in Fig. 2) confirm that, on the MATH500 dataset, the\npass@k performance of thinking models is comparable to their non-thinking counterparts when\nprovided with the same inference token budget. However, we observed that this performance gap\nwidens on the AIME24 benchmark and widens further on AIME25. This widening gap presents\nan interpretive challenge. It could be attributed to either: (1) increasing complexity requiring\nmore sophisticated reasoning processes, thus revealing genuine advantages of the thinking models\nfor more complex problems, or (2) reduced data contamination in newer benchmarks (particularly\nAIME25). Interestingly, human performance on AIME25 was actually higher than on AIME24\n, suggesting that AIME25 might be less complex. Yet models perform worse on AIME25\nthan AIME24 potentially suggesting data contamination during the training of frontier LRMs.\nGiven these non-justified observations and the fact that mathematical benchmarks do not allow for\ncontrolled manipulation of problem complexity, we turned to puzzle environments that enable more\nprecise and systematic experimentation.\nFigure: Illustration of the four puzzle environments. Columns show the progression frominitial\nstate (top)through intermediate state (middle)to target state (bottom)for puzzles: Tower\nof Hanoi (disk transfer across pegs), Checkers Jumping (position swapping of colored tokens), River\nCrossing (transporting entities across a river), and Blocks World (stack reconfiguration).\n3.1 Puzzle Environments\nWe evaluate LRM reasoning on four controllable puzzles spanning compositional depth, planning\ncomplexity, and distributional settings. The puzzles are defined below and illustrated in Fig. 3.\npeg in size order (largest at bottom). The goal is to transfer all disks from the first peg to the third\npeg. Valid moves include moving only one disk at a time, taking only the top disk from a peg, and\nnever placing a larger disk on top of a smaller one. The difficulty in this task can be controlled by\n2n 1. However, in this work we do not grade for optimality of final solution and only measuring\nthe correctness of each move and reaching the target state.\nChecker Jumpingis a one-dimensional puzzle arranging red checkers, blue checkers, and a single\nempty space in a line. The objective is to swap the positions of all red and blue checkers, effectively\nmirroring the initial configuration. Valid moves include sliding a checker into an adjacent empty\nspace or jumping over exactly one checker of the opposite color to land in an empty space. No checker\ncan move backward in the puzzle process. The complexity of this task can be controlled by the\nnumber of checkers: with2n checkers, the minimum number of moves required will be(n + 1)2 1.\nn agents who must cross a river using a boat. The goal is to transport all2n individuals from the\nleft bank to the right bank. The boat can carry at mostk individuals and cannot travel empty.\npresent, as each agent must protect their client from competing agents. The complexity of this task\ncan also be controlled by the number of actor agent pairs present. Forn = 2, n= 3 pairs, we use\nboat capacity ofk = 2 and for larger number of pairs we usek = 3.\nBlocks Worldis a block-stacking puzzle requiring rearrangement of blocks from an initial configu-\nration into a specified goal configuration. The objective is to find the minimum number of moves\nneeded for this transformation. Valid moves are restricted to the topmost block of any stack, which\ncan be placed either on an empty stack or on top of another block. The complexity in this task can\nbe controlled by the number of blocks present.\nFigure: Accuracy of thinking models (Claude 3.7 Sonnet with thinking, DeepSeek-R1) versus their\nnon-thinking counterparts (Claude 3.7 Sonnet, DeepSeek-V3) across all puzzle environments and\nvarying levels of problem complexity.\nExperiments & Results\n4.1 Experimental Setup\nMost of our experiments are conducted on reasoning models and their non-thinking counterparts,\nsuch as Claude 3.7 Sonnet (thinking non-thinking) and DeepSeek-R1 V3. We chose these models\nbecause they allow access to the thinking tokens, unlike models such as OpenAI s o-series. For\nexperiments focused solely on final accuracy, we also report results on the o-series models. For Claude\n3.7 Sonnet models, we allow the maximum token budget (64k). Similarly, for DeepSeek-R1 V3\nmodels on local servers, we allow the maximum length to be up to64k tokens. For each puzzle\ninstance, we generate 25 samples and report the average performance of each model across them.\nComprehensive details of our experimental setup and results are provided in the Appendix.\n4.2 How Does Complexity Affect Reasoning?\n4.2.1 Three Regimes of Complexity\nMotivatedbytheobservationsinFig.2, tosystematicallyinvestigatetheimpactofproblemcomplexity\non reasoning behavior, we conducted experiments comparingthinking and non-thinking model\npairs across our controlled puzzle environments. Our analysis focused on matched pairs of LLMs\nwith identical model backbones, specificallyClaude-3.7-Sonnet (w. vs. w o thinking)and DeepSeek\n(R1 vs. V3). In each puzzle, we vary the complexity by manipulating problem sizeN (representing\ndisk count, checker count, block count, or crossing elements).\nFig. 4 presents the accuracy of both model types as a function of problem complexity across all\npuzzle environments. Complementing this, Fig. 5 shows the upper bound performance capabilities\n(pass@k) of these model pairs under equivalent inference token compute (averaged across all puzzles),\nextending earlier analyses from mathematical benchmarks (Fig. 2) to the controlled puzzle environ-\nments. Results from both these figures demonstrate that, unlike observations from math, there exists\nthree regimesin the behavior of these models with respect to complexity. In thefirst regime where\nproblem complexity is low, we observe that non-thinking models are capable to obtain performance\ncomparable to, or even better than thinking models with more token-efficient inference. In the\nFigure: Pass@k performance of thinking vs. non-thinking models across equivalent compute\nbudgets in puzzle environments oflow, medium, and high complexity. Non-thinking models excel\nin simple problems, thinking models show advantages at medium complexity, while both approaches\nfail at high complexity regardless of compute allocation.\nsecond regime with medium complexity, the advantage of reasoning models capable of generating\nlong chain-of-thought begin to manifest, and the performance gap between model pairs increases. The\nof both models have collapsed to zero. Results show that while thinking models delay this collapse,\nthey also ultimately encounter the same fundamental limitations as their non-thinking counterparts.\n4.2.2 Collapse of Reasoning Models\nto increasing problem complexity. Our experiments evaluate five state-of-the-art thinking models:\no3-mini (medium and high configurations), DeepSeek-R1, DeepSeek-R1-Qwen-32B, andClaude-3.7-\nSonnet (thinking). Fig. 6 demonstrates these models performance in terms of accuracy (top) and\nthinking token usage (bottom) across varying complexity levels. Results show that all reasoning\nmodels exhibit a similar pattern with respect to complexity: accuracy progressively declines as\nproblem complexity increases until reaching complete collapse (zero accuracy) beyond a model-\nspecific complexity threshold. Analysis of inference thinking token compute also reveals an intriguing\npattern in thinking token allocation learned by these models. We observe that reasoning models\ninitially increase their thinking tokens proportionally with problem complexity. However, upon\ncounterintuitively begin to reduce their reasoning effort despite increasing problem difficulty. This\nphenomenon is most pronounced in o3-mini variants and less severe in the Claude-3.7-Sonnet\n(thinking) model. Notably, despite operating well below their generation length limits with ample\ninference budget available, these models fail to take advantage of additional inference compute during\nthe thinking phase as problems become more complex. This behavior suggests a fundamental scaling\nlimitation in the thinking capabilities of current reasoning models relative to problem complexity.\nFigure: Accuracy and thinking tokens vs. problem complexity for reasoning models across puzzle\nenvironments. As complexity increases, reasoning models initially spend more tokens while accuracy\ndeclines gradually, until a critical point where reasoning collapses performance drops sharply and\nreasoning effort decreases.\n4.3 What Happens Inside the Thoughts of Reasoning Models?\nTo gain deeper insights into the thinking processes of reasoning models, we conducted a fine-grained\nanalysis of their reasoning traces. As shown in Fig. 1, our setup with puzzle environments allows us\nto look beyond final answer and obtain more detailed insight into the reasoning traces ( thoughts )\nproduced by these models. We extract and analyze the intermediate solutions exploredwithin the\nthoughts of a model with the help of puzzle simulators. Our investigation examines the patterns and\ncharacteristics of these intermediate solutions, their correctness relative to their sequential position\nin the reasoning process, and how these patterns evolve with increasing problem complexity. For\nthis analysis, we focus on the reasoning traces generated byClaude-3.7-Sonnet-Thinking across\nour puzzle suite. For each intermediate solution identified within the traces, we recorded: (1) its\nrelative position within the reasoning trace (normalized by total thought length), (2) its correctness\nas validated by our puzzle simulators, and (3) the complexity of the corresponding problem. This\nFig. 7a demonstrates the relation between the position of intermediate solutions within thoughts, their\ncorrectness, and problem complexity across all puzzle environments. Our analysis from reasoning\ntraces also further validates three regimes of complexity discussed above. For simpler problems,\nincorrect solutions. Note the distribution of incorrect solutions (red) is shifted more upward towards\nend of thinking compared to correct solutions (green). This phenomenon, referred to as overthinking\nin the literature, leads to the waste of compute. As problems become moderately more complex,\nthis trend reverses: models first explore incorrect solutions and mostly later in thought arrive at\nthe correct ones. This time the distribution of incorrect solutions (red) is shifted more downward\ncompared to correct ones (green). Finally, for the problems with higher complexity, collapse emerges,\nPosition in Thinking (Token)\n100Solution Accuracy (%)\nFigure: Left & Middle: Position and correctness of intermediate solutions within reasoning traces\nacross four puzzles at varying complexity levels. indicates correct solutions, indicates incorrect\nsolutions, with distribution density shown by shading; Right: Solution accuracy versus position\nin thinking for Tower of Hanoi at different complexity levels. Simple problems (N=1-3) show early\naccuracy declining over time (overthinking), moderate problems (N=4-7) show slight improvement\nin accuracy with continued reasoning, and complex problems (N 8) exhibit consistently near-zero\naccuracy, indicating complete reasoning failure.\nmeaning that the model fails to generate any correct solutions within the thought.\nFig. 7b presents a complementary analysis of solution accuracy within sequential segments (bins)\nof the thoughts in the Tower of Hanoi environment. It can be observed that for simpler problems\n(smaller N), solution accuracy tends to decrease or oscillate as thinking progresses, providing further\nevidence of the overthinking phenomenon. However, this trend changes for more complex problems,\nwhere solution accuracy increases with thinking progression up to a certain threshold. Beyond this\ncomplexity threshold, in the collapse mode, accuracy is zero.\n4.4 Open Questions: Puzzling Behavior of Reasoning Models\nIn this section, we present surprising results concerning the limitations of reasoning models in\nexecuting exact problem-solving steps, as well as demonstrating different behaviors of the models\nbased on the number of moves.\nAs shown in Figures 8a and 8b, in the Tower of Hanoi environment, even when we provide the\ndoes not improve, and the observed collapse still occurs at roughly the same point. This is noteworthy\nbecause finding and devising a solution should require substantially more computation (e. g., for search\nand verification) than merely executing a given algorithm. This further highlights the limitations of\nreasoning models in verification and in following logical steps to solve a problem, suggesting that\nfurther research is needed to understand the symbolic manipulation capabilities of such models .\nMoreover, in Figures 8c and 8d, we observe very different behavior from the Claude 3.7 Sonnet think-\ning model. In the Tower of Hanoi environment, the model s first error in the proposed solution often\noccurs much later, e. g., around move 100 for (N=10), compared to the River Crossing environment,\nwhere the model can only produce a valid solution until move 4. Note that this model also achieves\nnear-perfect accuracy when solving the Tower of Hanoi with (N=5), which requires 31 moves, while\nit fails to solve the River Crossing puzzle when (N=3), which has a solution of 11 moves. This likely\nsuggests that examples of River Crossing with N 2 are scarce on the web, meaning LRMs may not\nhave frequently encountered or memorized such instances during training.\nComplexity (Number of Disks)\n100Accuracy (%)\nDeepSeek-R1\nComplexity (Number of Disks)\n100Accuracy (%)\nClaude-3.7-Sonnet (thinking)\nDefault (b)\nComplexity (Number of Disks)\n100First Wrong Move (Median)\nClaude-3.7-Sonnet (thinking) (c)\nComplexity (Number of People)\n10First Wrong Move (Median)\nClaude-3.7-Sonnet (thinking) (d)\nFigure: (a) & (b)Despite providing the solution algorithm in the prompt, execution failure\noccurs at similar points, highlighting reasoning model limitations in logical step execution.(c) &\n(d) Notably, the Claude 3.7 Sonnet model demonstrates much longer error-free sequences in the\nTower of Hanoi compared to early errors in the River Crossing scenario.\nIn this paper, we systematically examine frontier Large Reasoning Models (LRMs) through the lens\nof problem complexity using controllable puzzle environments. Our findings reveal fundamental\nlimitations in current models: despite sophisticated self-reflection mechanisms, these models fail to\ndevelop generalizable reasoning capabilities beyond certain complexity thresholds. We identified\nthree distinct reasoning regimes: standard LLMs outperform LRMs at low complexity, LRMs excel at\nmoderate complexity, and both collapse at high complexity. Particularly concerning is the counterin-\ntuitive reduction in reasoning effort as problems approach critical complexity, suggesting an inherent\ncompute scaling limit in LRMs. Our detailed analysis of reasoning traces further exposed complexity-\ndependent reasoning patterns, from inefficient overthinking on simpler problems to complete failure\non complex ones. These insights challenge prevailing assumptions about LRM capabilities and\nsuggest that current approaches may be encountering fundamental barriers to generalizable reasoning.\nFinally, we presented some surprising results on LRMs that lead to several open questions for future\nwork. Most notably, we observed their limitations in performing exact computation; for example,\nwhen we provided the solution algorithm for the Tower of Hanoi to the models, their performance\non this puzzle did not improve. Moreover, investigating the first failure move of the models revealed\nsurprising behaviors. For instance, they could perform up to 100 correct moves in the Tower of\nHanoi but fail to provide more than 5 correct moves in the River Crossing puzzle. We believe our\nresults can pave the way for future investigations into the reasoning capabilities of these systems.\nWe acknowledge that our work has limitations. While our puzzle environments enable controlled\nexperimentation with fine-grained control over problem complexity, they represent a narrow slice of\nreasoning tasks and may not capture the diversity of real-world or knowledge-intensive reasoning\nproblems. It is notable that most of our experiments rely on black-box API access to the closed frontier\nLRMs, limiting our ability to analyze internal states or architectural components. Furthermore, the\nstep. However, in less structured domains, such precise validation may not be feasible, limiting the\ntransferability of this analysis to other more generalizable reasoning.\nThe authors would like to thank Scott Hoang, Yichen Jiang, Minsik Cho, Mohammad Sekhavat, David\nHarrison, Mohammadreza Armandpour and Devi Krishna for the valuable feedback and support.\n            \n            Each agent should:\n            1. Read and understand the paper from your role's perspective\n            2. Identify key points relevant to your expertise\n            3. Prepare questions or concerns to discuss\n            4. Consider the implications from your viewpoint\n            \n            Language: Spanish\n            ",
      "expected_output": "Initial analysis and key points from each role's perspective",
      "agent_role": "Coordinator"
    },
    {
      "description": "\n            Based on the initial analysis, conduct a thorough discussion of the paper.\n            \n            The discussion should:\n            1. Cover all major points of the paper\n            2. Include different perspectives from each role\n            3. Address potential concerns and criticisms\n            4. Explore implications and applications\n            5. Be engaging and conversational\n            \n            Create a rich dialogue that would be interesting for a podcast audience.\n            Language: Spanish\n            ",
      "expected_output": "Comprehensive discussion transcript with multiple perspectives",
      "agent_role": "Critical Thinker"
    },
    {
      "description": "\n            Transform the discussion into a relaxed educational lecture text.\n            \n            The script should be in the style of popular science educators like 3Blue1Brown:\n            1. Written as a SINGLE EDUCATOR speaking directly to the listener (use \"tú\"/\"usted\")\n            2. Use analogies and accessible explanations\n            3. Include all key insights from the discussion\n            4. Be engaging and educational, not just informative\n            5. Flow naturally from concept to concept with smooth transitions\n            6. Include moments of wonder and intellectual curiosity\n            7. Break down complex ideas into digestible parts\n            8. Use a teaching tone that makes the listener feel they're learning something fascinating\n            9. Write as continuous text ready to be read by a voice actor\n            10. NO section headers, NO subheaders, NO formatting marks\n            11. Don't address the public with greetings or goodbyes, but make questions\n            12. Always end up with questions for the reader and practical implications\n            13. Write as plain text that flows naturally for voice reading\n            14. NO [PAUSES], NO [MUSIC], NO stage directions - just the educational content\n            15. CRITICAL: Address the listener directly - \"puedes imaginar\", \"si consideras\", \"te darás cuenta\"\n            16. DO NOT write as if summarizing a discussion - write as if YOU are the teacher\n            17. Avoid phrases like \"los expertos discutieron\" or \"el equipo concluyó\"\n            \n            \n            SIMPLE LEVEL REQUIREMENTS - FOR NON-TECHNICAL AUDIENCE:\n            15. AVOID ALL TECHNICAL JARGON - use only everyday language\n            16. Use SIMPLE analogies from daily life (like cooking, sports, relationships)\n            17. Focus ONLY on \"what this means for YOU\" - direct personal relevance\n            18. Explain concepts as if talking to a curious friend with no scientific background\n            19. NO scientific terms without VERY simple explanations (e.g., \"neurons - the tiny messengers in your brain\")\n            20. Use SHORT sentences and familiar words only\n            21. Focus on PRACTICAL takeaways: \"What can you do with this information?\"\n            22. Tell STORIES and use EXAMPLES from everyday situations\n            23. Ask rhetorical questions that connect to personal experience: \"¿Alguna vez has notado que...?\"\n            24. Make it feel like a friendly conversation, not a lecture\n            25. Focus on the BIG PICTURE and skip complex details entirely\n            26. Use comparisons to things everyone knows: \"like your smartphone battery\", \"like driving a car\"\n            27. This should sound like explaining to a family member who's genuinely curious but has no technical background\n            \n            \n            \n        DURATION REQUIREMENT: EXACTLY 3 minutes of content (420-480 words) - THIS IS MANDATORY\n        \n        DEPTH GUIDANCE FOR 3 MINUTES:\n        \n            - Focus on 1-2 main concepts only\n            - Keep explanations concise but complete\n            - Include one compelling example per main point\n            - Go straight to the point without much additional context\n            \n        \n        TECHNICAL CALCULATION:\n        - Target reading speed: ~150 words per minute\n        - Word range: 420-480 words\n        - If content is too short, EXPAND significantly with more detail and depth\n        - If too long, maintain quality but adjust information density\n        \n            \n            \n            LANGUAGE REQUIREMENTS FOR SPANISH:\n            \n            CRITICAL: AVOID ANGLICISMS whenever possible and use proper Spanish terms:\n            - Instead of \"link\" use \"enlace\" or \"vínculo\"\n            - Instead of \"feedback\" use \"retroalimentación\" or \"respuesta\"\n            - Insted of \"puzzle\" use \"rompecabezas\" or \"problema\"\n            - Instead of \"performance\" use \"rendimiento\" or \"desempeño\"\n            - Instead of \"input/output\" use \"entrada/salida\"\n            - Instead of \"update\" use \"actualizar\" or \"poner al día\"\n            \n            EXCEPTIONS - You CAN use anglicisms for:\n            1. Very new technical terms with no established translation (e.g., \"blockchain\", \"ChatGPT\")\n            2. Proper names of tools/companies (e.g., \"TensorFlow\", \"GitHub\", \"OpenAI\")\n            3. Widely adopted terms in scientific literature (e.g., \"machine learning\" vs \"aprendizaje automático\")\n            4. When the Spanish term is more confusing than helpful\n            \n            GENERAL RULES:\n            - Always prioritize natural Spanish expressions\n            - Use Spanish sentence structures and idioms\n            - Make it sound like a native Spanish speaker wrote it\n            - When you must use an anglicism, briefly explain it if needed\n            \n            \n            Language: Spanish\n            ",
      "expected_output": "Clean educational text ready for voice actor reading",
      "agent_role": "Educational Writer"
    },
    {
      "description": "\n            FINAL STEP: Transform the educational content into a PERFECT voice-ready script.\n            \n            CRITICAL: Verify the content meets the 3-minute target (420-480 words). If it's too short, EXPAND it significantly.\n            CRITICAL: Ensure technical level is simple - keep extremely simple and conversational.\n            \n            MANDATORY VOICE OPTIMIZATION REQUIREMENTS:\n            1. Create a SINGLE, CONTINUOUS text ready for a voice actor to read\n            2. Markdown formatting, but NO headers, NO bullet points, NO lists\n            3. Convert ALL content into natural, flowing sentences\n            4. Replace any remaining bullet points with complete sentences\n            5. Ensure PERFECT flow from sentence to sentence\n            6. Remove formatting marks: #, -, •, etc for titles and subtitles, but keep for bold and italic text\n            7. Make sure sentences are not too long or complex for voice delivery\n            8. Use natural speech patterns and rhythm\n            9. Include natural transitions between concepts (\"Ahora consideremos...\", \"Lo que resulta particularmente interesante es...\")\n            10. NO stage directions, NO [PAUSES], NO [MUSIC], NO technical annotations\n            11. NO greetings or goodbyes - start directly with content\n            12. End with thought-provoking questions and practical implications\n            13. If content is too short, SIGNIFICANTLY EXPAND with more detail and depth\n            14. This must be PUBLICATION-READY text that a voice actor can read smoothly\n            15. Every word should sound natural when spoken aloud\n            16. CRITICAL: When writing in second person, use second person relaxed pronouns, like \"tú\" in spanish, for the listener, but use it naturally\n            17. DO NOT refer to \"we researchers\", \"our discussion\", \"the team analyzed\" - this is NOT a research report\n            18. Instead use: \"si consideras\", \"puedes ver que\", \"imagina que\", \"te preguntarás\", etc.\n            19. Write as if a SINGLE EDUCATOR is teaching directly to the listener\n            20. Remove any references to \"coordinators\", \"discussions between experts\", or \"our analysis\"\n            21. This should sound like ONE VOICE teaching, not a summary of multiple voices\n            22. Avoid words that could make this sound like written by an LLM, like not often used words: \"fascinante\", \"delve\",\n            \"revelador\".\n            23. Introduction should be a catchy hook that makes the listener want to listen to the entire video, something like a question or a statement that makes the listener want to know more.\n\n            \n            LANGUAGE REQUIREMENTS FOR SPANISH:\n            \n            CRITICAL: AVOID ANGLICISMS whenever possible and use proper Spanish terms:\n            - Instead of \"link\" use \"enlace\" or \"vínculo\"\n            - Instead of \"feedback\" use \"retroalimentación\" or \"respuesta\"\n            - Insted of \"puzzle\" use \"rompecabezas\" or \"problema\"\n            - Instead of \"performance\" use \"rendimiento\" or \"desempeño\"\n            - Instead of \"input/output\" use \"entrada/salida\"\n            - Instead of \"update\" use \"actualizar\" or \"poner al día\"\n            \n            EXCEPTIONS - You CAN use anglicisms for:\n            1. Very new technical terms with no established translation (e.g., \"blockchain\", \"ChatGPT\")\n            2. Proper names of tools/companies (e.g., \"TensorFlow\", \"GitHub\", \"OpenAI\")\n            3. Widely adopted terms in scientific literature (e.g., \"machine learning\" vs \"aprendizaje automático\")\n            4. When the Spanish term is more confusing than helpful\n            \n            GENERAL RULES:\n            - Always prioritize natural Spanish expressions\n            - Use Spanish sentence structures and idioms\n            - Make it sound like a native Spanish speaker wrote it\n            - When you must use an anglicism, briefly explain it if needed\n            \n            \n            CRITICAL: This is the FINAL version that will be published. Make it PERFECT.\n            \n            Language: Spanish\n            ",
      "expected_output": "FINAL publication-ready voice script (420-480 words)",
      "agent_role": "Voice Director"
    }
  ]
}