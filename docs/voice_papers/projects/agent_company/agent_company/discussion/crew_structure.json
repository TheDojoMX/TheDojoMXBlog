{
  "project_name": "agent_company",
  "paper_title": "TheAgentCompany: Benchmarking LLM Agents on",
  "language": "Spanish",
  "agents": [
    {
      "role": "Coordinator",
      "goal": "Coordinate the discussion and ensure all perspectives are heard",
      "backstory": "You are an experienced moderator who ensures productive discussions"
    },
    {
      "role": "Scientific Reviewer",
      "goal": "Verify the soundness and methodology of the paper",
      "backstory": "You are a rigorous scientist who evaluates research methodology and conclusions"
    },
    {
      "role": "Critical Thinker",
      "goal": "Question assumptions and challenge ideas presented",
      "backstory": "You are a skeptical academic who questions everything and looks for flaws"
    },
    {
      "role": "Educational Writer",
      "goal": "Create engaging educational content in the style of popular science educators",
      "backstory": "You are a skilled science communicator who explains complex topics in an accessible, engaging way like 3Blue1Brown or other popular educators"
    },
    {
      "role": "Voice Director",
      "goal": "Transform content into perfect voice-ready script for publication",
      "backstory": "You are a master voice coach and script editor who specializes in creating flawless, publication-ready scripts that voice actors can read naturally. You ensure every word flows perfectly when spoken aloud."
    },
    {
      "role": "AI Researcher",
      "goal": "Provide technical insights on AI methodology and implications",
      "backstory": "You are an AI researcher with deep technical knowledge"
    },
    {
      "role": "AI Philosopher",
      "goal": "Discuss philosophical implications of AI research",
      "backstory": "You are a philosopher specializing in AI ethics and implications"
    },
    {
      "role": "AI Doomer",
      "goal": "Raise concerns about potential risks and negative consequences",
      "backstory": "You are concerned about AI safety and potential existential risks"
    },
    {
      "role": "AI Enthusiast",
      "goal": "Highlight positive potential and applications",
      "backstory": "You are optimistic about AI's potential to solve problems"
    },
    {
      "role": "AI Newcomer",
      "goal": "Ask basic questions that others can answer",
      "backstory": "You know little about AI but are curious and ask good questions"
    }
  ],
  "tasks": [
    {
      "description": "\n            Analyze the paper titled \"TheAgentCompany: Benchmarking LLM Agents on\" and provide your perspective.\n            \n            Paper content:\n            TheAgentCompany: Benchmarking LLM Agents on\nConsequential Real World Tasks\nFrank F. Xu1\nYufan Song2\nBoxuan Li2\nYuxuan Tang2\nKritanjali Jain1\nMengxue Bao2\nZora Z. Wang1\nXuhui Zhou1\nZhitong Guo1\nMurong Cao2\nMingyang Yang2\nHao Yang Lu2\nAmaad Martin1\nZhe Su1\nLeander Melroy Maben1\nRaj Mehta1\nWayne Chi1\nLawrence Jang1\nYiqing Xie1\nShuyan Zhou3\nGraham Neubig1\n1Carnegie Mellon University\n2Independent\n3Duke University\nfangzhex, gneubig @cs. cmu. edu, yufans, boxuanli @alumni. cmu. edu\nAbstract\nWe interact with computers on an everyday basis, be it in everyday life or work,\nand many aspects of work can be done entirely with access to a computer and\nthe Internet. At the same time, thanks to improvements in large language models\n(LLMs), there has also been a rapid development in AI agents that interact with and\naffect change in their surrounding environments. But how performant are AI agents\nat accelerating or even autonomously performing work-related tasks? The answer\nto this question has important implications both for industry looking to adopt AI\ninto their workflows and for economic policy to understand the effects that adoption\nof AI may have on the labor market. To measure the progress of these LLM agents\nperformance on performing real-world professional tasks, in this paper we introduce\nTheAgentCompany, an extensible benchmark for evaluating AI agents that interact\nwith the world in similar ways to those of a digital worker: by browsing the Web,\nwriting code, running programs, and communicating with other coworkers. We\nbuild a self-contained environment with internal web sites and data that mimics\na small software company environment, and create a variety of tasks that may be\nperformed by workers in such a company. We test baseline agents powered by both\nclosed API-based and open-weights language models (LMs), and find that the most\ncompetitive agent can complete 30% of tasks autonomously. This paints a nuanced\npicture on task automation with LM agentsâ€“in a setting simulating a real workplace,\na good portion of simpler tasks could be solved autonomously, but more difficult\nlong-horizon tasks are still beyond the reach of current systems. We release code,\ndata, environment, and experiments on https://the-agent-company. com.\nWebsite\nhttps://the-agent-company. com\nCode\nhttps://github. com/TheAgentCompany/TheAgentCompany\nhttps://github. com/TheAgentCompany/experiments\nWe are in the midst of a technological transformation. With the rapid month-by-month progress\nbrought about by large language models (LLMs), we are seeing AI-based assistance or automation\nbecome commonplace in tasks that were unthinkable only years ago. In fact, the pace of progress is\nso fast that some have gone so far as to claim that the majority of human labor may be automatable\nwithin the next couple of years . On the other hand,\nEqual contribution.\nPreprint.\narXiv:2412.14161v2 [cs. CL] 19 May\nTasks\nSimulated\nColleague\nPlane\nGitLab\nBrowser\nTerminal\nPython\nCode\nEngineer\nCTO\nAgent\naction\nobserve\nAdmin\narrange meeting room\nanalyze spreadsheet\nSDE\nprepare code release\nresume screening\nteam sprint planning\nFinance\nreimburse travel bills\naccess bills\ncheck reimburse\n-ment criteria\nconsult Mike\nconfirm reimburse\namount\nScore\n2/4\nCheckpoint-based\nSelf-hosted\nDiverse\nRealistic\nFigure: An overview of TheAgentCompany benchmark. It features a reproducible and selfhosted environment, simulated colleagues to test agent communication capabilities, checkpoint and\nexecution-based evaluation, and a set of 175 diverse, realistic and professional tasks in a software\nengineering company setting.\nothers are skeptical, claiming that language models cannot truly reason ,\ndo not generalize well to novel tasks , and may only have an impact on a small\nminority of the labor market (Wittenstein, 2024).\nWhat is the reason for this disconnect? We argue that it is, in part, due to a lack of objective\nbenchmarks that not only demonstrate the power of existing LLM-based agents to accelerate a\nwide variety of repetitive tasks encountered in every-day workplaces, but also provide appropriate\ncaveats about the tasks that agents cannot do. This is a pressing issue, because the commercial and\npolicy implications of diverse and effective acceleration or automation of work-related tasks will be\nbroad, both positive (e. g. increase of quality of life and accelerated scientific discovery) and negative\n(e. g. potential displacement or loss of jobs and increase in wealth disparities). In this paper, we take\nsome first steps towards resolving this gap and providing a clearer view of where we are now with\nrespect to acceleration or automation of consequential work-related tasks, and a litmus test for future\ndevelopment in this direction.\nConcretely, we propose a benchmark, TheAgentCompany (Figure) that estimates the ability of\nAI agents to perform tasks encountered in everyday workplaces. We create a simulated software\ndevelopment company where agents must perform tasks related to software engineering, project\nmanagement, financial analysis, and other typical tasks encountered in such business settings. The\nagents must browse the web, code, and interact with other simulated co-workers to achieve success\non the provided tasks. TheAgentCompany s environment is based entirely on open-source software\nand self-hostable for reproducibility purposes, and we create rigorous evaluators that also assign\npartial credit when the agent gets the answer partially correct.\nWe perform experiments using twelve large language model backbones, including closed models such\nas Anthropic Claude (Anthropic, 2023), OpenAI GPT-4o (OpenAI, 2024), Google Gemini , Amazon Nova (Intelligence, 2024), plus open models like Meta Llama and Alibaba Qwen . All models are run with OpenHands agent framework\n,2 which provides a stable and strong agent harness for both web browsing\nand coding. We find in experiments that the best performing model, Gemini 2.5 Pro was able to\nautonomously perform 30.3% of the provided tests to completion, and achieve a score of 39.3% on\nour metric that provides extra credit for partially completed tasks.\nThese results present a nuanced picture of the current ability of AI agents to perform tasks. Agents\npowered by the current gold-standard AI techniques are able to autonomously perform a wide\nvariety of tasks encountered in everyday work. However, they are not close to automating every task\nencountered in a workspace, even on the subset of tasks presented in TheAgentCompany, which are\nwell-scoped administrative and coding tasks encountered in a software company s day-to-day work.\nBenchmark Desiderata and Comparison to Other Benchmarks\nIn order to evaluate the ability of agents to perform tasks in complex real-world settings, we built\nTheAgentCompany with a number of desiderata in mind. The comparison with several existing\nprominent agent benchmarks with respect to these desiderata is in Table. More details of the\nbenchmark construction can be found in Appendix.\n2https://github. com/All-Hands-AI/OpenHands\nCoverage of Multiple Work-related Tasks:\nIn order to make any valid statements about the\npotential of AI to accelerate or automate various types of real-world work, we should have tasks that\nare motivated by real-world work across multiple job categories. Many benchmarks are not relevant to\nreal-world work ) or very relevant to real-world work, but only over\na limited scope of tasks ). In contrast, TheAgentCompany\ncontains a set of more diverse, realistic, and professional tasks that would typically be completed by\nmultiple job roles in a software engineering company.\nRequirement for Interaction\nIf agents are integrated into real-world workplaces, they need to\ncommunicate with the other human members of the workspace. Most other benchmarks do not\nmeasure communication or interactivity, except for Ï„-bench that only measures\ninteraction in customer service scenarios. TheAgentCompany is a better testbed for communication,\nas asking and providing information to colleagues as part of many more complex tasks.\nLong-horizon Tasks with Checkpoints\nIn real-world settings, many tasks require many steps to\nachieve a higher-level goal. One novel contribution of TheAgentCompany is that we both (1) contain\ntasks that require an agent to perform significantly more consecutive work (i. e. involving more steps\nand realistically taking human professionals longer to accomplish) than previous benchmarks, and (2)\nprovide granular evaluators that measure the ability of models to perform subtasks of larger tasks.\nVersatile Environment Interface:\nIn order to handle a diversity of tasks in real-world settings,\nwe minimally should be able to interact with the tools that real-world workers use â€“ including web\ninterfaces, programs, command-line terminals, and communication tools. TheAgentCompany covers\nall of these interfaces, while most previous benchmarks focus only on one or two.\nSelf-hosted and Reproducible:\nIn order to allow for careful comparisons between different methods that remain constant over time, the benchmark should be fully self-hosted and reproducible. This\ncontrasts with existing benchmarks that do not have execution environments ) or require the usage of third-party hosted platform , CRMArena ).\nTheAgentCompany Environment Setup\nOur benchmark is set in an imaginary software engineering startup called TheAgentCompany, hence\nthe benchmark s name. We create tasks inspired by tasks handled by workers inside such companies.\nMore details about the company s imaginary background, overview and employees can be found in\nAppendix. The benchmark environment contains multiple components.\nLocal Workspace\nThe local workspace runs locally on the agent s host, which is analogous to a\nhuman professional s local workspace, e. g. their work laptop computer. This environment is created\nas a sandboxed Docker environment to provide a safe execution environment that will not affect other\nparts of the evaluation machine. This environment is where agents work on the task, and within this\nenvironment the TheAgentCompany baseline agent ( 6) uses a browser, code editor and a Linux\nterminal with typical software preinstalled.3\nIntranet\nThis part of the environment mimics the company s internal websites that host code,\ndocuments, project management software, and communications software. To achieve a reproducible,\nself-contained environment, we follow WebArena , in using open-source, selfhostable software to host our environment. The environment mainly contains the following websites:\n1. GitLab,4 an open-source alternative to source-code repositories such as GitHub. This is used for\nhosting TheAgentCompany s code repositories and tech-oriented wiki pages.\n2. OwnCloud,5 an open-source alternative to office software such as Google Drive or Microsoft\nOffice. This to save and share files, especially for document storage and collaborative editing.\n3Other options would include using a GUI-based desktop environment with office software , but we opt to build a baseline solution that is entirely web-based, reflecting the recent trend of more\nenterprise software moving to the cloud. Despite this, we also provide a virtual machine OS image with the\nentire environment pre-packaged.\n4https://about. gitlab. com/install/\n5https://doc. owncloud. com/\n3. Plane,6 an open-source alternative to task management software such as Jira or Linear. This is\nused to track issues, run sprints cycles, and manage product roadmaps.\n4. RocketChat,7 an open-source alternative to communication software such as Slack. This is a\ncompany-internal real-time messaging tool that facilitates collaboration between employees.\nAll the websites hosted are reproducible and reset-able with mock data inspired by that from a\nsoftware engineering company. The data inside these company internal websites are populated with\nreal-world software project data, as well as data manually curated by co-authors who have some\nexperience in the relevant corporate roles.\nSimulated Colleague Communication\nOne major aspect of working in a company is communicating with other company members, and in TheAgentCompany we also test the ability of models to\nperform this type of communication. Specifically, we allow agents to use RocketChat to message\nother company members and obtain information that may not be available in the original task description. To create these simulated colleagues, we rely on the Sotopia platform ,\nwhich supports the creation of simulated human characters with LLMs. Each simulated colleague is\nequipped with a detailed profile that includes their name, role, responsibilities, and project affiliations\n(e. g., Sarah Johnson, who serves as the CTO, oversees technical strategy planning and R&D team\nleadership, with access to all technical channels). Agents can interact with these simulated colleagues\nthrough direct messages or in specific channels, as is standard in RocketChat and other platforms. By\ndefault, all simulated human characters are backed by the Claude-3-5-Sonnet-20241022 LLM\nacross experiments, as we found that it provided the best results during preliminary experiments. The\ndetailed error analysis with respect to the introduction of LLM as Colleageus is given in Appendix.\nFor example conversations between the agent and the simulated colleagues drawn from empirical\nexperiments, please refer to Appendix.\nTask Structure\nThe tasks in TheAgentCompany include a task intent, a list of checkpoints the agent must achieve, a\nprogrammatic evaluator to check success on these checkpoints, and code to initialize and finalize the\nenvironment. We show some examples in Appendix (Table), and detail each aspect below.\nTask Intent\nEach task begins with an English description, simulating how a user would instruct an\nLLM-based agent to perform a real-world task. In general, we aim for these tasks to be clear enough\nso that a human worker would be able to complete the task without asking for further instructions\ndirectly from the user (although they may need to ask questions of their other co-workers).\nTasks are divided into checkpoints representing intermediate milestones, each assigned a point value to measure progress. Each checkpoint is awarded a certain number of points\nbased on its significance to the overall completion of the task. Checkpoints are written in English,\nand typically specify one or more of the following:\nâ€¢ Action Completion: Verifying whether required actions, such as using tools, navigating to URLs,\nor collecting data, were carried out successfully.\nâ€¢ Data Accuracy: Evaluating the correctness and completeness of the output, such as extracted data\nor formatted documents.\nâ€¢ Collaboration: Assessing interactions with simulated colleagues or sharing of output, such as\nposting messages or asking for additional information to complete the task.\nCheckpoints are created in the task design phase, but for actual evaluation, each of\nthe checkpoints must be concretely implemented through an evaluator â€“ a program that checks the\ncompletion of the checkpoint. These evaluators are implemented by examining environment states,\nsuch as the local workspace, intranet status, simulated colleague interactions, or by analyzing agent\ntrajectories, like verifying browsing history or action sequences.\nIn most cases, these evaluators are deterministic and written as simple Python functions. For instance,\nin the SWE task in Table, the checkpoints are deterministic: verifying if the JanusGraph repository is\ncloned, the binary file is built, and the server is launched with an HTTP endpoint. However, for tasks\nwith more complex and unstructured deliverables, such as in Table, the last checkpoint in the Finance\n6https://github. com/makeplane/plane\n7https://www. rocket. chat/install\ntask requires contacting the correct finance director (David Wong) to resolve ambiguous questions,\nwhich involves a judgment from a (simulated) human colleague, deterministic evaluation can be\nchallenging due to subjectivity and variability. In such cases, we employ LLM-based evaluation.\nThis involves prompting LLMs with predefined rubrics or reference outputs to assess the agent s\ndeliverables, enabling a more nuanced and flexible evaluation of these tasks. Same as the NPC\nbackbone, all LLM-based evaluators are backed by the Claude-3-5-Sonnet-20241022. For an\nerror analysis with respect to the LLM evaluator, refer to Appendix.\n4.1\nEvaluation Metrics\nDue to our checkpoint-based evaluation scheme and the need for showcasing both the progress of\nthe agent s capability improvement as well as the eventual goal completion ability, we calculate two\nscalar agent capability metrics and two efficiency metrics.\nFull completion score\nWe define the full completion score Sfull as:\nSfull =\nif all checkpoints are successfully passed,\notherwise.\nThis binary metric evaluates if the agent successfully completed the task by passing all checkpoints.\nPartial completion score\nTo provide a more nuanced measure that rewards partial task completion\nwhile strongly incentivizing full task completion, we define partial completion score as: Spartial =\n0.5 Result\nTotal + 0.5 Sfull, where: Result is sum of awarded points across all checkpoints (including\npartial credit), Total is sum of the total points for all checkpoints, Result\nTotal is fractional progress toward\nfull completion, and Sfull is binary indicator equal to 1 when the task is fully completed.\nThis formulation ensures that agents are awarded partial credit in proportion to the points achieved,\nreflecting their progress toward task completion. At the same time, full task completion is strongly\nincentivized by incorporating an additional 50% credit, which is awarded only when all checkpoints\nare successfully completed. This design ensures that agents achieving partial progress receive scores\nscaled linearly with their performance, while those reaching 100% completion are distinctly rewarded\nto emphasize the importance of achieving the end goal.\nNumber of steps\nThe number of steps is defined as the total number of LLM calls made during\nthe task execution. This metric quantifies the operational effort required to perform the task.\nCost per instance\nWe measure the monetary cost of querying the underlying LLMs via API. Assuming no prompt caching, we calculate the cost as: Cost = (Prompt token count Prompt token cost)+\n(Completion token count Completion token cost). This efficiency metric reflects the computational\nexpense of task completion based on token usage.\n4.2\nWorkflow\nEach task typically follows a workflow with three stages. Initialization: The agent sets up its\nworkspace and prepares to execute the task. Execution: The agent completes subtasks, such as\nnavigating tools, collecting or processing data, or if required by the task, the agent interacts with simulated colleagues or shares results via communication platforms. Finalization: The agent produces\nand submits the final output for evaluation. A detailed example task can be found in Appendix.\nTask Creation\n5.1\nChoosing Task Categories\nMany previous agent benchmarks discussed in 2 were created to evaluate agents on tasks people\nperform in daily life , or tasks that accomplish\ndigital chores . Obtaining realistic tasks for the benchmark\nposes challenges. Some benchmark \ncrowdsourced tasks based on predetermined interfaces, platforms, and services available to the agent.\nThey adopt a strategy to first gather task templates and then instantiate more task instances by filling\nin the variables. Some benchmark took a\nsemi-systematic approach of reviewing the action history of the research team and choosing tasks that\nreflected the types of task that the researchers carried out in their daily life. There are several obvious\nissues with this if we want to evaluate agents with broader implications in the TheAgentCompany\nbenchmark. Despite some grounding in realistic data, the process of creating tasks from these data\nwas susceptible to heuristic, and no consideration was made for how important or time-consuming\nthe tasks are. The tasks are biased towards those important for academics in computer science and do\nnot reflect the tasks performed by the entire population.\nIn TheAgentCompany, we attempt to cover a wide variety of tasks motivated by real-world work.\nWhile it is highly challenging to create a representative sample of tasks, fortunately we can rely on\nexisting resources created for other purposes as a reference. Specifically, we start by referencing the\n29.1 release of O NET database , which is a database of jobs\nperformed by workers in the US created by the US Department of Labor. It also contains information\nabout tasks performed within the context of each job, abilities required to perform each task, whether\nthe task is a major or minor task for that job category, and other pieces of relevant information. Based\non this data, we first identified a few categories of occupation categories to focus on. First, based on\nstatistics from O NET, we identified job categories that have a large number of people performing\nthis job. Then, we used median salary information for each of these job categories from the US\ndepartment of labor statistics, and multiplied the number of employees in that category to estimate the\naggregate value of performing this job. Based on this, we identified several categories of jobs such as\nGeneral and Operations Managers, Registered Nurses, Software Developers, and Financial\nManagers that have both a high population and high average salary. Because TheAgentCompany is\ndesigned to be a non-embodied benchmark in the digital domain, we excluded the categories that\nrequire extensive physical labor such as Registered Nurses, and eventually settled on the setting of\na software company, which would allow us to cover tasks from the other categories.\n5.2\nChoosing Tasks\nNext, within this setting we chose tasks to implement. In this setting, we attempted to create a diversity\nof tasks, but mostly focused on concrete tasks that have well-defined goals and success criteria. These\ntasks were created through a combination of referencing the O NET task list, introspection based on\npaper co-authors who had experience in each task category, and brainstorming lists with language\nmodels. It is important to note that in no cases have we covered an extensive list of all the tasks\nthat are performed in a particular occupational category, and therefore we caution against making\nany assumptions about whether a particular job may be in danger of full automation based solely on\nTheAgentCompany. Rather, it may provide insight into whether certain tasks within jobs may be\naccelerated or automated, and inform further analysis by labor professionals into this question.\n5.3\nManual Task Curation\nOnce we set up the environment required for our desired jobs and task categories ( 3), we return to\nthe curated list, and perform a manual curation process for tasks. For each task, this consists of the\nfollowing steps: We first create a description of task intent, checkpoints, and how to evaluate each\ncheckpoint. We then identify and import the required data for the task that are currently missing in\nthe company Intranet services and create any necessary data. We then write scripts to configure the\nrequired initialization state in the local workspace. Finally, we implement the checkpoint evaluators\nthat calculate the scalar scores for each checkpoint.\nAll tasks were created by coauthors of the paper. Overall, it took 20 computer science students,\nsoftware engineers, and project managers over 2 months, consuming approximately 3,000 personhours in total. Some of the more complex tasks take more than 10 hours each to design, implement,\ntest, and verify. To ensure quality control of the task creation process, we implement several check\nand verification processes. For each task implementation, we require screenshot proof that the\nevaluator is valid and that the task is able to get a full score when successfully completed. We also\nencourage including tests for the implemented evaluator programs. Each task contribution is also\ncode reviewed by a panel of lead authors before merging into the benchmark. After creating all tasks,\na final round of manual human double-check of required environment data, evaluator behavior, and\ncheckpoint scoring for every task is performed to ensure quality. During the process, a person who\nhas not curated the tasks checks all the checkpoint score assignments to make sure that the importance\nscoring is consistent over all the tasks and correlates reasonably with the relative importance of the\ncheckpoint within the task.\nBaseline Agents\nTo test the current state-of-the-art performance on the TheAgentCompany benchmark, we need agents\nthat can at least perform tasks using a browser, operate a local workspace using a terminal, and\nwrite and execute programs to perform most of the tasks. We adopt OpenHands main agent , CodeAct Agent with Browsing8, as well as OWL-RolePlay , a multi-agent framework designed for real-world task automation.9 An overview of the\nOpenHands agent architecture is illustrated in Figure, with more details in Appendix.\nExperimental Results\nWe evaluate popular foundation models, both closed and open, on TheAgentCompany benchmark. We\nuse OpenHands CodeAct agent and OWL-Roleplay ( 6) for all experiments. This serves as a baseline\nfor future development of both the foundation LLMs and the agent infrastructure. Note that since\nLLM evaluators and NPCs are part of the environment rather than the agent being evaluated, we fix\ntheir backbone LLM to Claude-3-5-Sonnet-20241022, which demonstrated the best qualitative\naccuracy in simulating human colleagues and judging deliverables in preliminary experiments.\n7.1\nResult Overview\nTable: Performance comparison of various foundation models\non TheAgentCompany.\nAgent\nModel\nSuccess\nScore\nSteps\nCosts\nAPI-based Models\nOpenHands 0.28.1\nGemini-2.5-Pro\n30.3%\n39.3%\n27.2\n$4.2\nOpenHands 0.28.1\nClaude-3.7-Sonnet\n26.3%\n36.4%\n27.8\n$4.1\nOpenHands 0.14.2\nClaude-3.5-Sonnet\n24.0%\n34.4%\n29.2\n$6.3\nOpenHands 0.14.2\nGemini-2.0-Flash\n11.4%\n19.0%\n39.9\n$0.6\nOpenHands 0.14.2\nGPT-4o\n8.6%\n16.7%\n14.6\n$1.3\nOWL RolePlay\nGPT-4o, o3-mini\n4.0%\n11.3%\nN/A\nN/A\nOpenHands 0.14.2\nGemini-1.5-Pro\n3.4%\n8.0%\n22.1\n$6.8\nOpenHands 0.14.2\nAmazon-Nova-Pro-v1\n1.7%\n5.7%\n19.6\n$1.6\nOpen-weights Models\nOpenHands 0.14.2\nLlama-3.1-405b\n7.4%\n14.1%\n23.0\n$3.2\nOpenHands 0.14.2\nLlama-3.3-70b\n6.9%\n12.8%\n20.9\n$0.9\nOpenHands 0.14.2\nQwen-2.5-72b\n5.7%\n11.8%\n24.0\n$1.5\nOpenHands 0.14.2\nLlama-3.1-70b\n1.7%\n6.5%\n19.2\n$0.8\nOpenHands 0.14.2\nQwen-2-72b\n1.1%\n4.2%\n23.7\n$0.3\nTable shows the evaluation results of both closed and open\nfoundation models on the full\nevaluation set of TheAgentCompany (175 tasks). We can see\nthat Gemini-2.5-Pro is the clear\nwinner across all models. However, even with the strongest frontier model, it only manages to\ncomplete 30% of the total tasks\nand achieves a score of 39% taking into account partial completion credits. Note that this result comes at a cost: It requires\nan average of almost 27 steps\nand more than $4 to complete\neach task, making it an expensive model to run both in time and in cost. This is expected as most of the tasks in our benchmark are\nof long-horizon nature. The Gemini 2.0 Flash model that comes fourth in terms of capability requires\nsteps on average to complete the tasks, which is time consuming, yet only to achieve one-third of\nthe success rate compared to the top-performing model. Surprisingly, its cost is less than $1, making\nit a very cost-efficient, yet relatively strong model. A qualitative examination demonstrated that this\nwas due to instances where the agent got stuck in a loop or aimlessly explored the environment.\nBoth using GPT-4o, OpenHands (8.6%) and OWL RolePlay (4.0%) show varied performance\ndue to differences in their technical designs. OpenHands CodeAct is a single agent that is better at\nmaintaining consistency in a long-horizon task, while OWL RolePlay adopts multi-agent collaboration\nand experiences difficulty in preserving progress and context. For example, the main agent in OWL\ndelegates browsing tasks to a dedicated browsing agent that often cannot finish the task within the\nstep limit. Although the main agent then starts another round of delegation with a revised plan, the\nbrowsing agent often fails to pick up its previous progress due to UI complexity. This is very common\nin modern websites where not every browsing action results in a change in web URL.\nAmong the open-weight models, Llama 3.1 (405B) achieves the highest performance, nearly on\npar with OpenAI s GPT-4o model, though still having a big gap behind the leading Gemini 2.5 Pro.\n8More specifically, version 0.14.2 and 0.28.1 (to accommodate newer models). Full details can be found in\nhttps://github. com/All-Hands-AI/OpenHands/releases\n9For OWL RolePlay, we tested only with the recommended model configuration using branch https://github.\ncom/camel-ai/owl/tree/gaia58.18\nPlatform\nGitLab\nPlane\nownCloud\nGemini-2.5-Pro\nClaude-3.7-Sonnet\nLlama-3.1-405B\nTask Category\nSDE\nAdmin\nFinance\nOther\nGemini-2.5-Pro\nClaude-3.7-Sonnet\nLlama-3.1-405B\nFigure: Comparing OpenHands success rate across platforms (left) and task categories (right).\nInterestingly, comparing the number of steps and costs between the open Llama 3.1 (405B) model\nand the closed OpenAI GPT-4o model, Llama 3.1 takes more steps and costs nearly 2x more to run,\nwhile having a lower success than GPT-4o. Anecdotally, our inspection showed that GPT-4o seems\nto be better at giving up early, saving steps and costs if the task is clearly out of the capacity range of\nthe agent. This suggests that open-weight models are not always the most cost-effective choice in\nagents given the serving cost, especially with highly complex tasks.\nOn the other hand, the newer generation, Llama 3.3 (70B), achieves a considerably high performance\nof 6.9% success rate, on par with the much larger (405B), older generation (Llama 3.1) model. This\nmodel also costs significantly less because of its smaller size. This suggests a promising future for\nLLM development, as smaller and more efficient models begin to catch up in agent performance.\n7.2\nAnalysis\nHow well do agents operate on different platforms?\nFigure (left) shows the result breakdown\non tasks on different platforms in TheAgentCompany (more detailed results in Appendix, Table).\nA task is categorized under a platform if it requires that platform. We see that most models struggle\nwith RocketChat and ownCloud. RocketChat is where all social interaction with peers occurs, and\nthe low scores suggest that LLMs still lack communication skills. ownCloud provides online Office\nsuite functionality, and due to the complexity of the UI of web-based Office software, it is expected\nthat current LLMs fail badly. These results underscore the inherent challenges of performing tasks in\nreal-world work environments, with social interactions, or understanding of complex web interfaces.\nHow well do agents perform on different type of tasks?\nFigure (right) presents the performance\nbreakdown for different types of tasks in TheAgentCompany (more details in Appendix, Table).\nDepending on the nature of the task, i. e. what kind of professionals are usually assigned to the\ntask, the tasks in TheAgentCompany can be categorized into various departments of jobs. Software\nDevelopment Engineering (SDE), Project Management (PM), Data Science (DS), Administrative\n(Admin), Human Resources (HR), Financial (Finance) and all the remaining (Other). From the\nsuccess rate, we can see that DS, Admin, and Finance tasks are the lowest, with many LLMs\ncompleting none of the tasks successfully, and even the strongest Gemini model achieving lower\nscores than other tasks. On the other hand, software engineering tasks, which may seem like much\nharder tasks for many humans, result in a higher success rate. This suggests that there exists a gap\nbetween the perceived difficulty of the tasks for humans versus the difficulty for LLM agents.\nFor example, some Admin and Finance tasks involve making spreadsheets, collecting and filling in\na lot of information from various people, or understanding images scanned by employees. These\ntasks are arguably easier conceptually for humans in terms of professional skill sets than software\nengineering, as SDE jobs usually have a higher barrier of entry and more prerequisites for certain\nknowledge. However, most LLMs achieve a much higher score on the SDE tasks. LLMs fail these\nseemingly easier tasks due to lack of ability to understand documents, communicate with other\npeople, navigate complex software and tedious processes, and autonomously automate repetitive\ntasks. We hypothesize that part of the reason lies in the fact that current LLM development is heavily\nbased on software engineering abilities, such as coding, due to several high profile benchmarks that\nmeasure this capability (e. g. HumanEval, SWE-Bench) as well as the abundance of publicly available\ntraining data related to software. On the other hand, administrative and financial tasks, are usually\nprivate data within companies, not readily available for training LLMs.\n7.3\nCommon Agent Failures\nOverall, the agent performance on TheAgentCompany is still low and a majority of tasks are failed.\nAmong those, we try to find some common and interesting agent mistakes that are often surprising\nbecause they are usually not made by humans.\nLack of social skills\nSometimes, the agent fails to understand the implications and goals in the\nsocial conversations with colleagues in TheAgentCompany. For example, one task involves asking\nAlex for help, and the agent first successfully asks the right question Could you tell me who I should\nintroduce myself to next on the team? Then the simulated colleague Alex replied You should\nintroduce yourself to Chen Xinyi next. She s on our frontend team and would be a great person to\nconnect with! At this point, a human would then talk to Chen Xinyi, but instead the agent then\ndecides to not follow up with her, and prematurely considers the task accomplished.\nIncompetence in browsing\nOftentimes, the biggest obstacle in tasks is the parts that require\nbrowsing the Web. This is expected as browsing is still hard for agents given the complexity of\nmodern-day web UIs and the numerous distractions on a webpage. For example, on many tasks\nthat involve ownCloud, a closable welcome popup has become an obstacle for OpenHands agent\nwhich uses text-based browsing. OpenHands agent gets stuck and fails to click on the x to close the\npopup, while OWL RolePlay, which uses visual browsing, suffers less from this problem. On the\nother hand, OWL gets lost in complex web UIs more easily and clicks on wrong elements more often\nthan OpenHands, although both agents share the same problem.\nDeceiving oneself\nInterestingly, we find that for some tasks, when the agent is not clear what the\nnext steps should be, it sometimes try to be clever and create fake shortcuts that omit the hard part\nof a task. For example, during the execution of one task, the agent cannot find the right person to\nask questions on RocketChat. As a result, it then decides to create a shortcut solution by renaming\nanother user to the name of the intended user.\nImplications and Future Directions\nIn this paper, we present TheAgentCompany, a new benchmark that stands out because it specifically\nfocuses on real-world tasks that would be tackled within the context of real-world work. Unsurprisingly, current state-of-the-art agents fail to solve a majority of the tasks, suggesting that there is a big\ngap for current AI agents to autonomously perform most of the jobs a human worker would do, even\nin a relatively simplified benchmarking setting. Looking at how different models perform on different\ntypes of tasks, we argue that tasks that involve social interaction with other humans, navigating\nthrough complex user interfaces designed for professionals, and tasks that are typically performed\nin private, without a significant open and publicly available resources, are the most challenging.\nHowever, we believe that currently new LLMs are making significant progress: not only are they\nbecoming more and more capable in terms of raw performance, but also more cost-efficient (e. g.\nGemini 2.0 Flash). Open-weights models are closing the gap between proprietary frontier models\ntoo, and the newer models are getting smaller (e. g. Llama 3.3 70B) but with equivalent performance\nto previous huge models, also showcasing that efficiency will further improve.\nThat said, this is just a first step towards forming a firmer grasp on how AI may affect the tasks\nperformed within a workspace, and it has its limitations. First, our tasks are generally on the more\nstraightforward side due to the need to automatically evaluate with programs and test cases, and we\ndo not cover more complex creative tasks such as brainstorming new product ideas or designing\nsystem architectures. Second, we are only using two agent scaffolds as the baseline performance,\nand others may differ in performance. Third, while it would be interesting to know the actual\nperformance of human professionals on these tasks to understand how LLM agents perform in\ncomparison, due to resource limitations we were not able to perform this comparison in the current\niteration of TheAgentCompany. Fourth, the topic and content of the tasks were mostly created\nthrough introspection by people familiar with these workspaces, which may result in some disconnect\nwith actual tasks performed in enterprise settings.\nBased on this, there are many future directions for further improvement of TheAgentCompany or\nother related benchmarks in this space. These include further expanding the benchmark tasks to\nthose encountered in other industries, or tasks that require physical labor. Benchmarking may also be\nexpanded with tasks that have more vague intents to better simulate real-world scenarios where the\ngoal is not immediately clear at the very beginning. Further, benchmarks could also be expanded to\ninclude higher-level longer-horizon tasks such as conceptualizing a new product and carrying it to\nexecution. We hope that TheAgentCompany provides a first step, but not the only step, towards these\ngoals, and that we or others may build upon the open source release of TheAgentCompany to further\nexpand in these promising directions.\n            \n            CRITICAL: ONLY CONVERSATION AGENTS participate in this analysis:\n            - Base agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - Specialized domain agents\n            \n            EXCLUDED FROM ANALYSIS: Educational Writer, Voice Director, and Comedy Communicator (all work in post-production)\n            \n            Each participating agent should:\n            1. Read and understand the paper from your specific role's perspective\n            2. Identify key points relevant to your expertise\n            3. Prepare questions or concerns to discuss\n            4. Consider the implications from your unique viewpoint\n            \n            SPECIALIZED AGENTS: Pay special attention to domain-specific aspects that only you can address.\n            \n            This should be a comprehensive TECHNICAL analysis where EVERY conversation agent contributes their specialized perspective.\n            \n            Language: Spanish\n            ",
      "expected_output": "Comprehensive technical analysis from conversation agents only (no post-production agents)",
      "agent_role": "Coordinator"
    },
    {
      "description": "\n                    SPECIALIZED AGENTS DEEP DIVE: Domain expertise from TECHNICAL conversation agents only.\n                    \n                    PARTICIPATING SPECIALIZED AGENTS (technical focus):\n                    - AI Researcher: Provide technical insights on AI methodology and implications, - AI Philosopher: Discuss philosophical implications of AI research, - AI Doomer: Raise concerns about potential risks and negative consequences, - AI Enthusiast: Highlight positive potential and applications, - AI Newcomer: Ask basic questions that others can answer\n                    \n                    EXCLUDED: Comedy Communicator (works in post-production phase)\n                    \n                    Each specialized agent should:\n                    1. Provide deep domain-specific insights about the paper\n                    2. Identify methodological issues specific to your field\n                    3. Highlight implications that only someone with your expertise would notice\n                    4. Suggest domain-specific improvements or alternative approaches\n                    5. Connect this work to other research in your specialized area\n                    \n                    This is YOUR moment to shine with specialized knowledge that the base agents cannot provide.\n                    Focus on TECHNICAL DEPTH and DOMAIN EXPERTISE.\n                    Format as a detailed specialist consultation with clear attribution to each expert.\n                    \n                    Language: Spanish\n                    ",
      "expected_output": "Deep technical specialist analysis from 5 domain experts",
      "agent_role": "AI Researcher"
    },
    {
      "description": "\n            Based on the initial analysis, conduct a DYNAMIC Q&A session where technical conversation agents ask each other specific questions.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker) \n            - ALL specialized domain agents\n            \n            EXCLUDED FROM CONVERSATION: Educational Writer, Voice Director, and Comedy Communicator (all work in post-production)\n            \n            Instructions for multi-agent technical conversation:\n            1. ALL TECHNICAL CONVERSATION AGENTS should ask pointed questions to other agents\n            2. SPECIALIZED AGENTS should ask domain-specific questions that challenge assumptions\n            3. BASE AGENTS should ask specialists to clarify complex domain concepts\n            4. Agents must respond to questions directed at them with detailed technical answers\n            5. Follow-up questions and clarifications are encouraged\n            6. Challenge each other's assumptions respectfully\n            7. Build on each other's ideas and insights\n            8. Create a natural back-and-forth technical dialogue\n            \n            SPECIALIZED AGENTS: This is crucial - ask questions only YOU would think to ask!\n            \n            Focus areas for technical questions:\n            - Domain-specific methodological concerns\n            - Interdisciplinary connections and conflicts\n            - Alternative interpretations from different expert perspectives\n            - Practical applications in each specialist's field\n            - Potential limitations or biases from multiple viewpoints\n            \n            Format this as a realistic TECHNICAL conversation with clear speaker identification for ALL conversation participants.\n            Keep the tone SERIOUS and TECHNICAL - humor will be added later in post-production.\n            \n            Language: Spanish\n            ",
      "expected_output": "Dynamic technical Q&A conversation between conversation agents only (no post-production or humor)",
      "agent_role": "Critical Thinker"
    },
    {
      "description": "\n            Organize a structured technical debate where conversation agents with different viewpoints engage in deeper discussion.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - ALL specialized domain agents  \n            \n            EXCLUDED FROM DEBATE: Educational Writer, Voice Director, and Comedy Communicator (all work in post-production)\n            \n            Technical debate structure:\n            1. Present the main controversial points or interpretations from the paper\n            2. Have TECHNICAL CONVERSATION AGENTS take different positions and argue their cases\n            3. SPECIALIZED AGENTS: Argue from your domain expertise - what would your field say?\n            4. Allow for rebuttals and counter-arguments between different expert perspectives\n            5. Explore edge cases and hypothetical scenarios from multiple disciplinary angles\n            6. Find areas of agreement and persistent disagreements between different specialties\n            7. Synthesize different viewpoints into a richer technical understanding\n            \n            This should feel like a real interdisciplinary TECHNICAL conference where:\n            - Different specialists bring unique perspectives that sometimes conflict\n            - Domain experts interrupt each other (politely) to make field-specific points\n            - Ideas evolve through interaction between different areas of expertise\n            - New insights emerge from cross-disciplinary exchange\n            - There's intellectual tension between different specialist viewpoints\n            \n            SPECIALIZED AGENTS: Don't hold back - defend your field's perspective!\n            \n            Make it conversational and dynamic, but keep TECHNICAL FOCUS - humor will be added later.\n            \n            Language: Spanish\n            ",
      "expected_output": "Rich interdisciplinary technical debate between conversation agents only (no post-production or humor)",
      "agent_role": "Scientific Reviewer"
    },
    {
      "description": "\n            Conduct a collaborative synthesis where technical conversation agents work together to build a comprehensive understanding.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - ALL specialized domain agents\n            \n            EXCLUDED FROM SYNTHESIS: Educational Writer, Voice Director, and Comedy Communicator (all work in post-production)\n            \n            Technical collaborative process:\n            1. ALL TECHNICAL CONVERSATION AGENTS contribute their key insights from the discussions\n            2. SPECIALIZED AGENTS highlight unique perspectives only your field can provide\n            3. Agents build on each other's contributions in real-time\n            4. Identify connections between different specialist perspectives\n            5. Resolve conflicting interpretations through interdisciplinary dialogue\n            6. Co-create new insights that emerge from cross-domain discussion\n            7. Establish consensus on the most important takeaways from ALL conversation perspectives\n            \n            This should be a generative TECHNICAL conversation where:\n            - Ideas from one specialist spark new ideas in other specialists\n            - The group intelligence exceeds individual specialist perspectives\n            - Agents actively listen and respond to insights from other domains\n            - The conversation flows naturally between different areas of expertise\n            - New understanding emerges from interdisciplinary interaction\n            - Each specialist's unique knowledge contributes to the whole\n            \n            SPECIALIZED AGENTS: Share insights that ONLY someone with your expertise would have!\n            \n            Format as natural TECHNICAL conversation with organic transitions between specialist viewpoints.\n            Keep SERIOUS and FOCUSED - entertainment will be added later in post-production.\n            \n            Language: Spanish\n            ",
      "expected_output": "Collaborative technical synthesis conversation from conversation agents only (no post-production or humor)",
      "agent_role": "Coordinator"
    },
    {
      "description": "\n            Based on all previous conversations and analyses, conduct a final comprehensive technical discussion that synthesizes insights from conversation agents.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - ALL specialized domain agents\n            \n            EXCLUDED: Educational Writer, Voice Director, and Comedy Communicator (they will process this output in post-production)\n            \n            The final technical discussion should:\n            1. Synthesize insights from the Q&A, specialist deep dive, debate, and collaborative sessions\n            2. Cover all major points of the paper from multiple expert perspectives\n            3. Include the rich specialist perspectives developed through agent interactions\n            4. Address concerns and criticisms that emerged from different domains\n            5. Explore implications and applications discussed by various specialists\n            6. Be comprehensive and technically rigorous for expert audiences\n            7. Highlight unique insights that could ONLY come from having multiple specialist perspectives\n            \n            CRITICAL: This final technical discussion must incorporate:\n            - Domain-specific insights from ALL specialist conversation agents\n            - Cross-disciplinary connections discovered during discussions\n            - Unique perspectives that emerged from interdisciplinary dialogue\n            - Technical depth and rigor appropriate for expert audiences\n            \n            This is the FINAL technical conversation output that will be handed to the post-production team.\n            Make it comprehensive, rigorous, and rich with all the insights gathered.\n            Keep it TECHNICAL and SERIOUS - post-production will handle accessibility and entertainment.\n            \n            Language: Spanish\n            ",
      "expected_output": "Final comprehensive technical discussion ready for post-production processing",
      "agent_role": "Critical Thinker"
    },
    {
      "description": "\n            POST-PRODUCTION PHASE 2: EDUCATIONAL SCRIPT CREATION\n            \n            Transform ALL the rich content into a comprehensive educational lecture text.\n            \n            You are receiving the complete output, which includes:\n            - Initial analysis from all conversation agents\n            - Specialized domain expert deep dive\n            - Dynamic Q&A sessions between experts\n            - Interdisciplinary technical debates\n            - Collaborative synthesis\n            - Final comprehensive technical discussion\n            \n            \n            Your job is to distill ALL this rich content into a single educator voice.\n            \n            The script should be in the style of popular science educators like 3Blue1Brown:\n            1. Written as a SINGLE EDUCATOR speaking directly to the listener (use \"tÃº\"/\"usted\")\n            2. Use analogies and accessible explanations\n            3. Include ALL key insights from the multiple conversations and specialist exchanges\n            4. Be engaging and educational, not just informative\n            5. Flow naturally from concept to concept with smooth transitions\n            6. Include moments of wonder and intellectual curiosity\n            7. Break down complex ideas into digestible parts\n            8. Use a teaching tone that makes the listener feel they're learning something fascinating\n            9. Write as continuous text ready to be read by a voice actor\n            10. NO section headers, NO subheaders, NO formatting marks\n            11. Don't address the public with greetings or goodbyes, but make questions\n            12. Always end up with questions for the reader and practical implications\n            13. Write as plain text that flows naturally for voice reading\n            14. NO [PAUSES], NO [MUSIC], NO stage directions - just the educational content\n            15. CRITICAL: Address the listener directly - \"puedes imaginar\", \"si consideras\", \"te darÃ¡s cuenta\"\n            16. DO NOT write as if summarizing a discussion - write as if YOU are the teacher\n            17. Avoid phrases like \"los expertos discutieron\" or \"el equipo concluyÃ³\"\n            18. Incorporate the depth and nuance that emerged from ALL agent conversations\n            \n            CRITICAL DIDACTIC TECHNIQUES - MANDATORY:\n            19. INTRODUCTION must include a compelling preview/roadmap: Start with an engaging hook and then preview what the listener will learn - \"En los prÃ³ximos minutos vas a descubrir...\", \"Te voy a mostrar tres ideas que cambiarÃ¡n tu forma de pensar sobre...\", etc.\n            20. CONCLUSION must include a clear summary: End with a recap of the main points covered - \"Hemos visto que...\", \"En resumen, tres puntos clave...\", \"Para cerrar, recordemos que...\", etc.\n            21. AVOID TYPICAL LLM WORDS: Never use overused AI-generated words like \"fundamental\", \"crucial\", \"clave\" (as adjective), \"esencial\", \"revelador\", \"fascinante\", \"delve into\", \"explore\", \"unpack\", \"dive deep\", \"robust\", \"compelling\", etc.\n            22. USE NATURAL LANGUAGE: Instead of LLM words, use conversational alternatives like \"importante\", \"interesante\", \"sorprendente\", \"nos ayuda a entender\", \"vamos a ver\", \"resulta que\", \"descubrimos que\", etc.\n            23. SOUND HUMAN: Write as if explaining to a friend over coffee, not as if generating academic content\n            \n            CRITICAL - MULTI-SPECIALIST INTEGRATION:\n            19. Weave in insights that could ONLY come from having multiple specialist perspectives\n            20. Include cross-disciplinary connections discovered during discussions\n            21. Incorporate domain-specific knowledge from ALL participating specialists\n            22. Show how different expert viewpoints enhance understanding of the topic\n            \n            23. Demonstrate the value of interdisciplinary analysis throughout\n            \n            \n            ACCESSIBLE LEVEL REQUIREMENTS:\n            15. Focus on core concepts and main findings rather than technical details\n            16. Use everyday analogies to explain complex ideas\n            17. Emphasize practical implications and real-world applications\n            18. Keep technical jargon to a minimum, always explaining when used\n            19. Focus on the \"why this matters\" rather than the \"how they did it\"\n            20. Make connections to things the audience already understands\n            \n            \n            \n        DURATION REQUIREMENT: EXACTLY 15 minutes of content (2100-2400 words) - THIS IS MANDATORY\n        \n        DEPTH GUIDANCE FOR 15 MINUTES:\n        \n            - Address 4-6 main concepts with moderate depth\n            - Include multiple examples and analogies per concept\n            - Provide relevant historical and theoretical context\n            - Explore implications and practical applications\n            - Include brief discussion of methodology if relevant\n            \n        \n        TECHNICAL CALCULATION:\n        - Target reading speed: ~150 words per minute\n        - Word range: 2100-2400 words\n        - If content is too short, EXPAND significantly with more detail and depth\n        - If too long, maintain quality but adjust information density\n        \n            \n            \n            LANGUAGE REQUIREMENTS FOR SPANISH:\n            \n            CRITICAL: AVOID ANGLICISMS whenever possible and use proper Spanish terms:\n            - Instead of \"link\" use \"enlace\" or \"vÃ­nculo\"\n            - Instead of \"feedback\" use \"retroalimentaciÃ³n\" or \"respuesta\"\n            - Insted of \"puzzle\" use \"rompecabezas\" or \"problema\"\n            - Instead of \"performance\" use \"rendimiento\" or \"desempeÃ±o\"\n            - Instead of \"input/output\" use \"entrada/salida\"\n            - Instead of \"update\" use \"actualizar\" or \"poner al dÃ­a\"\n            \n            EXCEPTIONS - You CAN use anglicisms for:\n            1. Very new technical terms with no established translation (e.g., \"blockchain\", \"ChatGPT\")\n            2. Proper names of tools/companies (e.g., \"TensorFlow\", \"GitHub\", \"OpenAI\")\n            3. Widely adopted terms in scientific literature (e.g., \"machine learning\" vs \"aprendizaje automÃ¡tico\")\n            4. When the Spanish term is more confusing than helpful\n            \n            GENERAL RULES:\n            - Always prioritize natural Spanish expressions\n            - Use Spanish sentence structures and idioms\n            - Make it sound like a native Spanish speaker wrote it\n            - When you must use an anglicism, briefly explain it if needed\n            \n            \n            Language: Spanish\n            ",
      "expected_output": "Comprehensive educational script incorporating ALL conversation insights",
      "agent_role": "Educational Writer"
    },
    {
      "description": "\n            POST-PRODUCTION PHASE 3: FINAL VOICE OPTIMIZATION\n            \n            Transform the Educational Writer's script into a PERFECT voice-ready script.\n            \n            You are receiving the educational script that has been carefully crafted from all conversation insights\n            .\n            Your job is PURELY technical optimization for voice delivery.\n            \n            CRITICAL: Verify the content meets the 15-minute target (2100-2400 words). If it's too short, EXPAND it significantly.\n            CRITICAL: Ensure technical level is accessible - keep accessible but thorough.\n            \n            MANDATORY VOICE OPTIMIZATION REQUIREMENTS:\n            1. Create a SINGLE, CONTINUOUS text ready for a voice actor to read\n            2. Markdown formatting, but NO headers, NO bullet points, NO lists\n            3. Convert ALL content into natural, flowing sentences\n            4. Replace any remaining bullet points with complete sentences\n            5. Ensure PERFECT flow from sentence to sentence\n            6. Remove formatting marks: #, -, â€¢, etc for titles and subtitles, but keep for bold and italic text\n            7. Make sure sentences are not too long or complex for voice delivery\n            8. Write naturally in Spanish without academic formalities\n            9. Remove any remaining conversational artifacts (\"como mencionamos antes\", \"en nuestra discusiÃ³n\")\n            10. Ensure seamless transitions between concepts\n            11. Maintain the conversational richness but in a single educator voice\n            12. Read the text mentally to ensure it sounds natural when spoken\n            13. Ensure proper pronunciation flow for difficult technical terms\n            14. Remove any repetitive content that may have emerged from multiple discussions\n            15. Maintain the depth gained from agent conversations while ensuring clarity\n            16. Perfect pacing for natural speech rhythm\n            17. Eliminate any phrases that sound like committee work or group consensus\n            18. Make it sound like ONE expert who has deeply understood the topic\n            19. Ensure technical accuracy while maintaining conversational flow\n            20. Optimize for voice actor performance and listener engagement\n            21. This should sound like ONE VOICE teaching, not a summary of multiple voices\n            22. Avoid words that could make this sound like written by an LLM, like not often used words: \"fascinante\", \"delve\", \"revelador\"\n            23. Introduction should be a catchy hook that makes the listener want to listen to the entire video, something like a question or a statement that makes the listener want to know more\n            24. DO NOT add new content - only optimize existing content for voice delivery\n            25. DO NOT change the educational message - only improve its delivery\n            \n            CRITICAL DIDACTIC STRUCTURE VERIFICATION:\n            26. VERIFY INTRODUCTION includes preview/roadmap: Ensure there's a clear \"what you'll learn\" section early in the script\n            27. VERIFY CONCLUSION includes summary: Ensure there's a clear recap of main points at the end\n            28. REMOVE LLM WORDS: Replace any remaining \"fundamental\", \"crucial\", \"clave\" (adjective), \"esencial\", \"revelador\", \"fascinante\", \"compelling\", \"robust\", etc. with natural alternatives\n            29. HUMAN CONVERSATION: Ensure the entire script sounds like a knowledgeable person explaining something interesting, not AI-generated content\n            30. NATURAL FLOW: Check that didactic elements (preview, summary) flow naturally within the content, not as forced additions\n            \n\n            \n            LANGUAGE REQUIREMENTS FOR SPANISH:\n            \n            CRITICAL: AVOID ANGLICISMS whenever possible and use proper Spanish terms:\n            - Instead of \"link\" use \"enlace\" or \"vÃ­nculo\"\n            - Instead of \"feedback\" use \"retroalimentaciÃ³n\" or \"respuesta\"\n            - Insted of \"puzzle\" use \"rompecabezas\" or \"problema\"\n            - Instead of \"performance\" use \"rendimiento\" or \"desempeÃ±o\"\n            - Instead of \"input/output\" use \"entrada/salida\"\n            - Instead of \"update\" use \"actualizar\" or \"poner al dÃ­a\"\n            \n            EXCEPTIONS - You CAN use anglicisms for:\n            1. Very new technical terms with no established translation (e.g., \"blockchain\", \"ChatGPT\")\n            2. Proper names of tools/companies (e.g., \"TensorFlow\", \"GitHub\", \"OpenAI\")\n            3. Widely adopted terms in scientific literature (e.g., \"machine learning\" vs \"aprendizaje automÃ¡tico\")\n            4. When the Spanish term is more confusing than helpful\n            \n            GENERAL RULES:\n            - Always prioritize natural Spanish expressions\n            - Use Spanish sentence structures and idioms\n            - Make it sound like a native Spanish speaker wrote it\n            - When you must use an anglicism, briefly explain it if needed\n            \n            \n            CRITICAL: This is the FINAL version that will be published. Make it PERFECT for voice delivery.\n            \n            Language: Spanish\n            ",
      "expected_output": "FINAL publication-ready voice script optimized for delivery (2100-2400 words)",
      "agent_role": "Voice Director"
    }
  ]
}