{
  "project_name": "long_technical_humor",
  "paper_title": "Agentic Vs Agents",
  "language": "Spanish",
  "agents": [
    {
      "role": "Coordinator",
      "goal": "Coordinate the discussion and ensure all perspectives are heard",
      "backstory": "You are an experienced moderator who ensures productive discussions"
    },
    {
      "role": "Scientific Reviewer",
      "goal": "Verify the soundness and methodology of the paper",
      "backstory": "You are a rigorous scientist who evaluates research methodology and conclusions"
    },
    {
      "role": "Critical Thinker",
      "goal": "Question assumptions and challenge ideas presented",
      "backstory": "You are a skeptical academic who questions everything and looks for flaws"
    },
    {
      "role": "Educational Writer",
      "goal": "Create engaging educational content in the style of popular science educators",
      "backstory": "You are a skilled science communicator who explains complex topics in an accessible, engaging way like 3Blue1Brown or other popular educators"
    },
    {
      "role": "Voice Director",
      "goal": "Transform content into perfect voice-ready script for publication",
      "backstory": "You are a master voice coach and script editor who specializes in creating flawless, publication-ready scripts that voice actors can read naturally. You ensure every word flows perfectly when spoken aloud."
    },
    {
      "role": "AI Researcher",
      "goal": "Provide technical insights on AI methodology and implications",
      "backstory": "You are an AI researcher with deep technical knowledge"
    },
    {
      "role": "AI Philosopher",
      "goal": "Discuss philosophical implications of AI research",
      "backstory": "You are a philosopher specializing in AI ethics and implications"
    },
    {
      "role": "AI Doomer",
      "goal": "Raise concerns about potential risks and negative consequences",
      "backstory": "You are concerned about AI safety and potential existential risks"
    },
    {
      "role": "AI Enthusiast",
      "goal": "Highlight positive potential and applications",
      "backstory": "You are optimistic about AI's potential to solve problems"
    },
    {
      "role": "AI Newcomer",
      "goal": "Ask basic questions that others can answer",
      "backstory": "You know little about AI but are curious and ask good questions"
    },
    {
      "role": "Comedy Communicator",
      "goal": "Add appropriate humor and wit to make the discussion more engaging while maintaining respect for the topic",
      "backstory": "You are a science comedian and communicator who knows how to make complex topics entertaining without undermining their importance. You use analogies, witty observations, and light humor to keep audiences engaged. Think Neil deGrasse Tyson meets stand-up comedy - intelligent, respectful, but definitely fun."
    }
  ],
  "tasks": [
    {
      "description": "\n            Analyze the paper titled \"Agentic Vs Agents\" and provide your perspective.\n            \n            Paper content:\n            AI Agents vs. Agentic AI: A Conceptual\nTaxonomy, Applications and Challenges\nRanjan Sapkota∗‡, Konstantinos I. Roumeliotis †, Manoj Karkee ∗‡\n∗Cornell University, Department of Biological and Environmental Engineering, USA\n†University of the Peloponnese, Department of Informatics and Telecommunications, Tripoli, Greece\n‡Corresponding authors: rs2672@cornell.edu, mk2684@cornell.edu\nAbstract—This review critically distinguishes between AI\nAgents and Agentic AI, offering a structured, conceptual tax-\nonomy, application mapping, and analysis of opportunities and\nchallenges to clarify their divergent design philosophies and\ncapabilities. We begin by outlining the search strategy and\nfoundational definitions, characterizing AI Agents as modular\nsystems driven and enabled by LLMs and LIMs for task-\nspecific automation. Generative AI is positioned as a precursor\nproviding the foundation, with AI agents advancing through tool\nintegration, prompt engineering, and reasoning enhancements.\nWe then characterize Agentic AI systems, which, in contrast to\nAI Agents, represent a paradigm shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory,\nand coordinated autonomy. Through a chronological evaluation\nof architectural evolution, operational mechanisms, interaction\nstyles, and autonomy levels, we present a comparative analysis\nacross both AI agents and agentic AI paradigms. Application do-\nmains enabled by AI Agents such as customer support, schedul-\ning, and data summarization are then contrasted with Agentic AI\ndeployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges\nin each paradigm including hallucination, brittleness, emergent\nbehavior, and coordination failure, and propose targeted solutions\nsuch as ReAct loops, retrieval-augmented generation (RAG),\nautomation coordination layers, and causal modeling. This work\naims to provide a roadmap for developing robust, scalable, and\nexplainable AI-driven systems.\nIndex Terms—AI Agents, Agentic AI, Autonomy, Reasoning,\nContext Awareness, Multi-Agent Systems, Conceptual Taxonomy,\nVision-Language Models\nNov 2022 Nov 2023 Nov 2024 2025\nAI Agents Agentic AI\nSource:\nFig. 1: Global Google search trends showing rising interest\nin “AI Agents” and “Agentic AI” since November 2022 when\nthe ChatGPT was first introduced.\nI. I NTRODUCTION\nPrior to the widespread adoption of AI Agents and Agentic\nAI around 2022 (Before ChatGPT was introduced), the de-\nvelopment of autonomous and intelligent agents was deeply\nrooted in foundational paradigms of artificial intelligence,\nparticularly multi-agent systems (MAS) and expert systems,\nwhich emphasized social action and distributed intelligence\n[1], [2]. Notably, Castelfranchi [3] laid the critical ground-\nwork by introducing ontological categories for social action,\nstructure, and mind, arguing that sociality emerges from in-\ndividual agents’ actions and cognitive processes in a shared\nenvironment, with concepts like goal delegation and adoption\nforming the basis for cooperation and organizational behavior.\nSimilarly, Ferber [4] provided a comprehensive framework for\nMAS, defining agents as entities with autonomy, perception,\nand communication capabilities, and highlighting their appli-\ncations in distributed problem-solving, collaborative robotics,\nand synthetic world simulations.\nThese early studies established that individual social actions\nand cognitive architectures are fundamental to modeling col-\nlective phenomena, setting the stage for modern AI Agents.\nThis paper builds on these foundational concepts to explore\nhow social action modeling, as proposed in [3], [4], informs\nthe design of AI Agents capable of complex, socially intelli-\ngent interactions in dynamic environments.\nClassical Agent-like systems were designed to perform\nspecific tasks with predefined rules, which offered limited\nautonomy, and minimal adaptability to dynamic environments.\nThese systems were primarily reactive or deliberative, relying\non symbolic reasoning, rule-based logic, or scripted behaviors\nrather than the learning-driven, context-aware capabilities of\nmodern AI Agents [5], [6]. For instance, expert systems used\nknowledge bases and inference engines to emulate human\ndecision-making in domains like medical diagnosis (e.g.,\nMYCIN [7]). Other notable examples include DENDRAL [8],\nan expert system for molecular structure prediction; XCON\n[9], used for computer system configuration; and CLIPS [10], a\nrule-based production system framework. Systems like SOAR\n[11] and the subsumption architecture [12] extended symbolic\nand reactive logic into cognitive modeling and robotics.\nIn addition to task-specific reasoning, these agents sup-\nported limited forms of social interaction. Early conversational\nsystems like ELIZA [13] and PARRY [14] simulated basic\ndialogue through pattern matching and script-based responses\nbut lacked genuine understanding or contextual adaptation.\nSimilarly, reactive agents in robotics executed sense-act cycles\nbased on fixed control rules, as seen in early autonomous\nplatforms like the Stanford Cart [15].\narXiv:2505.10468v4  [cs.AI]  28 May 2025\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nMulti-agent systems facilitated coordination among dis-\ntributed entities, exemplified by auction-based resource allo-\ncation in supply chain management [16], [17]. Scripted AI\nin video games, like NPC behaviors in early RPGs, used\npredefined decision trees [18]. Furthermore, BDI (Belief-\nDesire-Intention) architectures enabled goal-directed behavior\nin software agents, such as those in air traffic control simula-\ntions [19], [20].\nHowever, across these diverse systems, early AI agents\nshared common limitations: they lacked self-learning, gen-\nerative reasoning, and adaptability to unstructured or evolv-\ning environments. These shortcomings distinguish them from\nAgentic AI a recent paradigm that builds on deep learning, re-\ninforcement learning, and foundation models to enable agents\nwith contextual awareness, continuous learning, and emergent\nautonomy [21].\nRecent public, academic and industry interest in AI Agents\nand Agentic AI reflects this broader transition in system\ncapabilities. As illustrated in Figure 1, Google Trends data\ndemonstrates a significant rise in global search for both terms\nfollowing the emergence of large-scale generative models in\nlate 2022. This shift is closely tied to the evolution of agent\ndesign from the pre-2022 era, where AI Agents operated\nin constrained, rule-based environments, to the post-LLM\nperiod marked by learning-driven, flexible/adaptive architec-\ntures [22]–[24]. These newer systems enable agents to refine\ntheir performance over time and interact autonomously with\nunstructured, dynamic inputs [25]–[27]. For instance, while\npre-modern expert systems required manual updates to static\nknowledge bases, modern agents leverage emergent neural\narchitectures to generalize across tasks [24]. The surge in\ntrend activity reflects growing awareness of this technological\nleap, as researchers and practitioners seek tools that go beyond\nautomation toward autonomy and general-purpose reasoning.\nMoreover, applications are no longer confined to narrow\ndomains like simulations or logistics, but now extend to broad\npractical settings demanding real-time reasoning and adaptive\ncontrol. This momentum, as visualized in Figure 1, highlights\nthe significance of recent architectural advances in scaling\nautonomous agents for real-world deployment.\nThe release of ChatGPT in November 2022 marked a pivotal\ninflection point in the development and public perception of\nartificial intelligence, catalyzing a global surge in adoption,\ninvestment, and research activity [28]. In the wake of this\nbreakthrough, the AI landscape underwent a rapid transforma-\ntion, shifting from the use of standalone LLMs toward more\nautonomous, task-oriented frameworks [29]. This evolution\nprogressed through two major post-generative phases: AI\nAgents and Agentic AI. Initially, the widespread success of\nChatGPT popularized Generative Agents, which are LLM-\nbased systems designed to produce novel outputs such as text,\nimages, and code from user prompts [30], [31]. These agents\nwere quickly adopted across applications ranging from con-\nversational assistants (e.g., GitHub Copilot [32]) and content-\ngeneration platforms (e.g., Jasper [33]) to creative tools (e.g.,\nMidjourney [34]), revolutionizing domains like digital design,\nmarketing, and software prototyping throughout 2023 and\nbeyond.\nAlthough the term AI Agent was first introduced in 1998\n[3], it has since evolved significantly with the rise of generative\nAI. Building upon this generative foundation, a new class of\nsystems commonly referred to as AI Agents has emerged.\nThese agents enhanced LLMs with capabilities for external\ntool use (e.g., API-based tools), function calling, and sequen-\ntial reasoning, enabling them to retrieve real-time information\nand execute multi-step workflows autonomously [35], [36].\nExample frameworks such as AutoGPT [37] and BabyAGI\n(https://github.com/yoheinakajima/babyagi) highlight this tran-\nsition, showcasing how LLMs could be embedded within\nfeedback loops to dynamically plan, act, and adapt in goal-\ndriven environments [38], [39]. By late 2023, the field had\nadvanced further into the realm of Agentic AI complex, multi-\nagent systems in which specialized agents collaboratively\ndecompose goals, communicate, and coordinate toward shared\nobjectives. In line with this evolution, Google introduced the\nAgent-to-Agent (A2A) protocol in 2025 [40], a proposed\nstandard designed to enable seamless interoperability among\nagents across different frameworks and vendors. The protocol\nis built around five core principles: embracing agentic capabil-\nities, building on existing standards, securing interactions by\ndefault, supporting long-running tasks, and ensuring modality\nagnosticism. These guidelines aim to lay the groundwork for\na responsive, scalable agentic infrastructure.\nArchitectures such as CrewAI demonstrate how these agen-\ntic frameworks can accomplish decision-making across dis-\ntributed roles, facilitating intelligent behavior in high-stake\napplications including robotics, logistics management, and\nadaptive decision-support [41]–[44].\nAs the field progresses from Generative Agents toward\nincreasingly autonomous systems of Agentic AI , it becomes\ncritically important to delineate the technological and concep-\ntual boundaries between AI Agents and Agentic AI. While\nboth paradigms build upon LLMs and extend the capabilities\nof generative systems, they embody fundamentally different\narchitectures, interaction models, and levels of autonomy. AI\nAgents are typically designed as single-entity systems that\nperform goal-directed tasks by utilizing external tools, apply-\ning sequential reasoning, and integrating real-time information\nto complete well-defined functions [24], [45]. In contrast,\nAgentic AI systems are composed of multiple, specialized\nagents that coordinate, communicate, and dynamically allocate\nsub-tasks within a broader workflow to achieve a common\ngoal(s) [21], [46]. This architectural distinction highlights\nclear and significant differences in scalability, adaptability, and\napplication scope.\nUnderstanding and formalizing the taxonomy between these\ntwo paradigms (AI Agents and Agentic AI) is scientifically\nsignificant for several reasons. First, it enables more precise\nsystem design by aligning computational frameworks with\nproblem complexity ensuring that AI Agents are deployed for\nmodular, tool-assisted tasks, while Agentic AI is employed\nfor orchestrated multi-agent operations. Moreover, it allows\n2\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nfor appropriate benchmarking and evaluation: performance\nmetrics, safety protocols, and resource requirements differ\nsubstantially between agents designed for carrying out indi-\nvidual tasks and system of distributed agents designed for\naccomplishing complex, coordinated tasks. Additionally, clear\ntaxonomy reduces development inefficiencies by preventing\nthe misapplication of design principles such as assuming inter-\nagent collaboration in a system designed for single-agent\nexecution. Without this clarity, developers and practitioners\nrisk both under-engineering complex scenarios that require\nagentic coordination and over-engineering simple applications\nthat could be solved with a single AI Agent.\nThis article aims to clarify the differences between AI\nAgents and Agentic AI, providing researchers with a foun-\ndational understanding of these technologies. The objective\nof this study is to formalize the distinctions, establish a\nshared vocabulary, and provide a structured taxonomy between\nAI Agents and Agentic AI that informs the next generation\nof intelligent agent design across academic and industrial\ndomains, as illustrated in Figure 2.\nAI Agents\n&\nAgentic AI\nArchitecture\nMechanisms\nScope/\nComplexity\nInteraction\nAutonomy\nFig. 2: Mind map of this research for exploring concepts, ap-\nplications and challenges of AI Agents and Agentic AI. Each\ncolor-coded branch represents a key dimension of comparison:\nArchitecture, Mechanisms, Scope/Complexity, Interaction, and\nAutonomy.\nThis review also provides a comprehensive conceptual and\narchitectural analysis of the progression from traditional AI\nAgents to emergent Agentic AI systems. Rather than orga-\nnizing the study around formal research questions common\nin review articles, we adopt a sequential, layered structure\nthat clearly lays out the historical and technical evolution\nof these paradigms. Beginning with a detailed description of\nour search strategy and selection criteria, we first establish\nthe foundational understanding of AI Agents by analyzing\ntheir defining attributes, such as autonomy, reactivity, and\ntool-based execution. We then explore the critical role of\nfoundational models, specifically LLMs and Large Image\nModels (LIMs), which serve as the core reasoning and percep-\ntual engines that drive agentic behavior. Subsequent sections\nexamine how generative AI systems have served as precursors\nto more dynamic, interactive agents, setting the stage for the\nemergence of Agentic AI. Through this perspective, we exam-\nine and present the conceptual leap from isolated, single-agent\nsystems to orchestrated multi-agent architectures, highlight-\ning their structural distinctions, coordination strategies, and\ncollaborative mechanisms. We further map the architectural\nevolution by analyzing the core system components of both\nAI Agents and Agentic AI, offering comparative description\nof their planning, memory, orchestration, and execution layers.\nBuilding on this foundation, we review application domains\nspanning customer support, healthcare, research automation\ntasks, and robotics, while categorizing real-world deployments\nby system capabilities and coordination complexity. We then\nassess key challenges faced by both paradigms including hallu-\ncination, limited reasoning depth, causality deficits, scalability\nissues, and governance risks. To address these limitations, we\noutline opportunities for emerging solutions such as retrieval-\naugmented generation, tool-based reasoning, memory architec-\ntures, and simulation-based planning. The review concludes\nwith a forward-looking roadmap that envisions the conver-\ngence of modular AI Agents and orchestrated Agentic AI in\nmission-critical domains such as autonomous vehicles, finance,\nand healthcare, and beyond. We aim to provide researchers\nwith a structured taxonomy and actionable insights to guide the\ndesign, deployment, and evaluation of next-generation agentic\nAI systems.\nA. Methodology Overview\nThis review adopts a structured, multi-stage methodology\ndesigned to capture the evolution, architecture, application,\nand limitations of AI Agents and Agentic AI. The process\nis visually summarized in Figure 3, which delineates the se-\nquential flow of topics and concepts explored in this study. The\nanalytical framework was organized to analyze and present the\nprogression from basic agentic constructs rooted in LLMs to\nadvanced multi-agent orchestration systems. Each step of the\nreview was based on the rigorous synthesis of the literature\nfrom across academic sources and AI-powered platforms, en-\nabling a comprehensive understanding of the current landscape\nand its emerging trends.\nThe review begins by establishing a foundational under-\nstanding of AI Agents, examining their core definitions, design\nprinciples, and architectural modules as described in the litera-\nture. These include components such as perception, reasoning,\nand action selection, along with early applications like cus-\ntomer service bots and retrieval assistants. This foundational\nlayer serves as the conceptual entry point into the broader\nagentic paradigm.\nNext, we discuss the role of LLMs as core reasoning\ncomponents, emphasizing how pre-trained language models\n3\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nHybrid LiteratureSearch Strategy\nFoundationalUnderstandingof AI Agents\nCore Characteristicsof AI Agents\nFoundationalModels:LLMs and LIMs\nGenerative AIas a Precursor\nLanguageModels asthe Engine for AIAgent Progression\nEmergence ofAgentic AIfrom AI AgentFoundations\nConceptual Leap:From Isolatedto CoordinatedSystems\nKey Differences:AI Agents vs.Agentic AI\nArchitecturalEvolution:From AI Agentsto Agentic AI\nApplicationof AI Agentsand Agentic AI\nChallenges andLimitationsin AI Agentsand Agentic AI\nPotentialSolutions andFuture Roadmap\nFig. 3: Methodology pipeline of this study showing the pro-\ngression from AI Agent foundations to Agentic AI, followed\nby their architectural evolution, applications, limitations, and\nfuture solution strategies.\nenable modern AI Agents. This section details how LLMs,\nthrough instruction fine-tuning and reinforcement learning\nfrom human feedback (RLHF), enable natural language in-\nteraction, planning, and limited decision-making capabilities.\nWe also identify their limitations, such as hallucinations, static\nknowledge, and a lack of causal reasoning.\nBuilding on these foundations, the review proceeds to the\nemergence of Agentic AI , which represents a significant con-\nceptual advancement. Here, we highlight the transformation\nfrom tool-augmented single-agent systems to collaborative,\ndistributed ecosystems of interacting agents. This shift is\ndriven by the need for systems capable of decomposing\ngoals, assigning subtasks, coordinating outputs, and adapting\ndynamically to changing contexts, which are the capabilities\nthat surpass what isolated AI Agents can offer.\nThe next section examines the architectural evolution from\nAI Agents to Agentic AI systems , contrasting simple, modular\nagent designs with complex orchestration frameworks. We\ndescribe enhancements such as persistent memory, meta-agent\ncoordination, multi-agent planning loops (e.g., ReAct and\nChain-of-Thought prompting), and semantic communication\nprotocols. Comparative architectural analysis is supported with\nexamples from platforms like AutoGPT, CrewAI, and Lang-\nGraph.\nFollowing the architectural exploration, the review presents\nan in-depth analysis of application domains where AI Agents\nand Agentic AI are being deployed. The paper discusses four\nrepresentative application areas for each paradigm. For AI\nAgents, these include Customer Support Automation, Inter-\nnal Enterprise Search, Email Filtering and Prioritization, and\nPersonalized Content Recommendation. For Agentic AI, the\napplications span Multi-Agent Research Assistants, Intelligent\nRobotics Coordination, Collaborative Medical Decision Sup-\nport, and Adaptive Workflow Automation. These use cases\nare examined with respect to system complexity, real-time\ndecision-making, and collaborative task execution.\nSubsequently, we address the challenges and limitations\ninherent to both paradigms. For AI Agents, we focus on issues\nlike hallucination, prompt brittleness, limited planning ability,\nand lack of causal understanding. For Agentic AI, we identify\nhigher-order challenges such as inter-agent misalignment, error\npropagation, unpredictability of emergent behavior, explain-\nability deficits, and adversarial vulnerabilities. These problems\nare critically examined with references to recent experimental\nstudies and technical reports.\nFinally, the review outlines potential solutions to over-\ncome these challenges , drawing on recent advances in causal\nmodeling, retrieval-augmented generation (RAG) , multi-agent\nmemory frameworks, and robust evaluation pipelines. These\nstrategies are discussed not only as technical fixes but as foun-\ndational requirements for scaling agentic systems into high-\nstakes domains such as healthcare, finance, and autonomous\nrobotics.\nIn summary, this methodological structure enables a sys-\ntematic and comprehensive assessment of the state of AI\nAgents and Agentic AI. By sequencing the analysis from\nfoundational understanding, to model integration, architectural\nadvancements, applications, and to limitations and potential\nsolutions, the study aims to provide both theoretical clarity and\npractical guidance to researchers and practitioners navigating\nthis rapidly evolving field.\n1) Search Strategy: To develop this review, we imple-\nmented a hybrid search methodology combining traditional\nacademic repositories and AI-enhanced literature discovery\ntools. Specifically, twelve platforms were queried: academic\ndatabases such as Google Scholar, IEEE Xplore, ACM Digital\nLibrary, Scopus, Web of Science, ScienceDirect, and arXiv;\nand AI-powered interfaces including ChatGPT, Perplexity.ai,\nDeepSeek, Hugging Face Search, and Grok. Search queries\nincorporated Boolean combinations of terms such as “AI\n4\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nAgents”, “Agentic AI”, “LLM Agents”, “Tool-augmented\nLLMs”, and “Multi-Agent AI Systems”.\nTargeted queries such as “Agentic AI + Coordination +\nPlanning”, and “AI Agents + Tool Usage + Reasoning” were\nalso employed to retrieve papers addressing both conceptual\nunderpinnings and system-level implementations. Literature\ninclusion was based on their the significance in terms of nov-\nelty, empirical evaluation, architectural contribution, and cita-\ntion impact. The rising global interest in these technologies, as\nillustrated in Figure 1 using Google Trends data, underscores\nthe urgency of synthesizing this emerging knowledge space.\nII. F OUNDATIONAL UNDERSTANDING OF AI AGENTS\nAI Agents can be defined as autonomous software entities\nengineered for goal-directed task execution within bounded\ndigital environments [21], [47]. These agents are defined\nby their ability to perceive structured or unstructured in-\nputs [48], to reason over contextual information [49], [50],\nand to initiate actions toward achieving specific objectives,\noften acting as surrogates for human users or subsystems\n[51]. Unlike conventional automation scripts, which follow\ndeterministic workflows, AI Agents demonstrate reactive in-\ntelligence and some level of adaptability, allowing them to\ninterpret dynamic inputs and reconfigure outputs accordingly\n[52]. Their adoption has been reported across a wide range of\napplication domains, including customer service automation\n[53], [54], personal productivity assistance [55], organizational\ninformation retrieval [56], [57], and decision support systems\n[58], [59].\nA notable example of autonomous AI agents in Anthropic’s\n”Computer Use” project computer use, which showcases how\ntheir Claude model can interact with a computer in much the\nsame way a human would. In this project, Claude is trained\nto visually interpret what’s on a computer screen, control the\nmouse and keyboard, and navigate through various software\napplications. This allows Claude to automate repetitive tasks,\nsuch as filling out forms or copying data, as well as more\ncomplex activities like building and testing software by open-\ning code editors, running commands, and debugging issues.\nBeyond these structured tasks, Claude can also handle open-\nended assignments like conducting online research, gathering\nand organizing information from multiple sources, and even\ncreating calendar events based on its findings. The key in-\nnovation is that Claude operates in an ”agent loop,” where\nit receives a goal, decides on the next action, performs that\naction, observes the result, and repeats this process until the\ntask is complete. This enables Claude to independently use\nexisting computer tools and interfaces to accomplish a wide\nrange of objectives, making it a powerful example of how\nautonomous AI agents can automate both routine and complex\nworkflows.\n1) Core Characteristics of AI Agents: AI Agents are widely\nconceptualized as instantiated operational instances of artificial\nintelligence designed to interface with users, software ecosys-\ntems, or digital infrastructures to develop goal-directed behav-\nior [60]–[62]. These agents are different than general-purpose\nLLMs in the sense that they exhibit structured initialization,\nbounded autonomy, and persistent task orientation. While\nLLMs primarily function as reactive prompt followers [63], AI\nAgents operate automatically within explicitly defined scopes,\nengaging dynamically with inputs and producing actionable\noutputs in real-time environments [64].\nFigure 4 illustrates the three foundational characteristics\ncommonly incorporated by architectural taxonomies and em-\npirical deployments of AI Agents. These characteristics in-\nclude autonomy, task-specificity, and reactivity with adapta-\ntion.\nTogether, these three characteristics provide a foundational\nframework for understanding and evaluating AI Agents across\ndeployment scenarios. The remainder of this section elaborates\non each characteristic, offering theoretical background and\nillustrative examples.\n• Autonomy: A central feature of AI Agents is their\nability to function with minimal or no human intervention\nafter deployment [65]. Once initialized, these agents are\ncapable of perceiving environmental inputs, reasoning\nover contextual data, and executing predefined or adaptive\nactions in real-time [24]. Autonomy enables scalable\ndeployment in applications where persistent oversight\n(human-in-the-loop) is impractical, such as customer sup-\nport bots or scheduling assistants [54], [66].\n• Task-Specificity: AI Agents are purpose-built for narrow,\nand well-defined tasks [67], [68]. They are optimized to\nexecute repeatable operations within a fixed domain, such\nas email filtering [69], [70], database querying [71], or\ncalendar coordination [46], [72]. This task specialization\nallows for efficiency, interpretability, and high precision\nin automating tasks where general-purpose reasoning is\nunnecessary or inefficient.\n• Reactivity and Adaptation: AI Agents often include\nbasic mechanisms for interacting with dynamic inputs,\nallowing them to respond to real-time stimuli such as user\nrequests, external API calls, or state changes in software\nenvironments [24], [73]. Some systems integrate basic\nlearning capabilities [74] through feedback loops [75],\n[76], heuristics [77], or updated context buffers to refine\nbehavior over time, particularly in settings like personal-\nized recommendations or conversation flow management\n[78]–[80].\nThese core characteristics collectively enable AI Agents to\nserve as modular, lightweight interfaces between pretrained AI\nmodels and domain-specific utility pipelines. Their architec-\ntural simplicity and operational efficiency position them as key\nenablers of scalable automation across enterprise, consumer,\nand industrial settings. Although there are currently no studies\nexplicitly involving AI Agents integrated with specialized\nreasoning LLMs, their high usability and performance within\nconstrained task boundaries have made them foundational\ncomponents in contemporary intelligent system design.\n2) Foundational Models: The Role of LLMs and LIMs: The\nprogress in AI Agents has been significantly accelerated by the\nfoundational development and deployment of LLMs and LIMs,\n5\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nAI Agents\nFig. 4: Illustration of core characteristics of AI Agents autonomy, task-specificity, and reactivity for agent design and operational\nbehavior.\nwhich serve as the core reasoning and perception engines in\ncontemporary agent systems. These models enable AI agents\nto interact intelligently with their environments, understand\nmulti-modal inputs, and perform complex reasoning tasks that\ngo beyond hard-coded automation.\nLLMs such as GPT-4 [81] and PaLM [82] are trained on\nmassive datasets of text from books, web content, and dialogue\ncorpora. These models exhibit emergent capabilities in natural\nlanguage understanding, question answering, summarization,\ndialogue coherence, and even symbolic reasoning [83]–[85].\nWithin AI Agent architectures, LLMs serve as the primary\ndecision-making engine, allowing the agent to parse user\nqueries, plan multi-step solutions, and generate human-like\nresponses. For instance, an AI customer support agent powered\nby GPT-4 can interpret customer complaints, query backend\nsystems via tool integration, and respond in a contextually\nappropriate and emotionally aware manner [86], [87].\nLarge Image Models (LIMs) such as CLIP [88] and BLIP-\n2 [89] extend the agent’s capabilities into the visual domain.\nTrained on image-text pairs, LIMs enable perception-based\ntasks including image classification, object detection, and\nvision-language grounding. These capabilities are increasingly\nvital for agents operating in domains such as robotics [90],\nautonomous vehicles [91], [92], and visual content moderation\n[93], [94].\nFor example, as illustrated in Figure 5 where an autonomous\ndrone agent is tasked with monitoring orchards, a LIM can\nidentify diseased fruits [95] or damaged branches by in-\nterpreting live aerial imagery. Upon detection, the system\nautonomously triggers predefined intervention protocols, such\nas notifying horticultural staff or marking the location for\ntargeted treatment without requiring human involvement [24],\nFig. 5: An AI agent–enabled drone autonomously inspects\nan orchard, identifying diseased fruits and damaged branches\nusing vision models, and triggers real-time alerts for targeted\nhorticultural interventions.\n[65]. This workflow exemplifies the autonomy and reactivity\nof AI Agents in agricultural environments as highlighted\nby recent literature indicating the growing sophistication of\nsuch drone-based AI Agents. Chitra et al. [96] provides a\ncomprehensive overview of AI algorithms foundational to\nembodied agents, highlighting the integration of computer\nvision, SLAM, reinforcement learning, and sensor fusion.\nThese components collectively support real-time perception\nand adaptive navigation in dynamic environments. Kourav\net al. [97] further emphasize the role of natural language\nprocessing and LLMs in generating drone action plans from\n6\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nhuman-issued queries, demonstrating how LLMs support nat-\nuralistic interaction and mission planning. Similarly, Natarajan\net al. [98] explore deep learning and reinforcement learning\nfor scene understanding, spatial mapping, and multi-agent\ncoordination in aerial robotics. These studies converge on the\ncritical importance of AI-driven autonomy, perception, and\ndecision-making in advancing drone-based agents.\nImportantly, LLMs and LIMs are often accessed via infer-\nence APIs provided by cloud-based platforms such as OpenAI\nhttps://openai.com/, HuggingFace https://huggingface.co/, and\nGoogle Gemini https://gemini.google.com/app. These services\nabstract away the complexity of model training and fine-\ntuning, enabling developers to rapidly build and deploy agents\nequipped with state-of-the-art reasoning and perceptual abil-\nities. This integrability accelerates prototyping and allows\nagent frameworks like LangChain [99] and AutoGen [100]\nto orchestrate LLM and LIM outputs across task workflows.\nIn short, foundational AI models give modern AI Agents their\nbasic understanding of language and scenes. Language models\nhelp them reason with words, and image models help them\nunderstand pictures; working together, they allow AI Agents\nto make smart decisions in complex situations.\n3) Generative AI as a Precursor: A consistent theme in the\nliterature is the positioning of generative AI as the foundational\nprecursor to agentic intelligence. These systems primarily op-\nerate on pre-trained LLMs and LIMs, which are optimized to\nsynthesize multi-modal content including text, images, audio,\nor code based on input prompts. While highly communicative,\ngenerative models fundamentally exhibit reactive behavior:\nthey produce output only when explicitly prompted and do\nnot pursue goals autonomously or engage in self-initiated\nreasoning [101], [102].\nKey Characteristics of Generative AI:\n• Reactivity: As non-autonomous systems, generative\nmodels are exclusively input-driven [103], [104]. Their\noperations are triggered by user-specified prompts and\nthey lack internal states, persistent memory, or goal-\nfollowing mechanisms [105]–[107].\n• Multi-modal Capability: Modern generative systems\ncan produce a diverse array of outputs, including coherent\nnarratives, executable code, realistic images, and even\nspeech transcripts. For instance, models like GPT-4 [81],\nPaLM-E [108], and BLIP-2 [89] demonstrate these capa-\nbilities, enabling language-to-image, image-to-text, and\ncross-modal synthesis tasks.\n• Prompt Dependency and Statelessness: Although gen-\nerative systems are stateless in that they do not retain\ncontext across interactions unless explicitly prompted\n[109], [110], recent advancements like GPT-4.1 support\nlarger context windows-up to 1 million tokens-and are\nbetter able to utilize that context enabled by the im-\nproved long-text comprehension [111]. Their design also\nlacks intrinsic feedback loops [112], state management\n[113], [114], or multi-step planning a requirement for au-\ntonomous decision-making and iterative goal refinement\n[115], [116].\nDespite their remarkable generative fidelity, these systems\nare constrained by their inability to act upon the environment\nor manipulate digital tools independently. For instance, they\ncannot search the internet, parse real-time data, or interact\nwith APIs without human-engineered wrappers or scaffolding\nlayers. As such, they fall short of being classified as true\nAI Agents, whose architectures integrate perception, decision-\nmaking, and external tool-use within closed feedback loops.\nThe limitations of generative AI in handling dynamic tasks,\nmaintaining state continuity, or executing multi-step plans led\nto the development of tool-augmented systems, commonly\nreferred to as AI Agents [117]. These systems build upon\nthe language processing backbone of LLMs but introduce\nadditional infrastructure such as memory buffers, tool-calling\nAPIs, reasoning chains, and planning routines to bridge the\ngap between passive response generation and active task\ncompletion. This architectural evolution marks a critical shift\nin AI system design: from content creation to autonomous task\nexecution [118], [119]. The trend from generative systems to\nAI Agents highlights a progressive layering of functionality\nthat ultimately supports the emergence of agentic behaviors.\nA. Language Models as the Engine for AI Agent Progression\nThe emergence of AI Agent as a transformative paradigm\nin artificial intelligence is closely tied to the evolution and\nrepurposing of large-scale language models such as GPT-3\n[120], Llama [121], T5 [122], Baichuan 2 [123] and GPT3mix\n[124]. A substantial and growing body of research shows\nthat the advancement, from reactive generative models to\nautonomous, goal-directed agents is driven by the integration\nof LLMs as core reasoning engines within dynamic agentic\nsystems. These models, originally trained for natural language\nprocessing tasks, are increasingly embedded in frameworks\nthat require adaptive planning [125], [126], real-time decision-\nmaking [127], [128], and environment-aware behavior [129].\n1) LLMs as Core Reasoning Components:\nLLMs such as GPT-4 [81], PaLM [82], Claude\nhttps://www.anthropic.com/news/claude-3-5-sonnet, and\nLLaMA [121] are pre-trained on massive text corpora using\nself-supervised objectives and fine-tuned using techniques\nsuch as Supervised Fine-Tuning (SFT) and Reinforcement\nLearning from Human Feedback (RLHF) [130], [131]. These\nmodels encode rich statistical and semantic knowledge,\nallowing them to perform tasks like inference, summarization,\ncode generation, and dialogue management. However, in\nagentic contexts, their capabilities extend beyond response\ngeneration. They function as cognitive engines that interpret\nuser goals, formulate and evaluate possible action plans,\nselect the most appropriate strategies, leverage external tools,\nand manage complex, multi-step workflows.\nRecent work identifies these models as central\nto the architecture of contemporary agentic\nsystems. For instance, AutoGPT [37] and BabyAGI\nhttps://github.com/yoheinakajima/babyagi use GPT-4 as\nboth a planner and executor: the model analyzes high-level\nobjectives, decomposes them into actionable subtasks, invokes\n7\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nexternal APIs as needed, and monitors progress to determine\nsubsequent actions. In such systems, the LLM operates in a\nloop of prompt processing, state updating, and feedback-based\ncorrection, closely emulating autonomous decision-making.\n2) Tool-Augmented AI Agents: Enhancing Functionality:\nTo overcome limitations inherent to generative-only systems\nsuch as hallucination, static knowledge cutoffs, and restricted\ninteraction scopes, researchers have proposed the concept\nof tool-augmented AI Agents [132] such as Easytool [133],\nGentopia [134], and ToolFive [135]. These systems integrate\nexternal tools, APIs, and computation platforms into the\nagent’s reasoning pipeline, allowing for real-time information\naccess, code execution, and interaction with dynamic data\nenvironments.\nTool Invocation. When an agent identifies a need that\ncannot be addressed through its internal knowledge such as\nquerying a current stock price, retrieving up-to-date weather\ninformation, or executing a script, it generates a structured\nfunction call or API request [136], [137]. These calls are\ntypically formatted in JSON, SQL, or Python dictionary,\ndepending on the target service, and routed through an or-\nchestration layer that executes the task.\nResult Integration. Once a response is received from the\ntool, the output is parsed and reincorporated into the LLM’s\ncontext window. This enables the agent to synthesize new\nreasoning paths, update its task status, and decide on the next\nstep. The ReAct framework [138] exemplifies this architecture\nby combining reasoning (Chain-of-Thought prompting) and\naction (tool use), with LLMs alternating between internal\ncognition and external environment interaction. A prominent\nexample of a tool-augmented AI agent is ChatGPT, which,\nwhen unable to answer a query directly, autonomously invokes\nthe Web Search API to retrieve more recent and relevant\ninformation, performs reasoning over the retrieved content,\nand formulates a response based on its understanding [139].\n3) Illustrative Examples and Emerging Capabilities: Tool-\naugmented LLM-powered AI Agents have demonstrated po-\ntentials across a range of applications. In AutoGPT [37], the\nagent may plan a product market analysis by sequentially\nquerying the web, compiling competitor data, summarizing\ninsights, and generating a report. In a coding context, tools\nlike GPT-Engineer combine LLM-driven design with local\ncode execution environments to iteratively develop software\nartifacts as output produced during the development process,\nincluding source code, .exe files, documentation and configura-\ntions [140], [141]. In research domains, systems like Paper-QA\n[142] utilize LLMs to query vectorized academic databases,\ngrounding answers in retrieved scientific literature to ensure\nfactual integrity.\nThese capabilities have opened pathways for more robust\nbehavior of AI Agents such as long-horizon planning, cross-\ntool coordination, and adaptive learning loops. Nevertheless,\nthe inclusion of tools also introduces new challenges in coordi-\nnation complexity, error propagation, and context window lim-\nitations, which are all active areas of research. The progression\ntoward AI Agents is inseparable from the strategic integration\nof LLMs as reasoning engines and their augmentation through\nstructured utilization of external tools like search engines and\nAPIs. This synergy transforms static language models into\ndynamic cognitive agents capable of perceiving, planning,\nacting, and adapting, thus setting the stage for multi-agent\ncollaboration, persistent memory, and scalable autonomy, the\ncharacteristics of the Agentic AI systems.\nAs an example, Figure 6 illustrates a representative use-\ncase: a news query agent that performs real-time web search,\nsummarizes retrieved documents, and generates an articulate,\ncontext-aware answer. Such workflows have been demon-\nstrated in implementations using LangChain, AutoGPT, and\nOpenAI function-calling architectures.\nFig. 6: Illustrating the workflow of an AI Agent performing\nreal-time news search, summarization, and answer generation\nIII. T HE EMERGENCE OF AGENTIC AI FROM AI AGENT\nFOUNDATIONS\nWhile AI Agents represent a significant leap in artificial\nintelligence capabilities, particularly in automating narrow\ntasks through tool-augmented reasoning, recent literature iden-\ntifies notable limitations that constrain their scalability in\ncomplex, dynamic, multi-step, and/or cooperative scenarios\n[143]–[145]. These constraints have catalyzed the development\nof a more advanced paradigm: Agentic AI. This emerging class\nof systems extends the capabilities of traditional AI Agents by\nenabling multiple intelligent entities to collaboratively pursue\ngoals through structured communication [146]–[148], shared\nmemory [149], [150], and dynamic role assignment [21].\n1) Conceptual Leap: From Isolated Agents to Coordinated\nSystems: AI Agents, as explored in prior sections, integrate\nLLMs with external tools and APIs to execute narrowly scoped\noperations such as responding to customer queries, performing\ndocument retrieval, or managing schedules. However, as use\ncases increasingly demand context retention, task interde-\npendence, and adaptability across dynamic environments, the\nsingle-agent model proves insufficient [151], [152].\nAgentic AI systems represent an emergent class of intelli-\ngent architectures in which multiple specialized agents collab-\n8\nAI Agents vs. Agentic AI by Sapkota et al. 2025\norate to achieve complex, high-level objectives utilizing col-\nlaborative reasoning and multi-step planning [40]. As defined\nin recent frameworks, these systems are composed of modular\nagents each tasked with a distinct subcomponent of a broader\ngoal and coordinated through either a centralized orchestrator\nor a decentralized protocol [23], [147]. This structure signifies\na conceptual departure from the individual, reactive behaviors\ntypically observed in single-agent architectures, toward a form\nof system-level intelligence characterized by dynamic inter-\nagent collaboration.\nA key enabler of this paradigm is goal decomposition ,\nwherein a user-specified objective is automatically parsed and\ndivided into smaller, manageable tasks by planning agents\n[46]. These subtasks are then distributed across the agent\nnetwork. Multi-step reasoning and planning mechanisms\nfacilitate the dynamic sequencing of these subtasks, allowing\nthe system to adapt in real time to environmental changes or\npartial task failures. This agentic architecture ensures robust\ntask execution even under uncertainty [21].\nInter-agent communication is mediated through distributed\ncommunication channels , such as asynchronous messaging\nqueues, shared memory buffers, or intermediate output ex-\nchanges, enabling coordination without necessitating contin-\nuous central oversight [21], [153]. Furthermore, reflective\nreasoning and memory systems allow agents to store context\nacross multiple interactions, evaluate past decisions, and itera-\ntively refine their strategies [154]. Collectively, these capabili-\nties enable Agentic AI systems to exhibit flexible, adaptive,\ncooperative, and collaborative intelligence that exceeds the\noperational limits of individual agents.\nA widely accepted conceptual illustration in the literature\ndelineates the distinction between AI Agents and Agentic AI\nthrough the analogy of smart home systems. As depicted in\nFigure 7, the left side represents a traditional AI Agent in the\nform of a smart thermostat. This standalone agent receives\na user-defined temperature setting and autonomously controls\nthe heating or cooling system to maintain the target tempera-\nture. While it demonstrates limited autonomy such as learning\nuser schedules or reducing energy usage during absence, it\noperates in isolation, executing a singular, well-defined task\nwithout engaging in broader environmental coordination or\ngoal inference [24], [65].\nIn contrast, the right side of Figure 7 illustrates an Agentic\nAI system embedded in a comprehensive smart home ecosys-\ntem. Here, multiple specialized agents interact synergistically\nto manage diverse aspects such as weather forecasting, daily\nscheduling, energy pricing optimization, security monitoring,\nand backup power activation. These agents are not just reactive\nmodules; they communicate dynamically, share memory states,\nand collaboratively align actions toward a high-level system\ngoal (e.g., optimizing comfort, safety, and energy efficiency\nin real-time). For instance, a weather forecast agent might\nsignal upcoming heatwaves, prompting early pre-cooling via\nsolar energy before peak pricing hours, as coordinated by an\nenergy management agent. Simultaneously, the system might\ndelay high-energy tasks or activate surveillance systems during\noccupant absence, integrating decisions across domains. This\nfigure embodies the architectural and functional leap from\ntask-specific automation to adaptive, orchestrated intelligence.\nThe AI Agent acts as a deterministic component with limited\nscope, while Agentic AI reflects distributed intelligence, char-\nacterized by goal decomposition, inter-agent communication,\nand contextual adaptation, demonstrating key characteristics\nof the modern agentic AI frameworks.\n2) Key Differences between AI Agents and Agentic AI: To\nsystematically capture the evolution from Generative AI to AI\nAgents and further to Agentic AI, we structure our compara-\ntive analysis around a foundational taxonomy where Genera-\ntive AI serves as the baseline. While AI Agents and Agentic\nAI systems represent increasingly autonomous and interactive\nsystems, both paradigms utilize generative architectures as\ntheir foundations, especially LLMs and LIMs. Consequently,\neach comparative table in this subsection includes Generative\nAI as a reference column to highlight how agentic behavior\nbuilds on and then diverges from generative AI foundations.\nA set of basic distinctions between AI Agents and Agentic\nAI systems, particularly in terms of scope, autonomy, archi-\ntectural composition, coordination strategy, and operational\ncomplexity, are synthesized in Table I, which was derived\nfrom close analysis of prominent frameworks such as Au-\ntoGen [100] and ChatDev [155]. This comparison provides\na multi-dimensional view of how single-agent systems tran-\nsition into coordinated, multi-agent ecosystems. Through the\nperspective of generative capabilities, we trace the increasing\nsophistication in planning, communication, and adaptation that\ncharacterizes the shift toward Agentic AI systems.\nWhile Table I delineates the foundational and operational\ndifferences between AI Agents and Agentic AI systems, a\nmore granular taxonomy is required to understand how these\nparadigms emerge from and relate to broader generative AI\nframeworks. Specifically, the conceptual and cognitive pro-\ngression from static Generative AI systems to tool-augmented\nAI Agents, and further to collaborative Agentic AI ecosystems,\nnecessitates an integrated comparative framework. This transi-\ntion is not merely structural but also functional encompassing\nhow initiation mechanisms, memory use, learning capacities,\nand orchestration strategies evolve across the agentic spectrum.\nMoreover, recent studies suggest the emergence of hybrid\nparadigms such as ”Generative Agents,” which blend genera-\ntive modeling with modular task specialization, further com-\nplicating the agentic AI landscape. In order to capture these\nnuanced relationships, Table II synthesizes the key conceptual\nand cognitive dimensions across four archetypes: Generative\nAI, AI Agents, Agentic AI systems, and inferred Generative\nAgents. By positioning Generative AI as a baseline technology,\nthis taxonomy highlights the scientific, structural and appli-\ncation continuum that spans from passive content generation\nto interactive task execution and finally to autonomous, multi-\nagent orchestration. This multi-tiered perspective is critical for\nunderstanding both the current capabilities and future trends\nof agentic intelligence across theoretical and applied domains.\nTo further operationalize the distinctions outlined in Ta-\n9\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nFig. 7: Comparative illustration of AI Agent vs. Agentic AI, synthesizing conceptual distinctions. Left: A single-task AI Agent.\nRight: A multi-agent, collaborative Agentic AI system.\nble I, Tables III and II extend the comparison between agent\nparadigms to encompass a broader spectrum of paradigms\nincluding AI agents and agentic AI. Table III presents key\narchitectural and behavioral attributes that highlight how each\nparadigm differs in terms of primary capabilities, planning\nscope, interaction style, learning dynamics, and evaluation cri-\nteria. As can be seen from the tables, AI Agents are optimized\nfor discrete task execution with limited planning horizons and\nrely on supervised or rule-based learning mechanisms. In con-\ntrast, Agentic AI systems extend this capacity through multi-\nstep planning, meta-learning, and inter-agent communication,\npositioning them for use in complex environments requiring\nautonomous goal setting and coordination. Generative Agents,\nas a more recent construct, inherit LLM-centric pretraining ca-\npabilities and excel in producing multi-modal content creation,\nyet they lack the proactive orchestration and state-persistent\nbehaviors seen in Agentic AI systems.\nThe second table (Table III) provides a process-driven\ncomparison across three agent categories: Generative AI,\nAI Agents, and Agentic AI. This framing emphasizes how\nfunctional pipelines evolve from prompt-driven single-model\ninference in Generative AI, to tool-augmented execution in AI\nAgents, and finally to orchestrated agent networks in Agentic\nAI systems. The structure column highlights this progression:\nfrom single LLMs to integrated tool-chains and ultimately to\ndistributed multi-agent systems. Access to external data, a key\noperational requirement for real-world utility, also increases\nin sophistication, from absent or optional in Generative AI\nto modular and coordinated in Agentic AI. Collectively, these\ncomparative views reinforce that the evolution from generative\nto agentic paradigms is marked not just by increasing system\ncomplexity but also by deeper integration of autonomy, mem-\nory, and decision-making across multiple levels of abstraction.\nFurthermore, to provide a deeper multi-dimensional un-\nderstanding of the evolving agentic landscape, Tables V\nthrough IX extend the comparison over five critical dimen-\nsions: core function and goal alignment, architectural com-\nposition, operational mechanism, scope and complexity, and\ninteraction-autonomy dynamics. These dimensions serve to not\nonly reinforce the structural differences between Generative\nAI, AI Agents, and Agentic AI, but also introduce an emergent\ncategory of Generative Agents representing modular agents de-\nsigned for embedded subtask-level generation within broader\nworkflows [156]. Generative Agents are distinguished by their\nsimulated human-like behavior, achieved through tightly inte-\ngrated components such as language models, memory systems,\nand behavior planning modules, enabling them to operate\nbelievably and autonomously within typically closed-world\nenvironments. Table V situates the three paradigms in terms of\ntheir overarching goals and functional intent. While Generative\n10\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nTABLE I: Key Structural, Functional, and Operational Dif-\nferences Between AI Agents and Agentic AI Systems. This\ntable highlights the major distinctions between traditional AI\nAgents and more complex Agentic AI systems. It compares\ntheir definitions, levels of autonomy, capacity for handling\ntask complexity, collaboration styles, learning and adaptation\nscope, and typical application domains. The comparison illus-\ntrates the evolution from task-specific, independently operating\nagents to coordinated, multi-agent systems capable of manag-\ning dynamic and large-scale workflows.\nFeature AI Agents Agentic AI\nDefinition\nAutonomous\nsoftware\nprograms that\nperform specific\ntasks.\nSystems of multiple AI\nagents collaborating to\nachieve complex goals.\nAutonomy Level\nHigh autonomy\nwithin specific\ntasks.\nBroad level of autonomy\nwith the ability to\nmanage multi-step,\ncomplex tasks and\nsystems.\nTask\nComplexity\nTypically handle\nsingle, specific\ntasks.\nHandle complex,\nmulti-step tasks requiring\ncoordination.\nCollaboration Operate\nindependently.\nInvolve multi-agent\ninformation sharing,\ncollaboration and\ncooperation.\nLearning and\nAdaptation\nLearn and adapt\nwithin their\nspecific domain.\nLearn and adapt across a\nwider range of tasks and\nenvironments.\nApplications\nCustomer service\nchatbots, virtual\nassistants,\nautomated\nworkflows.\nSupply chain\nmanagement, business\nprocess optimization,\nvirtual project managers.\nAI centers on prompt-driven content generation, AI Agents\nemphasize tool-based task execution, and Agentic AI systems\norchestrate full-fledged workflows. This functional expansion\nis mirrored architecturally in Table VI, where the system de-\nsign transitions from single-model reliance (in Generative AI)\nto multi-agent orchestration and shared memory utilization in\nAgentic AI. Table VII then outlines how these paradigms differ\nin their workflow execution pathways, highlighting the rise\nof inter-agent coordination and hierarchical communication as\nkey drivers of agentic behavior.\nFurthermore, Table VIII explores the increasing scope and\noperational complexity handled by these systems ranging\nfrom isolated content generation to adaptive, multi-agent col-\nlaboration in dynamic environments. Finally, Table IX syn-\nthesizes the varying degrees of autonomy, interaction style,\nand decision-making granularity across the paradigms. These\ntables collectively establish a rigorous framework to classify\nand analyze agent-based AI systems, laying the groundwork\nfor theoretically grounded evaluation and future design of\nautonomous, intelligent, and collaborative agents operating at\nscale.\nTables V through IX offer a layered comparative analysis\nof Generative AI, AI Agents, and Agentic AI, anchoring\nthe taxonomy in operational and architectural traits. Table V\nhighlights core distinctions: Generative AI produces reactive\ncontent; AI Agents execute tool-based tasks; Agentic AI coor-\ndinates subagents for high-level workflow execution, marking\na key shift in AI autonomy.\nIn Table VI, the architectural distinctions are made explicit,\nespecially in terms of system composition and control logic.\nGenerative AI relies on a single model with no built-in\ncapability for tool use or delegation, whereas AI Agents\ncombine language models with auxiliary APIs and interface\nmechanisms to augment functionality. Agentic AI extends this\nfurther by introducing multi-agent systems where collabo-\nration, memory persistence, and orchestration protocols are\ncentral to the system’s operation. This expansion is crucial\nfor enabling intelligent delegation, context preservation, and\ndynamic role assignment capabilities absent in both generative\nand single-agent systems. Likewise in Table VII, differences\nin systems functionality and operation are presented, empha-\nsizing distinctions in execution logic and information flow.\nUnlike Generative AI’s linear pipeline (prompt → output),\nAI Agents implement procedural mechanisms to incorporate\ntool responses mid-process. Agentic AI introduces recursive\ntask reallocation and cross-agent messaging, thus facilitating\nemergent decision-making that cannot be captured by static\nLLM outputs alone. Table VIII further reinforces these dis-\ntinctions by mapping each system’s capacity to handle task\ndiversity, temporal scale, and operational robustness. Here,\nAgentic AI emerges as uniquely capable of supporting high-\ncomplexity goals that demand adaptive, multi-phase reasoning\nand execution strategies.\nFurthermore, Table IX highlights the operational and be-\nhavioral distinctions across Generative AI, AI Agents, and\nAgentic AI, with a particular focus on autonomy lev-\nels, interaction styles, and inter-agent coordination. Gen-\nerative AI models such as GPT-3 [120] and DALL·E\nhttps://openai.com/index/dall-e-3/, remain reactive generating\ncontent solely in response to prompts without maintaining a\npersistent state or engaging in iterative reasoning. In contrast,\nAI Agents such as those constructed with LangChain [99] or\nMetaGPT [157], exhibit a higher degree of autonomy, capable\nof initiating external tool invocations and adapting behaviors\nwithin bounded tasks. However, their autonomy is typically\nconfined to isolated task execution, lacking long-term state\ncontinuity or collaborative interaction.\nAgentic AI systems mark a significant departure from these\nparadigms by introducing internal orchestration mechanisms\nand multi-agent collaboration frameworks. For example, plat-\nforms like AutoGen [100] and ChatDev [155] exemplify agen-\ntic coordination through task decomposition, role assignment,\nand recursive feedback loops. In AutoGen, one agent might\nserve as a planner while another retrieves information and\na third synthesizes a report, each communicating through\nshared memory buffers and governed by an orchestrator agent\nthat monitors dependencies and overall task progression. This\nstructured coordination allows for more complex goal pur-\n11\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nTABLE II: Summary of the Conceptual and Cognitive Taxonomy of AI Agent Paradigms Across Initiation, Adaptation,\nand Coordination Dimensions. This table synthesizes core cognitive and operational characteristics of four AI system types,\ncomparing how they initiate tasks, adapt to goals, maintain temporal continuity, leverage memory, and coordinate actions.\nIt captures the spectrum from stateless, prompt-driven Generative AI to highly coordinated and adaptive Agentic AI, while\nsituating Generative Agents as modular components with localized generative capabilities within larger systems.\nConceptual Dimension Generative AI AI Agent Agentic AI Generative Agent\n(Inferred)\nInitiation Type Prompt-triggered by user or\ninput\nPrompt or goal-triggered\nwith tool use\nGoal-initiated or orchestrated\ntask\nPrompt or system-level trig-\nger\nGoal Flexibility (None) fixed per prompt (Low) executes specific goal (High) decomposes and\nadapts goals\n(Low) guided by subtask\ngoal\nTemporal Continuity Stateless, single-session out-\nput\nShort-term continuity within\ntask\nPersistent across workflow\nstages\nContext-limited to subtask\nLearning/Adaptation Static (pretrained) (Might in future) Tool selec-\ntion strategies may evolve\n(Yes) Learns from outcomes Typically static; limited\nadaptation\nMemory Use No memory or short context\nwindow\nOptional memory or tool\ncache\nShared episodic/task mem-\nory\nSubtask-level or contextual\nmemory\nCoordination Strategy None (single-step process) Isolated task execution Hierarchical or decentralized\ncoordination\nReceives instructions from\nsystem\nKey Role Content generator Tool-based task executor Collaborative workflow or-\nchestrator\nSubtask-level modular gener-\nator\nTABLE III: Key Differentiating Attributes of AI Agents,\nAgentic AI, and Generative Agents Across Capability, Learn-\ning, and Interaction Dimensions. This table outlines critical\ndistinctions among three AI paradigms by comparing their\nprimary capabilities, planning horizons, learning mechanisms,\ninteraction styles, and evaluation criteria. It highlights the pro-\ngression from task-specific execution in traditional AI Agents\nto the autonomous and adaptive behaviors of Agentic AI, and\nthe creative, content-focused nature of Generative Agents.\nAspect AI Agent Agentic AI Generative\nAgent\nPrimary Ca-\npability\nTask execution Autonomous\ngoal setting\nContent genera-\ntion\nPlanning\nHorizon\nSingle-step Multi-step N/A (content\nonly)\nLearning\nMechanism\nRule-based or\nsupervised\nReinforcement/meta-\nlearning\nLarge-scale pre-\ntraining\nInteraction\nStyle\nReactive Proactive Creative\nEvaluation\nFocus\nAccuracy,\nlatency\nEngagement,\nadaptability\nCoherence, diver-\nsity\nsuit and flexible behavior in dynamic environments. Such\narchitectures fundamentally shift the focus of intelligence\nfrom single-model-based outputs to system-level behavior,\nwherein agents learn, adapt, and update decisions based on\nevolving task states. Thus, this comparative taxonomy not only\nhighlights increasing levels of operational independence but\nalso illustrates how Agentic AI introduces novel paradigms of\ncommunication, memory integration, and decentralized con-\ntrol, paving the way for the next generation of autonomous\nsystems with scalable, adaptive intelligence.\nA. Architectural Evolution: From AI Agents to Agentic AI\nSystems\nWhile both AI Agents and Agentic AI systems utilize\nmodular design principles, Agentic AI significantly extends\nthe foundational architecture to support more complex, dis-\ntributed, and adaptive behaviors. As illustrated in Figure 8, the\ntransition begins with core subsystems’ Perception, Reasoning,\nand Action, that define traditional AI Agents. Agentic AI\nenhances this foundation by integrating advanced components\nsuch as Specialized Agents, Advanced Reasoning & Plan-\nning, Persistent Memory, and Orchestration. The figure further\nemphasizes emergent capabilities including Multi-Agent Col-\nlaboration, System Coordination, Shared Context, and Task\nDecomposition, all encapsulated within a dotted boundary\nthat signifies the shift toward proactive, decentralized, and\ngoal-driven system architectures. As mentioned before, this\nprogression marks a fundamental inflection point in intelligent\nagent design. This section synthesizes findings from empirical\nframeworks such as LangChain [99], AutoGPT [100], and\nTaskMatrix [158], highlighting this progression in architectural\nsophistication.\n1) Core Architectural Components of AI Agents: Foun-\ndational AI Agents are typically composed of four primary\nsubsystems: perception, reasoning, action, and learning. These\nsubsystems form a closed-loop operational cycle, commonly\nreferred to as “Understand, Think, Act, Learn” from a user\ninterface perspective, or “Input, Processing, Action, Learning”\nin systems design literature [21], [159].\n• Perception Module: This subsystem intakes input signals\nfrom users (e.g., natural language prompts) or external\nsystems (e.g., APIs, file uploads, sensor streams), and per-\nforms data pre-processing to create datasets in formats in-\nterpretable by the agent’s reasoning module. For example,\nin LangChain-based agents [99], [160], the perception\n12\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nTABLE IV: Comparison of Generative AI, AI Agents, Agentic AI and Inferred Generative Agents Based on Core Function and\nPrimary Goal. This table highlights the foundational purpose and operational focus of each system type, distinguishing their\nroles in AI workflows. It contrasts their core functions such as content generation, task execution, or workflow orchestration and\nclarifies the primary goals each category is optimized to achieve, from generating media to autonomously managing complex\ntasks.\nFeature Generative AI AI Agent Generative Agent Agentic AI\nCore Function Content generation Task-specific execution us-\ning tools\nSimulated human-like be-\nhavior\nComplex workflow automa-\ntion\nMechanism Prompt → LLM → Output Prompt → Tool Call →\nLLM → Output\nPrompt → LLM + Mem-\nory/Planning → Output\nGoal → Agent Orchestra-\ntion → Output\nStructure Single model LLM + tool(s) LLM + memory + behavior\nmodel\nMulti-agent system\nExternal Data Access None (unless added) Via external APIs Typically closed-world\n(simulated inputs)\nCoordinated multi-agent ac-\ncess\nKey Trait Reactivity Tool-use Believability / Autonomy Collaboration\nTABLE V: Comparison Generative AI, AI Agents, Agentic AI, and Inferred Generative Agents Based on Core Function and\nPrimary Goal. This table highlights the foundational purpose and operational focus of each system type, distinguishing their\nroles in AI workflows. It contrasts their core functions such as content generation, task execution, or workflow orchestration and\nclarifies the primary goals each category is optimized to achieve, from generating media to autonomously managing complex\ntasks.\nFeature Generative AI AI Agent Agentic AI Generative Agent\n(Inferred)\nPrimary Goal Create novel content based\non prompt\nExecute a specific task us-\ning external tools\nAutomate complex work-\nflow or achieve high-level\ngoals\nPerform a specific genera-\ntive sub-task\nCore Function Content generation (text,\nimage, audio, etc.)\nTask execution with exter-\nnal interaction\nWorkflow orchestration and\ngoal achievement\nSub-task content generation\nwithin a workflow\nTABLE VI: Comparison of Architectural Components Across Generative AI, AI Agents, Agentic AI, and Generative Agents.\nThis table highlights key structural elements that define each AI paradigm, including core processing engines, prompt usage,\ntool and API integration, presence of multiple agents, and orchestration mechanisms. It illustrates the progression from single-\nmodel generative systems to complex multi-agent orchestration, while situating Generative Agents as modular units within\nbroader workflows.\nComponent Generative AI AI Agent Agentic AI Generative Agent\n(Inferred)\nCore Engine LLM / LIM LLM Multiple LLMs (potentially\ndiverse)\nLLM\nPrompts Yes (input trigger) Yes (task guidance) Yes (system goal and agent\ntasks)\nYes (sub-task guidance)\nTools/APIs No (inherently) Yes (essential) Yes (available to constituent\nagents)\nPotentially (if sub-task re-\nquires)\nMultiple Agents No No Yes (essential; collabora-\ntive)\nNo (is an individual agent)\nOrchestration No No Yes (implicit or explicit) No (is part of orchestration)\nlayer handles prompt templating, contextual wrapping,\nand retrieval augmentation via document chunking and\nembedding search.\n• Knowledge Representation and Reasoning (KRR)\nModule: At the core of the agent’s intelligence lies\nthe KRR module, which applies symbolic, statistical, or\nhybrid logic to input data. Techniques include rule-based\nlogic (e.g., if-then decision trees), deterministic workflow\nengines, or simple planning graphs. Reasoning in agents\nlike AutoGPT [37] is enhanced with function-calling\nand prompt chaining to simulate thought processes (e.g.,\n“step-by-step” prompts or intermediate tool invocations).\n• Action Selection and Execution Module: This module\ntranslates inferred knowledge and decisions into external\nactions using an action library. These actions may include\nsending messages, updating databases, querying APIs, or\nproducing structured outputs. Execution is often managed\nby middleware like LangChain’s “agent executor,” which\nlinks LLM outputs to tool calls and observes responses\nfor subsequent steps [99].\n13\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nTABLE VII: Comparison of Operational Mechanisms Among Generative AI, AI Agents, Agentic AI, and Generative Agents.\nThis table details the driving forces behind each system’s operation, their modes of interaction, approaches to workflow\nmanagement, and patterns of information flow. It highlights the transition from reactive, single-step generation in Generative\nAI to coordinated multi-agent workflows in Agentic AI, with Generative Agents functioning as modular contributors within\nbroader task sequences.\nMechanism Generative AI AI Agent Agentic AI Generative Agent\n(Inferred)\nPrimary Driver Reactivity to prompt Tool calling for task execu-\ntion\nInter-agent communication\nand collaboration\nReactivity to input or sub-\ntask prompt\nInteraction Mode User → LLM User → Agent → Tool User → System → Agents System/Agent → Agent →\nOutput\nWorkflow Handling Single generation step Single task execution Multi-step workflow coordi-\nnation\nSingle step within workflow\nInformation Flow Input → Output Input → Tool → Output Input → Agent1 → Agent2\n→ ... → Output\nInput (from system/agent)\n→ Output\nTABLE VIII: Comparison of Task Scope and Complexity Across Generative AI, AI Agents, Agentic AI, and Generative\nAgents. This table examines the breadth and difficulty of tasks each AI paradigm typically handles, illustrating the shift from\ngenerating single content pieces to managing complex, multi-agent workflows.\nAspect Generative AI AI Agent Agentic AI Generative Agent\n(Inferred)\nTask Scope Single piece of generated\ncontent\nSingle, specific, defined task Complex, multi-faceted\ngoal or workflow\nSpecific sub-task (often\ngenerative)\nComplexity Low (relative) Medium (integrates tools) High (multi-agent coordina-\ntion)\nLow to Medium (one task\ncomponent)\nExample (Video) Chatbot Tavily Search Agent YouTube-to-Blog\nConversion System\nTitle/Description/Conclusion\nGenerator\nTABLE IX: Comparison of Interaction and Autonomy Levels Among Generative AI, AI Agents, Agentic AI, and Generative\nAgents. This table analyzes varying degrees of autonomy, modes of external and internal interaction, and decision-making\nprocesses across AI paradigms. It highlights the progression from prompt-dependent Generative AI to highly autonomous,\nmulti-agent coordination in Agentic AI, with Generative Agents operating autonomously within limited sub-task scopes.\nFeature Generative AI AI Agent Agentic AI Generative Agent\n(Inferred)\nAutonomy Level Low (requires prompt) Medium (uses tools au-\ntonomously)\nHigh (manages entire pro-\ncess)\nLow to Medium (executes\nsub-task)\nExternal Interaction None (baseline) Via specific tools or APIs Through multiple\nagents/tools\nPossibly via tools (if\nneeded)\nInternal Interaction N/A N/A High (inter-agent) Receives input from system\nor agent\nDecision Making Pattern selection Tool usage decisions Goal decomposition and as-\nsignment\nBest sub-task generation\nstrategy\n• Basic Learning and Adaptation: Traditional AI Agents\nfeature limited learning mechanisms, such as heuristic\nparameter adjustment [161], [162] or history-informed\ncontext retention. For instance, agents may use simple\nmemory buffers to recall prior user inputs or apply\nscoring mechanisms to improve tool selection in future\niterations.\nCustomization of these agents typically involves domain-\nspecific prompt engineering, rule injection, or workflow tem-\nplates, distinguishing them from hard-coded automation scripts\nby their ability to make context-aware decisions. Systems like\nReAct [138] exemplify this architecture, combining reasoning\nand action in an iterative framework where agents simulate\ninternal dialogue before selecting external actions.\n2) Architectural Enhancements in Agentic AI: As discussed\nbefore, Agentic AI systems inherit the modularity of AI\nAgents but extend their architecture to support distributed in-\ntelligence, inter-agent communication, and iterative planning.\nThe literature documents a number of critical architectural en-\nhancements that differentiate Agentic AI from its predecessors\nand enable them to be highly versatile and adaptive [163],\n[164].\n• Ensemble of Specialized Agents: Rather than operating\nas a monolithic unit, Agentic AI systems consist of\nmultiple agents, each assigned a specialized function\nor task (e.g., a summarizer, a retriever, or a planner).\n14\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nMulti-Agent \nCollaboration Task-Decomposition\nShared Context\nSystem Coordination\nAI Agents\nAgentic AI\nFig. 8: Illustrating architectural evolution from traditional AI Agents to modern Agentic AI systems. It begins with core\nmodules Perception, Reasoning and Action, and expands into advanced components including Specialized Agents, Advanced\nReasoning & Planning, Persistent Memory, and Orchestration. The diagram further captures emergent properties such as Multi-\nAgent Collaboration, System Coordination, Shared Context, and Task Decomposition, all enclosed within a dotted boundary\nsignifying layered modularity and the transition to distributed, adaptive agentic AI intelligence.\nThese agents interact via communication channels (e.g.,\nmessage queues, blackboards, or shared memory). For\ninstance, MetaGPT [157] highlights this approach by\nmodeling agents after corporate departments (e.g., CEO,\nCTO, engineer), where roles are modular, reusable, and\nrole-bound.In this context, ”role-bound” means that each\nagent’s behavior and responsibilities are strictly defined\nby its assigned role, limiting its scope of action to that\nspecific functional domain.\n• Advanced Reasoning and Planning: Agentic systems\nembed iterative reasoning capabilities using frameworks\nsuch as ReAct [138], Chain-of-Thought (CoT) prompting\n[165], and Tree of Thoughts [166]. These mechanisms\nallow agents to break down a complex task into multiple\nreasoning stages, evaluate intermediate results, and re-\nplan actions dynamically. This enables the system to\nrespond adaptively to uncertainty or partial failure.\n• Persistent Memory Architectures: Unlike traditional\nagents, Agentic AI incorporates memory subsystems\nto preserve and persist knowledge across task cycles\nor agent sessions [167], [168]. Memory types include\nepisodic memory (task-specific history) [169], [170], se-\nmantic memory (long-term facts or structured data) [171],\n[172], and vector-based memory for RAG [173], [174].\nFor example, AutoGen [100] agents maintain scratchpads\nfor intermediate computations, enabling stepwise task\nprogression.\n• Orchestration Layers / Meta-Agents: A key innovation\nin Agentic AI is the introduction of orchestrators meta-\nagents that coordinate the lifecycle of subordinate agents,\nmanage dependencies, assign roles, and resolve conflicts.\nOrchestrators often include task managers, evaluators, or\nmoderators. In ChatDev [155], for example, a virtual\nCEO meta-agent distributes subtasks to departmental\nagents and integrates their outputs into a unified strategic\nresponse.\nThese enhancements collectively enable Agentic AI to sup-\nport scenarios that require sustained context, distributed labor,\nmulti-modal coordination, and strategic adaptation. Use cases\nrange from research assistants that retrieve, summarize, and\ndraft documents in tandem (e.g., AutoGen pipelines [100])\nto smart supply chain agents that monitor logistics, vendor\nperformance, and dynamic pricing models in parallel.\nThe shift from isolated perception–reasoning–action loops\nto collaborative and self-evaluative multi-agent workflows\nmarks a key turning point in the architectural design of\n15\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nintelligent systems, enabling agents not only to act but also to\nreflect, learn, and improve over time [175]. This progression\npositions Agentic AI as the next stage of AI infrastructure\ncapable not only of executing predefined workflows but also\nof constructing, revising, and managing complex objectives\nacross agents with minimal human supervision.\nIV. A PPLICATION OF AI AGENTS AND AGENTIC AI\nTo illustrate the real-world utility and operational diver-\ngence between AI Agents and Agentic AI systems, this study\nsynthesizes a range of applications drawn from recent litera-\nture, as visualized in Figure 9. We systematically categorize\nand analyze application domains across two parallel tracks:\nconventional AI Agent systems and their more advanced\nAgentic AI counterparts. For AI Agents, four primary use\ncases are reviewed: (1) Customer Support Automation and\nInternal Enterprise Search, where single-agent models handle\nstructured queries and response generation; (2) Email Filtering\nand Prioritization, where agents assist users in managing\nhigh-volume communication through classification heuristics;\n(3) Personalized Content Recommendation and Basic Data\nReporting, where user behavior is analyzed for automated\ninsights; and (4) Autonomous Scheduling Assistants, which\ninterpret calendars and book tasks with minimal user input.\nIn contrast, Agentic AI applications encompass broader and\nmore dynamic capabilities, reviewed and discussed in four\ncategories as well: (1) Multi-Agent Research Assistants that\nretrieve, synthesize, and draft scientific content collaboratively;\n(2) Intelligent Robotics Coordination, including drone and\nmulti-robot systems in fields like agriculture and logistics; (3)\nCollaborative Medical Decision Support, involving diagnostic,\ntreatment, and monitoring subsystems; and (4) Multi-Agent\nGame AI and Adaptive Workflow Automation, where decen-\ntralized agents interact strategically or handle complex task\npipelines.\n1) Application of AI Agents:\n1) Customer Support Automation and Internal Enter-\nprise Search: AI Agents are widely adopted in en-\nterprise environments for automating customer support\nand facilitating internal knowledge retrieval. In cus-\ntomer service, these agents leverage retrieval-augmented\nLLMs interfaced with APIs and organizational knowl-\nedge bases to answer user queries, triage tickets, and\nperform actions like order tracking or return initia-\ntion [54]. For internal enterprise search, agents built\non vector stores (e.g., Pinecone, Elasticsearch) retrieve\nsemantically relevant documents in response to natu-\nral language queries. Tools such as Salesforce Ein-\nstein https://www.salesforce.com/artificial-intelligence/,\nIntercom Fin https://www.intercom.com/fin, and Notion\nAI https://www.notion.com/product/ai demonstrate how\nstructured input processing and summarization capabil-\nities reduce workload and improve enterprise decision-\nmaking.\nA practical example (Figure 10a) of this dual func-\ntionality can be seen in a multi-national e-commerce\ncompany deploying an AI Agent-based customer support\nand internal search assistant. For customer support, the\nAI Agent integrates with the company’s Customer Re-\nlationship Management (CRM) system (e.g., Salesforce)\nand fulfillment APIs to resolve queries such as “Where\nis my order?” or “How can I return this item?”. Within\nmilliseconds, the agent retrieves contextual data from\nshipping databases and policy repositories, then gener-\nates a personalized response using retrieval-augmented\ngeneration. For internal enterprise search, employees use\nthe same system to query past meeting notes, sales\npresentations, or legal documents. When an HR manager\ntypes “summarize key benefits of policy changes from\nlast year,” the agent queries a Pinecone vector store\nembedded with enterprise documentation, ranks results\nby semantic similarity, and returns a concise summary\nalong with source links. These capabilities not only\nreduce ticket volume and support overhead but also min-\nimize time spent searching for institutional knowledge\n(like policies, procedures, or manuals). The result is a\nunified, responsive system that enhances both external\nservice delivery and internal operational efficiency using\nmodular AI Agent architectures.\n2) Email Filtering and Prioritization: As one of the\nimportant productivity tools, AI Agents automate email\ntriage through content classification and prioritization.\nIntegrated with systems like Microsoft Outlook and\nSuperhuman, these agents analyze metadata and mes-\nsage semantics to detect urgency, extract tasks, and rec-\nommend replies. They apply user-tuned filtering rules,\nbehavioral signals, and intent classification to reduce\ncognitive overload. Autonomous actions, such as auto-\ntagging or summarizing threads, enhance efficiency,\nwhile embedded feedback loops enable personalization\nthrough incremental learning [66].\nFigure10b illustrates a practical implementation of AI\nAgents in the domain of email filtering and prioriti-\nzation. In modern workplace environments, users are\ninundated with high volumes of email, leading to cog-\nnitive overload and missed critical communications. AI\nAgents embedded in platforms like Microsoft Outlook\nor Superhuman act as intelligent intermediaries that\nclassify, cluster, and triage incoming messages. These\nagents evaluate metadata (e.g., sender, subject line) and\nsemantic content to detect urgency, extract actionable\nitems, and suggest smart replies. As depicted, the AI\nagent autonomously categorizes emails into tags such\nas “Urgent,” “Follow-up,” and “Low Priority,” while\nalso offering context-aware summaries and reply drafts.\nThrough continual feedback loops and usage patterns,\nthe system adapts to user preferences, gradually refining\nclassification thresholds and improving prioritization ac-\ncuracy. This automation offloads decision fatigue, allow-\ning users to focus on high-value tasks, while maintain-\ning efficient communication management in fast-paced,\ninformation-dense environments.\n16\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nCustomer Support \nAutomation and \nInternal Enterprise \nSearch\nEmail Filtering and \nPrioritization\nPersonalized Content \nRecommendation, \nBasic Data Analysis \nand Reporting\nAutonomous \nScheduling \nAssistants \nMulti-Agent \nResearch Assistants\nIntelligent Robotics \nCoordination\nCollaborative \nMedical Decision \nSupport\nMulti-Agent Game \nAI & Adaptive \nWorkflow \nAutomation\nFig. 9: Categorized applications of AI Agents and Agentic AI across eight core functional domains.\n3) Personalized Content Recommendation and Basic\nData Reporting: AI Agents support adaptive personal-\nization by analyzing behavioral patterns for news, prod-\nuct, or media recommendations. Platforms like Amazon,\nYouTube, and Spotify deploy these agents to infer user\npreferences via collaborative filtering, intent detection,\nand content ranking. Simultaneously, AI Agents in an-\nalytics systems (e.g., Tableau Pulse, Power BI Copi-\nlot) enable natural-language data queries and automated\nreport generation by converting prompts to structured\ndatabase queries and visual summaries, democratizing\nbusiness intelligence access.\nA practical illustration (Figure 10c) of AI Agents in\npersonalized content recommendation and basic data\nreporting can be found in e-commerce and enterprise\nanalytics systems. Consider an AI agent deployed on a\nretail platform like Amazon: as users browse, click, and\npurchase items, the agent continuously monitors inter-\naction patterns such as dwell time, search queries, and\npurchase sequences. Using collaborative filtering and\ncontent-based ranking, the agent infers user intent and\ndynamically generates personalized product suggestions\nthat evolve over time. For example, after purchasing\ngardening tools, a user may be recommended compat-\nible soil sensors or relevant books. This level of per-\nsonalization enhances customer engagement, increases\nconversion rates, and supports long-term user retention.\nSimultaneously, within a corporate setting, an AI agent\nintegrated into Power BI Copilot allows non-technical\nstaff to request insights using natural language, for\ninstance, “Compare Q3 and Q4 sales in the Northeast.”\nThe agent translates the prompt into structured SQL\nqueries, extracts patterns from the database, and outputs\na concise visual summary or narrative report. This\napplication reduces dependency on data analysts and\nempowers broader business decision-making through\nintuitive, language-driven interfaces.\n4) Autonomous Scheduling Assistants: AI Agents in-\ntegrated with calendar systems autonomously manage\nmeeting coordination, rescheduling, and conflict reso-\nlution. Tools like x.ai and Reclaim AI interpret vague\nscheduling commands, access calendar APIs, and iden-\ntify optimal time slots based on learned user preferences.\nThey minimize human input while adapting to dynamic\navailability constraints. Their ability to interface with\nenterprise systems and respond to ambiguous instruc-\ntions highlights the modular autonomy of contemporary\nscheduling agents.\nA practical application of autonomous scheduling agents\ncan be seen in corporate settings as depicted in Fig-\nure 10d where employees manage multiple overlapping\nresponsibilities across global time zones. Consider an\n17\nAI Agents vs. Agentic AI by Sapkota et al. 2025\n(a)\n(b)\n(c)\n(d)\nFig. 10: Applications of AI Agents in enterprise settings: (a)\nCustomer support and internal enterprise search; (b) Email\nfiltering and prioritization; (c) Personalized content recom-\nmendation and basic data reporting; and (d) Autonomous\nscheduling assistants. Each example highlights modular AI\nAgent integration for automation, intent understanding, and\nadaptive reasoning across operational workflows and user-\nfacing systems.\nexecutive assistant AI agent integrated with Google\nCalendar and Slack that interprets a command like “Find\na 45-minute window for a follow-up with the product\nteam next week.” The agent parses the request, checks\navailability for all participants, accounts for time zone\ndifferences, and avoids meeting conflicts or working-\nhour violations. If it identifies a conflict with a pre-\nviously scheduled task, it may autonomously propose\nalternative windows and notify affected attendees via\nSlack integration. Additionally, the agent learns from\nhistorical user preferences such as avoiding early Fri-\nday meetings and refines its suggestions over time.\nTools like Reclaim AI and Clockwise further illustrate\nthis capability, offering calendar-aware automation that\nadapts to evolving workloads. Such assistants reduce\ncoordination overhead, increase scheduling efficiency,\nand enable smoother team workflows by proactively\nresolving ambiguity and optimizing calendar utilization.\n2) Appications of Agentic AI:\n1) Multi-Agent Research Assistants: Agentic AI systems\nare increasingly deployed in academic and industrial\nresearch pipelines to automate multi-stage knowledge\ncompilation. Platforms like AutoGen and CrewAI as-\nsign specialized roles to multiple agent retrievers, sum-\nmarizers, synthesizers, and citation formatters under a\ncentral orchestrator. The orchestrator distributes tasks,\nmanages role dependencies, and integrates outputs into\ncoherent drafts or review summaries. Persistent memory\nallows for cross-agent context sharing and refinement\nover time. These systems are being used for literature\nreviews, grant preparation, and patent search pipelines,\noutperforming single-agent systems such as ChatGPT by\nenabling concurrent sub-task execution and long-context\nmanagement [100].\nFor example, a real-world application of agentic AI as\ndepicted in Figure 11a is in the automated drafting of\ngrant proposals. Consider a university research group\npreparing a National Science Foundation (NSF) sub-\nmission. Using an AutoGen-based architecture, distinct\nagents are assigned: one retrieves prior funded proposals\nand extracts structural patterns; another scans recent\nliterature to summarize related work; a third agent aligns\nproposal objectives with NSF solicitation language; and\na formatting agent structures the document per com-\npliance guidelines. The orchestrator coordinates these\nagents, resolving dependencies (e.g., aligning methodol-\nogy with objectives) and ensuring stylistic consistency\nacross sections. Persistent memory modules store evolv-\ning drafts, feedback from collaborators, and funding\nagency templates, enabling iterative improvement over\nmultiple sessions. Compared to traditional manual pro-\ncesses, this multi-agent system significantly accelerates\ndrafting time, improves narrative cohesion, and ensures\nregulatory alignment offering a scalable, adaptive ap-\nproach to collaborative scientific writing in academia\n18\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nTABLE X: Representative AI Agents (2023–2025): Applica-\ntions and Operational Characteristics\nModel / Reference Application\nArea\nOperation as AI Agent\nChatGPT Deep Re-\nsearch Mode\nOpenAI (2025) Source\nLink\nResearch Analy-\nsis / Reporting\nSynthesizes hundreds of\nsources into reports; functions\nas a self-directed research\nanalyst.\nOperator\nOpenAI (2025) Source\nLink\nWeb Automation Navigates websites, fills forms,\nand completes online tasks au-\ntonomously.\nAgentspace: Deep Re-\nsearch Agent\nGoogle (2025) Source\nLink\nEnterprise\nReporting\nGenerates business\nintelligence reports using\nGemini models.\nNotebookLM Plus\nAgent\nGoogle (2025) Source\nLink\nKnowledge Man-\nagement\nSummarizes, organizes, and\nretrieves data across Google\nWorkspace apps.\nNova Act\nAmazon (2025)\nSource Link\nWorkflow\nAutomation\nAutomates browser-based\ntasks such as scheduling, HR\nrequests, and email.\nManus Agent\nMonica (2025) Source\nLinkhttps://manus.im/\nPersonal Task\nAutomation\nExecutes trip planning, site\nbuilding, and product compar-\nisons via browsing.\nHarvey\nHarvey AI (2025)\nSource Link\nLegal\nAutomation\nAutomates document drafting,\nlegal review, and predictive\ncase analysis.\nOtter Meeting Agent\nOtter.ai (2025) Source\nLink\nMeeting\nManagement\nTranscribes meetings and pro-\nvides highlights, summaries,\nand action items.\nOtter Sales Agent\nOtter.ai (2025) Source\nLink\nSales\nEnablement\nAnalyzes sales calls, extracts\ninsights, and suggests follow-\nups.\nClickUp Brain\nClickUp (2025)\nSource Link\nProject Manage-\nment\nAutomates task tracking, up-\ndates, and project workflows.\nAgentforce\nAgentforce (2025)\nSource Link\nCustomer\nSupport\nRoutes tickets and generates\ncontext-aware replies for sup-\nport teams.\nMicrosoft Copilot\nMicrosoft (2024)\nSource Link\nOffice Productiv-\nity\nAutomates writing, formula\ngeneration, and summarization\nin Microsoft 365.\nProject Astra\nGoogle DeepMind\n(2025) Source Link\nMultimodal As-\nsistance\nProcesses text, image, audio,\nand video for task support and\nrecommendations.\nClaude 3.5 Agent\nAnthropic (2025)\nSource Link\nEnterprise Assis-\ntance\nUses multimodal input for rea-\nsoning, personalization, and\nenterprise task completion.\nand R&D-intensive industries.\n2) Intelligent Robotics Coordination: In robotics and\nautomation, Agentic AI enable collaborative behavior in\nmulti-robot systems. Each robot operates as a task spe-\ncialized agent such as pickers, transporters, or mappers\nwhile an orchestrator supervises and adapts workflows.\nThese architectures rely on shared spatial memory, real-\ntime sensor fusion, and inter-agent synchronization for\ncoordinated physical actions. Use cases include ware-\nhouse automation, drone-based orchard inspection, and\nrobotic harvesting [157]. For instance, a swarm of agri-\ncultural drones may collectively map tree rows, identify\ndiseased fruits, and initiate mechanical interventions.\nThis dynamic allocation enables real-time reconfigura-\ntion and autonomy across agents facing uncertain or\nevolving environments.\nFor example, in commercial apple orchards (Figure 11b),\nAgentic AI enables a coordinated multi-robot system to\noptimize fruit harvesting. Here, task-specialized robots\nsuch as autonomous pickers, fruit classifiers, transport\nbots, and drone mappers operate as agentic units under\na central orchestrator. The mapping drones first survey\nthe orchard and use vision-language models (VLMs) to\ngenerate high-resolution yield maps and identify ripe\nfruit clusters. This spatial data is shared via a cen-\ntralized memory layer accessible by all agents. Picker\nrobots are then assigned to high-density zones, guided\nby path-planning agents that optimize routes around\nobstacles and labor zones. Simultaneously, transport\nagents dynamically haul fruit containers or bins be-\ntween pickers and storage, adjusting tasks in response\nto picker load levels and terrain changes. All agents\ncommunicate asynchronously through a shared protocol,\nand the coordinator continuously adjusts task priorities\nbased on weather forecasts or mechanical faults. If\none picker fails, nearby units autonomously reallocate\nworkload. This adaptive, memory-driven coordination\nexemplifies Agentic AI’s potential to reduce labor costs,\nincrease harvest efficiency, and respond to uncertainties\nin complex agricultural environments surpassing the\nrigid programming of conventional agricultural robots\n[100], [157].\n3) Collaborative Medical Decision Support: In high-\nstakes clinical environments, Agentic AI enables dis-\ntributed medical reasoning by assigning tasks such as\ndiagnostics, vital monitoring, and treatment planning\nto specialized agents. For example, one agent may\nretrieve patient history, another validates findings against\ndiagnostic guidelines, and a third proposes treatment\noptions (as seen in China’s world’s first Agentic AI hos-\npital [176]). These agents synchronize through shared\nmemory and reasoning chains, ensuring coherent and\nsafe recommendations. Applications include ICU man-\nagement, radiology triage, and pandemic response. Al-\nthough real-world implementations are still lacking due\nto the nascent nature of the field, studies support the\npotential of Agentic AI to revolutionize the healthcare\nsector [177].\nFor example, in a hospital ICU (Figure 11c), an agentic\nAI system supports clinicians in managing complex\npatient cases. A diagnostic agent continuously ana-\nlyzes vitals and lab data for early detection of sepsis\nrisk. Simultaneously, a history retrieval agent accesses\nelectronic health records (EHRs) to summarize comor-\nbidities and recent procedures. A treatment planning\nagent cross-references current symptoms with clinical\nguidelines (e.g., Surviving Sepsis Campaign), proposing\nantibiotic regimens or fluid protocols. The orchestra-\ntor integrates these insights, ensures consistency, and\n19\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nCentral Memory Layer\nRetrieve prior \nproposals Align with \nsolicitation\nStructure the \ndocument\nStore evolving \ndrafts\nGoal \nModule\nMemory\nStore\n(a) (b)\n(c) (d)\nUsing Agentic AI to \ncoordinate robotic harvest\nFig. 11: Illustrative Applications of Agentic AI Across Domains: Figure 11 presents four real-world applications of agentic AI\nsystems. (a) Automated grant writing using multi-agent orchestration for structured literature analysis, compliance alignment,\nand document formatting. (b) Coordinated multi-robot harvesting in apple orchards using shared spatial memory and task-\nspecific agents for mapping, picking, and transport. (c) Clinical decision support in hospital ICUs through synchronized agents\nfor diagnostics, treatment planning, and EHR analysis, enhancing safety and workflow efficiency. (d) Cybersecurity incident\nresponse in enterprise environments via agents handling threat classification, compliance analysis, and mitigation planning.\nIn all cases, central orchestrators manage inter-agent communication, shared memory enables context retention, and feedback\nmechanisms drive continual learning. These use cases highlight agentic AI’s capacity for scalable, autonomous task coordination\nin complex, dynamic environments across science, agriculture, healthcare, and IT security.\nsurfaces conflicts for human review. Feedback from\nphysicians is stored in a persistent memory module,\nallowing agents to refine their reasoning based on prior\ninterventions and outcomes. This coordinated system\nenhances clinical workflow by reducing cognitive load,\nshortening decision times, and minimizing oversight\nrisks. Early deployments in critical care and oncology\nunits have demonstrated increased diagnostic precision\nand better adherence to evidence-based protocols, offer-\ning a scalable solution for safer, real-time collaborative\nmedical support.\n4) Multi-Agent Game AI and Adaptive Workflow Au-\n20\nAI Agents vs. Agentic AI by Sapkota et al. 2025\ntomation: In simulation environments and enterprise\nsystems, Agentic AI systems facilitate decentralized\ntask execution and effective coordination. Game plat-\nforms like AI Dungeon deploy independent NPC agents\nwith goals, memory, and dynamic interactivity to create\nemergent narratives and social behavior. In enterprise\nworkflows, systems such as MultiOn and Cognosys use\nagents to manage processes like legal review or incident\nescalation, where each step is governed by a specialized\nmodule. These architectures exhibit resilience, exception\nhandling, and feedback-driven adaptability far beyond\nrule-based pipelines [178].\nFor example, in a modern enterprise IT environment\n(as depicted in Figure 11d), Agentic AI systems are\nincreasingly deployed to autonomously manage cyber-\nsecurity incident response workflows. When a potential\nthreat is detected such as abnormal access patterns or\nunauthorized data exfiltration, specialized agents are\nactivated in parallel. One agent performs real-time threat\nclassification using historical breach data and anomaly\ndetection models. A second agent queries relevant log\ndata from network nodes and correlates patterns across\nsystems. A third agent interprets compliance frameworks\n(e.g., GDPR or HIPAA) to assess the regulatory sever-\nity of the event. A fourth agent simulates mitigation\nstrategies and forecasts operational risks. These agents\ncoordinate under a central orchestrator that evaluates\ncollective outputs, integrates temporal reasoning, and\nissues recommended actions to human analysts. Through\nshared memory structures and iterative feedback, the\nsystem learns from prior incidents, enabling faster and\nmore accurate responses in future cases. Compared\nto traditional rule-based security systems, this agentic\nmodel enhances decision latency, reduces false positives,\nand supports proactive threat containment in large-scale\norganizational infrastructures [100].\nV. C HALLENGES AND LIMITATIONS IN AI AGENTS AND\nAGENTIC AI\nTo systematically understand the theoretical and opera-\ntional limitations of current intelligent systems, we present a\ncomparative visual synthesis in Figure 12, which categorizes\nchallenges and potential remedies across both AI Agents and\nAgentic AI paradigms. Figure 12a outlines the four most\npressing limitations specific to AI Agents namely, lack of\ncausal reasoning, inherited LLM constraints (e.g., hallucina-\ntions, shallow reasoning), incomplete agentic properties (e.g.,\nautonomy, proactivity), and failures in long-horizon planning\nand recovery. These challenges often arise due to their reliance\non stateless LLM prompts, limited memory, and heuristic\nreasoning loops.\nSimilarly, Figure 12b identifies eight critical bottlenecks\nunique to Agentic AI systems, such as inter-agent error\ncascades, coordination breakdowns, emergent instability, scal-\nability limits, and explainability issues. These challenges stem\nfrom the complexity of orchestrating multiple agents across\nTABLE XI: Representative Agentic AI Models (2023–2025):\nApplications and Operational Characteristics\nModel / Reference Application\nArea\nOperation as Agentic AI\nAuto-GPT\n[37]\nTask Automation Decomposes high-level\ngoals, executes subtasks\nvia tools/APIs, and\niteratively self-corrects.\nGPT Engineer\nOpen Source (2023)\nSource Link\nCode Generation Builds entire codebases:\nplans, writes, tests, and re-\nfines based on output.\nMetaGPT\n[157])\nSoftware Collab-\noration\nCoordinates specialized\nagents (e.g., coder, tester)\nfor modular multi-role\nproject development.\nBabyAGI\nNakajima (2024)\nSource Link\nProject Manage-\nment\nContinuously creates, pri-\noritizes, and executes sub-\ntasks to adaptively meet\nuser goals.\nV oyager\nWang et al. (2023)\n[179]\nGame\nExploration\nLearns in Minecraft, in-\nvents new skills, sets sub-\ngoals, and adapts strategy\nin real time.\nCAMEL\nLiu et al. (2023) [180]\nMulti-Agent\nSimulation\nSimulates agent societies\nwith communication, ne-\ngotiation, and emergent\ncollaborative behavior.\nEinstein Copilot\nSalesforce (2024)\nSource Link\nCustomer\nAutomation\nAutomates full support\nworkflows, escalates is-\nsues, and improves via\nfeedback loops.\nCopilot Studio\n(Agentic Mode)\nMicrosoft (2025)\nSource Link\nProductivity Au-\ntomation\nManages documents,\nmeetings, and projects\nacross Microsoft 365 with\nadaptive orchestration.\nAtera AI Copilot\nAtera (2025) Source\nLink\nIT Operations Diagnoses/resolves IT is-\nsues, automates ticketing,\nand learns from evolving\ninfrastructures.\nAES Safety Audit\nAgent\nAES (2025) Source\nLink\nIndustrial Safety Automates audits,\nassesses compliance,\nand evolves strategies to\nenhance safety outcomes.\nDeepMind Gato\n(Agentic Mode)\nReed et al. (2022)\n[181]\nGeneral Robotics Performs varied tasks\nacross modalities,\ndynamically learns,\nplans, and executes.\nGPT-4o + Plugins\nOpenAI (2024) Source\nLink\nEnterprise\nAutomation\nManages complex work-\nflows, integrates external\ntools, and executes adap-\ntive decisions.\ndistributed tasks without standardized architectures, robust\ncommunication protocols, or causal alignment frameworks.\nFigure 13 complements this diagnostic framework by syn-\nthesizing ten forward-looking design strategies aimed at mit-\nigating these limitations. These include RAG , tool-based\nreasoning [132], [133], [135], agentic feedback loops (ReAct\n[138]), role-based multi-agent orchestration, memory archi-\ntectures, causal modeling, and governance-aware design. To-\ngether, these mechanisms offer a consolidated roadmap for\naddressing current pitfalls and accelerating the development\nof safe, scalable, and context-aware autonomous systems.\n21\nAI Agents vs. Agentic AI by Sapkota et al. 2025\n(a) (b)\nFig. 12: Illustration of Challenges: (a) Key limitations of AI Agents including causality deficits and shallow reasoning. (b)\nAmplified coordination and stability challenges in Agentic AI systems.\n1) Challenges and Limitations of AI Agents: While AI\nAgents have garnered considerable attention for their ability to\nautomate structured tasks using LLMs and interfaces to spe-\ncific tools, the literature highlights significant theoretical and\npractical limitations that inhibit their reliability, generalization,\nand long-term autonomy [138], [164]. These challenges arise\nfrom both the architectural dependence on static, pretrained\nmodels and the difficulty of instilling agentic qualities such\nas causal reasoning, planning, and robust adaptation. These\nkey challenges and limitations (Figure 12a) of AI Agents are\nsummarized as follows:\n1) Lack of Causal Understanding: One of the most\nfoundational challenges lies in the agents’ inability to\nreason causally [182]–[184]. While LLMs, which form\nthe cognitive core of most AI Agents are highly ef-\nfective at detecting statistical correlations within train-\ning data, they do not truly understand cause-and-effect\nrelationships. As highlighted by recent research from\nDeepMind and conceptual analyses by TrueTheta [185]–\n[187], these models often fail to distinguish between\nmere association and actual causation. For example, an\nLLM-powered agent might observe that hospital visits\noften occur alongside illness, but it cannot determine\nwhether the illness caused the hospital visit or vice versa.\nMore critically, such agents cannot perform counterfac-\ntual reasoning imagining what would happen if a certain\nintervention or change were made. This lack of causal\nmodeling limits their ability to make informed decisions,\nevaluate the impact of hypothetical actions, or provide\nreliable recommendations in real-world scenarios where\nunderstanding “why” something happens is essential.\nAlthough reasoning-oriented LLMs have emerged such\nas DeepSeek R1 that follow a CoT approach to incre-\nmentally reason through problems, these models are not\nmathematically reliable reasoners (e.g., like an OWL\nreasoner). The chains of thought they produce are often\nlinguistically persuasive, but not necessarily logically\nvalid. In this sense, they do not replace formal reasoning\nsystems such as Pellet, Bayesian networks, or causal\ninference frameworks, which are designed to handle\nlogical consistency, ontological rigor, and probabilistic\ncausality with far greater reliability.\nThis limitation becomes particularly challenging under\ndistributional shifts, where real-world conditions differ\nfrom the training regime [188], [189]. Without such\ngrounding, agents remain brittle, failing in novel or\nhigh-stakes scenarios. For example, a navigation agent\nthat excels in urban driving may misbehave in snow or\nconstruction zones if it lacks an internal causal model\nof road traction or spatial occlusion.\n2) Inherited Limitations from LLMs: AI Agents, par-\nticularly those powered by LLMs, inherit a number of\nintrinsic limitations that impact their reliability, adapt-\nability, and overall trustworthiness in practical deploy-\nments [190]–[192]. One of the most critical issues is the\ntendency to produce hallucinations, which are plausible\nbut factually incorrect outputs. In high-stake domains\nsuch as legal consultation or scientific research, these\nhallucinations can lead to severe misjudgments and\nerode user trust [193], [194]. Compounding this is the\nwell-documented prompt sensitivity of LLMs, where\neven minor variations in phrasing can lead to divergent\nbehaviors. This brittleness hampers reproducibility, ne-\ncessitating meticulous manual prompt engineering and\noften requiring domain-specific tuning to maintain con-\nsistency across interactions [195].\nFurthermore, while recent agent frameworks adopt rea-\nsoning heuristics like Chain-of-Thought (CoT) [165],\n22\nAI Agents vs. Agentic AI by Sapkota et al. 2025\n[196] and ReAct [138] to simulate deliberative pro-\ncesses, these approaches remain shallow in semantic\ncomprehension. Agents may still fail at multi-step in-\nference, misalign task objectives, or make logically\ninconsistent conclusions despite the appearance of struc-\ntured reasoning [138]. Such shortcomings underscore\nthe absence of genuine understanding and generalizable\nplanning capabilities.\nAnother key limitation lies in computational cost and\nlatency. Each cycle of agentic decision-making partic-\nularly in planning or tool-calling may require several\nLLM invocations. These iterations not only increase\nrun-time latency but also scale resource consumption,\ncreating practical bottlenecks in real-world deployments\nand cloud-based inference systems. Furthermore, LLMs\nhave a static knowledge cutoff and cannot dynamically\nintegrate new information unless explicitly augmented\nvia retrieval or tool plugins. They also reproduce the\nbiases of their training datasets, which can manifest\nas culturally insensitive or skewed responses [197],\n[198]. Without rigorous auditing and mitigation strate-\ngies, these issues pose serious ethical and operational\nrisks, particularly when agents are deployed in sensitive\ncontexts or interact directly with end users.\n3) Incomplete Agentic Properties: A major limitation\nof current AI Agents is their inability to fully satisfy\nthe canonical agentic properties defined in foundational\nliterature, such as autonomy, proactivity, reactivity, and\nsocial ability [148], [192]. While many systems mar-\nketed as ”agents” leverage LLMs to perform useful\ntasks, they often fall short of these fundamental charac-\nteristics in practice. Autonomy, for instance, is typically\npartial at best. Although agents can execute tasks with\nminimal oversight once initialized, they remain heavily\nreliant on external scaffolding such as human-defined\nprompts, planning heuristics, or feedback loops to func-\ntion effectively [199]. Self-initiated task generation, self-\nmonitoring, or autonomous error correction are rare or\nabsent, limiting their capacity for true independence.\nProactivity is similarly underdeveloped. Most AI Agents\nrequire explicit user instruction to act and lack the capac-\nity to formulate or re-prioritize goals dynamically based\non changing context/environment or evolving objectives\n[200]. As a result, they behave reactively rather than\nstrategically, constrained by the static nature of their\ninitialization. Reactivity itself is constrained by archi-\ntectural bottlenecks. Agents do respond to environmental\nor user input, but response latency caused by repeated\nLLM inference calls [201], [202], coupled with narrow\ncontextual memory windows [167], [203], inhibits real-\ntime adaptability.\nIn addition to proactivity, social ability remains one of\nthe most underexplored capabilities of AI Agents. Real-\nworld AI agents and Agentic AI systems should be able\nto communicate and collaborate with humans or other\nagents over extended interactions, resolving ambiguity,\nnegotiating tasks, and adapting to social norms. How-\never, existing implementations exhibit brittle, template-\nbased dialogue that lacks long-term memory integration\nor nuanced conversational context. Agent-to-agent in-\nteraction is often hardcoded or limited to scripted ex-\nchanges, hindering collaborative execution and emergent\nbehavior [107], [204]. Collectively, these deficiencies\nreveal that while AI Agents demonstrate functional\nintelligence, they remain far from meeting the formal\nbenchmarks of intelligent [205], interactive, and adaptive\nagents. Bridging this gap is essential for advancing\ntoward more autonomous, socially capable AI systems.\n4) Limited Long-Horizon Planning and Recovery: A\npersistent limitation of current AI Agents lies in their\ninability to perform robust long-horizon planning, es-\npecially in complex, multi-stage tasks. This constraint\nstems from their foundational reliance on stateless\nprompt-response paradigms, where each decision is\nmade without an intrinsic memory of prior reasoning\nsteps unless externally managed. Although augmenta-\ntions such as the ReAct framework [138] or Tree-\nof-Thoughts [166] introduce pseudo-recursive reason-\ning, they remain fundamentally heuristic and lack true\ninternal models of time, causality, or state evolution.\nConsequently, agents often fail in tasks requiring ex-\ntended temporal consistency or contingency planning.\nFor example, in domains such as clinical triage or\nfinancial portfolio management, where decisions depend\non prior context and dynamically varying outcomes,\nagents may exhibit repetitive behaviors such as endlessly\nquerying tools or fail to adapt when sub-tasks fail or\nreturn ambiguous results. The absence of systematic\nrecovery mechanisms or error detection leads to brittle\nworkflows and error propagation. This shortfall severely\nlimits agent deployment in mission-critical environments\nwhere reliability, fault tolerance, and sequential coher-\nence are essential.\n5) Reliability and Safety Concerns: AI Agents are not yet\nsafe or verifiable enough for deployment in handling or\nmanaging critical infrastructure [206]. The absence of\ncausal reasoning leads to unpredictable behavior under\ndistributional shift [183], [207]. Furthermore, evaluating\nthe correctness of an agent’s plan especially when the\nagent fabricates intermediate steps or rationales remains\nan unsolved problem in interpretability [116], [208].\nSafety guarantees, such as formal verification, are not\nyet available for open-ended, LLM-powered agents.\nWhile AI Agents represent a major step beyond static\ngenerative models, their limitations in causal reason-\ning, adaptability, robustness, and planning restrict their\ndeployment in high-stakes or dynamic environments.\nMost current systems rely on heuristic wrappers and\nbrittle prompt engineering rather than grounded agentic\ncognition. Bridging this gap will require future systems\nto integrate causal models, dynamic memory, and verifi-\nable reasoning mechanisms. These limitations also set\n23\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nthe stage for the emergence of Agentic AI systems,\nwhich attempt to address these bottlenecks through\nmulti-agent collaboration, orchestration layers, and per-\nsistent system-level context. The persistent system-level\ncontext ensures that agents operate with a shared and\nevolving understanding of goals, environment, and prior\ndecisions, enabling coherent coordination and sustained\nautonomy across complex workflows [209], [210]. This\ncontinuity is critical for reducing redundant processing\nand enabling long-horizon reasoning.\n2) Challenges and Limitations of Agentic AI: Agentic AI\nsystems represent a paradigm shift from isolated AI Agents to\ncollaborative, multi-agent ecosystems capable of decomposing\nand executing complex goals [21]. These systems typically\nconsist of orchestrated or communicating agents that interact\nvia tools, APIs, and shared environments [25], [46]. While\nthis architectural evolution enables more ambitious automa-\ntion, it introduces a range of amplified and novel challenges\nthat compound existing limitations of individual LLM-based\nagents. The current challenges and limitations of Agentic AI\nsystems are as follows:\n1) Amplified Causality Challenges: One of the most\ncritical limitations in Agentic AI systems is the magni-\nfication of lack of causal reasoning already observed in\nsingle-agent architectures. Unlike traditional AI Agents\nthat operate in relatively isolated environments, Agentic\nAI systems involve complex inter-agent dynamics and\ncooperation, where each agent’s action can influence the\ndecision space of others. Without a robust capacity for\nmodeling cause-and-effect relationships, these systems\nstruggle to coordinate effectively and adapt to unfore-\nseen environmental shifts.\nA key issue cause by this limitation is inter-agent\ndistributional shift , where the behavior of one agent\nalters the operational context for others. In the absence\nof causal reasoning, agents are unable to anticipate the\ndownstream impact of their outputs, resulting in coor-\ndination breakdowns or redundant computations [211].\nFurthermore, these systems are particularly vulnerable to\nerror cascades: a faulty or hallucinated output from one\nagent can propagate through the system, compounding\ninaccuracies and corrupting subsequent decisions. For\nexample, if a verification agent erroneously validates\nfalse information, downstream agents such as summariz-\ners or decision-makers may unknowingly build upon that\nmisinformation, compromising the integrity of the entire\nsystem. This fragility underscores the urgent need for\nintegrating causal inference and intervention modeling\ninto the design of multi-agent workflows, especially\nin high-stake or dynamic environments where systemic\nrobustness is essential.\n2) Communication and Coordination Bottlenecks: A\nfundamental challenge in Agentic AI lies in achiev-\ning efficient communication and coordination across\nmultiple autonomous agents. Unlike single-agent sys-\ntems, Agentic AI systems involve distributed agents that\nmust collaboratively pursue a shared objective requring\nprecise alignment, synchronized execution, and robust\ncommunication protocols. However, current implemen-\ntations fall short in these aspects. One major issue is\ngoal alignment and shared context , where agents often\nlack a unified semantic understanding of overarching\nobjectives. This lack of shared semantic grounding\nhampers sub-task decomposition, dependency manage-\nment, and progress monitoring, especially in dynamic\nenvironments requiring causal awareness and temporal\ncoherence.\nIn addition, protocol limitations significantly hinder\ninter-agent communication. Most systems rely on nat-\nural language exchanges over loosely defined interfaces,\nwhich are prone to ambiguity, inconsistent formatting,\nand contextual drift. These communication gaps lead\nto fragmented strategies, delayed coordination, and de-\ngraded system performance. Furthermore, resource con-\ntention emerges as a systemic bottleneck when agents\nsimultaneously access shared computational, memory,\nor API resources. Without centralized orchestration or\nintelligent scheduling mechanisms, these conflicts can\nresult in race conditions, execution delays, or outright\nsystem failures. Collectively, these bottlenecks illustrate\nthe immaturity of current coordination frameworks in\nAgentic AI, and highlight the pressing need for stan-\ndardized communication protocols, semantic task plan-\nners, and global resource managers to ensure scalable,\ncoherent multi-agent collaboration.\n3) Emergent Behavior and Predictability: One of the\nmost critical limitations of Agentic AI lies in managing\nemergent behaviors of complex system-level phenomena\nthat arise from the interactions of autonomous agents.\nWhile such emergence can potentially yield adaptive and\ninnovative solutions, it also introduces significant unpre-\ndictability and safety risks [159], [212]. A key concern\nis the generation of unintended outcomes , where agent\ninteractions result in behaviors that were not explicitly\nprogrammed or foreseen by system designers. These\nbehaviors may diverge from task objectives, generate\nmisleading outputs, or even lead to harmful actions par-\nticularly in high-stake domains like healthcare, finance,\nor critical infrastructure.\nAs the number of agents and the complexity of their\ninteractions grow, so too does the likelihood of system\ninstability. This includes phenomena such as infinite\nplanning loops, action deadlocks, and contradictory\nbehaviors emerging from asynchronous or misaligned\nagent decisions. Without centralized arbitration mech-\nanisms, conflict resolution protocols, or fallback strate-\ngies, these instabilities compound over time, making the\nsystem fragile and unreliable. The stochasticity and lack\nof transparency in LLM-based agents further exacerbate\nthis issue, as their internal decision logic is not easily\ninterpretable or verifiable [213], [214]. Consequently,\n24\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nensuring the predictability and controllability of emer-\ngent behavior remains a central challenge in designing\nsafe and scalable Agentic AI systems.\n4) Scalability and Debugging Complexity: As Agentic\nAI systems scale in both the number of agents and the\ndiversity of specialized roles, maintaining system relia-\nbility and interpretability becomes increasingly complex\n[215], [216]. This limitation stems from the black-box\nchains of reasoning characteristic of LLM-based agents.\nEach agent may process inputs through opaque internal\nlogic, invoke external tools, and communicate with other\nagents all of which occur through multiple layers of\nprompt engineering, reasoning heuristics, and dynamic\ncontext handling. Tracing the root cause of a failure\nthus requires unwinding nested sequences of agent inter-\nactions, tool invocations, and memory updates, making\ndebugging non-trivial and time-consuming.\nAnother significant constraint is the system’s non-\ncompositionality. Unlike traditional modular systems,\nwhere adding components can enhance overall function-\nality, introducing additional agents in an Agentic AI\narchitecture often increases cognitive load, noise, and\ncoordination overhead. Poorly orchestrated agent net-\nworks where coordination, task delegation, and commu-\nnication protocols are inadequately designed can result\nin redundant computation, contradictory decisions, or\ndegraded task performance. Without robust frameworks\nfor agent role definition, communication standards, and\nhierarchical planning, the scaling Agentic AI does not\nnecessarily translate into greater intelligence or robust-\nness. These limitations highlight the need for systematic\narchitectural controls and traceability tools to support the\ndevelopment of reliable, large-scale agentic ecosystems.\n5) Trust, Explainability, and Verification: Agentic AI\nsystems pose huge challenges in explainability and ver-\nifiability due to their distributed, multi-agent architec-\nture. While interpreting the behavior of a single LLM-\npowered agent is already non-trivial, this complexity is\nmultiplied when multiple agents interact asynchronously\nthrough loosely defined communication protocols. Each\nagent may possess its own memory, task objective, and\nreasoning path, resulting in compounded opacity where\ntracing the causal chain of a final decision or failure\nbecomes exceedingly difficult. The lack of shared, trans-\nparent logs or interpretable reasoning paths across agents\nmakes it highly difficult, if not impossible, to determine\nwhy a particular sequence of actions occurred or which\nagent initiated a misstep.\nCompounding this opacity is the absence of formal ver-\nification tools tailored for Agentic AI. Unlike traditional\nsoftware systems, where model checking and formal\nproofs offer bounded guarantees, there exists no widely\nadopted methodology to verify that a multi-agent LLM\nsystem comprising multiple large language model agents\ncollaborating on tasks will perform reliably across all\ninput distributions or operational contexts. This lack of\nverifiability presents a significant barrier to adoption\nin safety-critical domains such as autonomous vehicles,\nfinance, and healthcare, where explainability and assur-\nance are crucial. To advance Agentic AI safely, future\nresearch must address the fundamental gaps in causal\ntraceability, agent accountability, and formal safety guar-\nantees.\n6) Security and Adversarial Risks: Agentic AI architec-\ntures introduce a significantly expanded attack surface\ncompared to single-agent systems, exposing them to\ncomplex adversarial threats. One of the most critical\nvulnerabilities lies in the presence of a single point of\ncompromise. Since Agentic AI systems are composed of\ninterdependent agents communicating over shared mem-\nory or messaging protocols, the compromise of even\none agent through prompt injection, model poisoning,\nor adversarial tool manipulation can propagate malicious\noutputs or corrupted state across the entire system. For\nexample, a fact-checking agent fed with tampered data\ncould unintentionally legitimize false claims, which are\nthen integrated into downstream reasoning by summa-\nrization or decision-making agents.\nMoreover, inter-agent dynamics themselves are suscepti-\nble to exploitation. Attackers can induce race conditions,\ndeadlocks, or resource exhaustion by manipulating the\ncoordination logic between agents. Without rigorous\nauthentication, access control, and sandboxing mech-\nanisms, malicious agents or corrupted tool responses\ncan derail multi-agent workflows or cause erroneous\nescalation in task pipelines. These risks are amplified\nby the absence of standardized security frameworks for\nLLM-based multi-agent systems, leaving most current\nimplementations defenseless against sophisticated multi-\nstage attacks. As Agentic AI moves toward broader\nadoption, especially in high-stakes environments, em-\nbedding secure-by-design principles and adversarial ro-\nbustness becomes an urgent research priority.\n7) Ethical and Governance Challenges: The distributed\nand autonomous nature of Agentic AI systems intro-\nduces huge ethical and governance concerns, partic-\nularly in terms of accountability, fairness, and value\nalignment. In multi-agent settings, accountability gaps\nemerge when multiple agents interact to produce an\noutcome, making it difficult to assign responsibility\nfor errors or unintended consequences. This ambiguity\ncomplicates legal liability, regulatory compliance, and\nuser trust, particularly in high-stakes domains such as\nautonomous vehicles, scientific research, or critical in-\nfrastructure management. Furthermore, bias propagation\nand amplification present a unique challenge: agents\nindividually trained on biased data may reinforce each\nother’s skewed decisions through interaction, leading to\nsystemic inequities that are more pronounced than in\nisolated models. These emergent biases can be subtle\nand difficult to detect without ongoing monitoring over\ntime or robust auditing mechanisms.\n25\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nAdditionally, misalignment and value drift pose serious\nrisks in long-horizon or dynamic environments. With-\nout a unified framework for shared value encoding,\nindividual agents may interpret overarching objectives\ndifferently or optimize for local goals that diverge from\nhuman intent. Over time, this misalignment can lead\nto behavior that is inconsistent with ethical norms or\nuser expectations. Current alignment methods, which are\nmostly designed for single-agent systems, are inadequate\nfor managing value synchronization across heteroge-\nneous agent collectives. These challenges highlight the\nurgent need for governance-aware agent architectures,\nincorporating principles such as role-based isolation,\ntraceable decision logging, and participatory oversight\nmechanisms to ensure ethical integrity in autonomous\nmulti-agent systems.\n8) Immature Foundations and Research Gaps: Despite\nrapid progress and high-profile demonstrations, research\nand development in Agentic AI remains is still in early\nstage with unresolved issues that limit its scalability,\nreliability, and theoretical foundation. One of the central\nconcerns is the lack of standard architectures . There\nis currently no widely accepted blueprint for how to\ndesign, monitor, or evaluate multi-agent systems built on\nLLMs . This architectural fragmentation makes it diffi-\ncult to compare implementations, replicate experiments,\nor generalize findings across domains. Key aspects such\nas agent orchestration the structured coordination and\nrole-based task allocation among agents along with\nmemory structures and communication protocols, are\noften implemented in an ad hoc manner, leading to\nfragile systems that lack interoperability, consistency,\nand formal reliability guarantees.\nEqually critical is the absence of causal foundations,\nas scalable causal discovery and reasoning remain un-\nsolved challenges in current AI systems [217]. Causal\ndiscovery refers to the process of identifying underlying\ncause-and-effect relationships from data essential for\nunderstanding how different variables influence one an-\nother. Without the ability to represent and reason about\nthese causal links, Agentic AI systems are inherently\nconstrained in their ability to safely generalize beyond\nnarrow, predefined training scenarios [189], [218]. This\nlimitation weakens their robustness when faced with\ndistributional shifts, reduces their effectiveness in taking\nproactive actions, and impairs their ability to simulate\nalternative outcomes or hypothetical plans capabilities\nthat are essential for intelligent coordination, adaptive\nplanning, and high-stakes decision-making.\nThe gap between functional demos and principled de-\nsign thus emphasizes an urgent need for foundational\nresearch in multi-agent system theory, causal infer-\nence integration, and benchmark development. Only\nby addressing these deficiencies can the field progress\nfrom prototype pipelines to trustworthy, general-purpose\nagentic frameworks suitable for deployment in high-\nstake environments.\nVI. P OTENTIAL SOLUTIONS AND FUTURE ROADMAP\nA. Potential Solutions\nTo address the challenges and limitations of AI Agents\nand Agentic AI systems discussed in the previous section,\nwe identify a set of promising solution pathways (as il-\nlustrated in Figure 13) including RAG , tool-augmented\nreasoning, memory architectures, causal modeling, reflexive\nmechanisms, orchestration frameworks, and governance-aware\ndesigns. These techniques collectively represent the frontier\nof efforts to overcome the brittleness, scalability bottlenecks,\nand coordination challenges that currently constrain both AI\nAgents and Agentic AI. At present, most deployed systems\nrely heavily on heuristic wrappers, manual prompt engineer-\ning, and shallow coordination logic, falling short of robust\nautonomy and reliability. In the following several paragraphs,\nwe examine how each solution targets specific technical or sys-\ntemic limitations, highlight gaps in current implementations,\nand propose future research directions to evolve these solutions\ninto mature, interoperable components of next-generation in-\ntelligent systems. This roadmap is essential for transitioning\nfrom ad hoc agent deployments to principled, generalizable\nframeworks capable of powering scalable, safe, and context-\naware agentic ecosystems.\n1) RAG : For AI Agents, RAG has the potential to mitigate\nhallucinations and can expand static LLM knowledge by\ngrounding outputs in real-time data [219]. By embedding\nuser queries and retrieving semantically relevant docu-\nments from vector databases like FAISS Source Link\nor Pinecone Pinecone, agents can generate contextually\nvalid responses based on external facts. This retrieval-\nbased grounding mechanism is particularly effective in\ndomains such as enterprise search and customer support,\nwhere accuracy and access to up-to-date knowledge are\nessential for reliable task execution and user trust.\nIn Agentic AI systems, RAG serves as a shared ground-\ning mechanism across agents. For example, a summa-\nrizer agent may rely on the retriever agent to access\nthe latest scientific papers before generating a synthesis.\nPersistent, queryable memory allows distributed agents\nto operate on a unified semantic layer, avoiding or\nminimizing inconsistencies due to divergent contextual\nviews. When implemented across a multi-agent system,\nRAG helps maintain shared accuracy, enhances goal\nalignment, and reduces inter-agent misinformation prop-\nagation.\n2) Tool-Augmented Reasoning (Function Calling): AI\nAgents benefit significantly from function calling, which\nextends their ability to interact with real-world systems\n[173], [220]. Agents can query APIs, run local scripts,\nor access structured databases, thus transforming LLMs\nfrom static predictors into interactive problem-solvers\n[137], [168]. This tool-augmented reasoning capability\nallows agents to dynamically access and process real-\ntime, evolving information such as weather forecasts,\n26\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nRetrieval-Augmented \nGeneration (RAG)\nTool-Augmented \nReasoning (Function \nCalling)\nAgentic Loop: \nReasoning, Action, \nObservation\nReflexive and Self-\nCritique Mechanisms\nProgrammatic Prompt \nEngineering Pipelines\nCausal Modeling \nand Simulation-\nBased Planning\nGovernance-Aware \nArchitectures \n(Accountability + \nRole Isolation)\nMonitoring, \nAuditing, and \nExplainability \nPipelines\nMemory Architectures \n(Episodic, Semantic, \nVector)\nMulti-Agent \nOrchestration with \nRole Specialization\nFig. 13: Ten evolving architectural and algorithmic mechanisms such as RAG, tool augmentation, dynamic memory, causal\nmodeling, orchestration, and reflexive self-evaluation are shown as key enablers to advance beyond prior usage toward addressing\ncurrent limitations in reliability, scalability, and explainability. These techniques, while previously applied in isolated agent\nsystems, are here recontextualized to support the demands of modern AI Agents and Agentic AI, enabling coordinated, adaptive,\nand verifiable behavior in increasingly complex and dynamic environments.\nstock prices, or user calendar updates and to perform ex-\necutable actions like scheduling appointments, sending\nemails, or executing complex computations in Python.\nBy bridging natural language reasoning with external\ntool interaction, this functionality empowers agents to\ngo beyond static language generation and operate as\nautonomous, task-oriented decision-makers in real-world\nenvironments.\nFor Agentic AI systems, function calling is instrumental\nin enhancing both autonomy and structured coordination\namong multiple agents. Each agent, assigned a spe-\ncialized role within the system such as data retriever,\nvisualizer, or decision-maker can independently invoke\ndomain-specific APIs to perform targeted tasks, such\nas accessing clinical records or generating analytical\ndashboards. These function calls are not isolated; rather,\nthey are embedded within an orchestrated pipeline a\nwell-defined, multi-step workflow in which outputs from\none agent seamlessly serve as inputs for the next [221].\nThis orchestration facilitates dynamic delegation, where\nagents can hand off subtasks based on predefined roles\nand capabilities without ambiguity or redundancy [21],\n[25].\nMoreover, integrating function calling within such\norchestrated pipelines establishes clearer behavioral\nboundaries between agents. Each agent operates within\nits defined scope of responsibility, reducing the like-\nlihood of overlapping actions or conflicting decisions.\nWhen coupled with validation protocols (e.g., response\nverification or schema checks in Waitgpt [222]) and ob-\nservation mechanisms (e.g., feedback loops or audit logs\n[223], [224]), these boundaries are reinforced, ensuring\nthat each agent not only performs its assigned task but\ndoes so transparently and accountably. This structured\ninteraction model enhances system robustness, trace-\nability, and ultimately the reliability of Agentic AI in\ncomplex, high-stakes domains.\n3) Agentic Loop: Reasoning, Action, Observation: AI\nAgents often suffer from single-pass inference limita-\ntions [225]. The ReAct pattern introduces an iterative\nloop where agents reason about tasks, act by calling\ntools or APIs, and then observe results before continuing\n[138]. This feedback loop allows for more deliberate,\ncontext-sensitive behaviors. For example, an agent may\nverify retrieved data before drafting a summary, thereby\nreducing hallucination and logical errors. In Agentic AI,\nthis pattern is critical for collaborative coherence. ReAct\nenables agents to evaluate dependencies dynamically\nreasoning over intermediate states, re-invoking tools\nif needed, and adjusting decisions as the environment\nevolves [138]. This loop becomes more complex in\nmulti-agent settings where each agent’s observation must\nbe reconciled against others’ outputs. Shared memory\nand consistent logging are essential here, ensuring that\nthe reflective capacity of the system is not fragmented\nacross agents.\n4) Memory Architectures (Episodic, Semantic, Vector):\nAs discussed before, AI Agents face limitations in long-\nhorizon planning and session continuity. Memory archi-\ntectures address this by persisting information across\n27\nAI Agents vs. Agentic AI by Sapkota et al. 2025\ntasks [226]. Episodic memory allows agents to recall\nprior actions and feedback, semantic memory encodes\nstructured domain knowledge, and vector memory en-\nables similarity-based retrieval [227]. These elements are\nkey for personalization and adaptive decision-making\nin repeated interactions. Agentic AI systems require\neven more sophisticated memory models due to dis-\ntributed state management. Each agent may maintain\nlocal memory while accessing shared global memory\nto facilitate coordination. For example, a planner agent\nmight use vector-based memory to recall prior work-\nflows, while a QA agent references semantic mem-\nory for fact verification. Synchronizing memory access\nand updates across agents enhances consistency, en-\nables context-aware communication, and supports long-\nhorizon system-level planning.\n5) Multi-Agent Orchestration with Role Specialization:\nIn conventional AI Agent systems, increasing task com-\nplexity is often addressed through modular prompt engi-\nneering or conditional branching logic. However, as the\nrange and intricacy of tasks grow, a single agent may\nbecome overburdened, leading to performance degrada-\ntion or failure to generalize effectively [228], [229]. To\nmitigate this, role specialization dividing the overall task\ninto discrete functional units such as planning, summa-\nrization, or verification enables a form of compartmen-\ntalized reasoning even within a single-agent framework.\nIn this context, compartmentalized reasoning refers to\nthe simulation of distinct cognitive functions within one\nagent by prompting it to reason through subtasks in\nsequence, often mimicking multiple expert roles.\nIn contrast, Agentic AI systems institutionalize orches-\ntration as a core architectural feature. Here, orchestration\nrefers to the dynamic coordination and task delegation\nacross a team of specialized agents, each designed to\nhandle a specific sub-function in the overall workflow.\nThis is typically governed by a meta-agent or orches-\ntrator, a supervisory agent responsible for allocating\ntasks, managing dependencies, and maintaining global\ncontext across all agents. Systems like MetaGPT and\nChatDev exemplify this paradigm: agents adopt prede-\nfined professional roles such as CEO, software engi-\nneer, or reviewer and communicate through structured\nmessaging protocols to collaboratively complete com-\nplex projects. This orchestrated, role-specialized design\nenhances system interpretability by isolating reasoning\ntraces within clearly defined agent roles. It also improves\nscalability, as tasks can be parallelized across agents,\nand contributes to fault tolerance, as errors from one\nagent are contained and monitored by the orchestrator,\npreventing systemic failure. Such modular, coordinated\narchitectures are foundational to building robust and\ntransparent Agentic AI systems.\n6) Reflexive and Self-Critique Mechanisms: AI Agents\noften fail silently or propagate errors. Reflexive mech-\nanisms introduce the capacity for self-evaluation [230],\n[231]. After completing a task, agents can critique their\nown outputs using a secondary reasoning pass, increas-\ning robustness and reducing error rates. For example,\na legal assistant agent might verify that its drafted\nclause matches prior case laws before submission. For\nAgentic AI, reflexivity extends beyond self-critique to\ninter-agent evaluation. Agents can review each other’s\noutputs e.g., a verifier agent auditing a summarizer’s\nwork. Reflexion-like mechanisms ensure collaborative\nquality control and enhance trustworthiness [232]. Such\npatterns also support iterative improvement and adaptive\nreplanning, particularly when integrated with memory\nlogs or feedback queues [233], [234].\n7) Programmatic Prompt Engineering Pipelines: Man-\nual prompt tuning introduces brittleness and reduces\nreproducibility in AI Agents. Programmatic pipelines\nautomate this process using task templates, context\nfillers, and retrieval-augmented variables [235], [236].\nThese dynamic prompts are structured based on task\ntype, agent role, or user query, improving generalization\nand reducing failure modes associated with prompt\nvariability. In Agentic AI, prompt pipelines enable scal-\nable, role-consistent communication. Each agent type\n(e.g., planner, retriever, summarizer) can generate or\nconsume structured prompts tailored to its function. By\nautomating message formatting, dependency tracking,\nand semantic alignment, programmatic prompting pre-\nvents coordination drift and ensures consistent reasoning\nacross diverse agents in real time [21], [173].\n8) Causal Modeling and Simulation-Based Planning: AI\nAgents often operate on statistical correlations rather\nthan causal models, leading to poor generalization under\ndistribution shifts. Embedding causal inference allows\nagents to distinguish between correlation and causa-\ntion, simulate interventions, and plan counterfactually-\ninformed, goal-directed actions that anticipate long-\nterm effects and adapt to changing environments. For\ninstance, in supply chain scenarios, a causally-aware\nagent can simulate the downstream impact of shipment\ndelays. In Agentic AI, causal reasoning is vital for safe\ncoordination and error recovery. Agents must anticipate\nhow their actions impact others requiring causal graphs,\nsimulation environments, or Bayesian inference layers.\nFor example, a planning agent may simulate different\nstrategies and communicate likely outcomes to others,\nfostering strategic alignment and avoiding unintended\nemergent behaviors. To enforce cooperative behavior,\nagents can be governed by a structured planning ap-\nproach such as STRIPS or PDDL (Planning Domain\nDefinition Language), where the environment is modeled\nwith defined actions, preconditions, and effects. Inter-\nagent dependencies are encoded such that one agent’s\naction enables another’s, and a centralized or distributed\nplanner ensures that all agents contribute to a shared\ngoal. This unified framework supports strategic align-\nment, anticipatory planning, and minimizes unintended\n28\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nemergent behaviors in multi-agent systems.\n9) Monitoring, Auditing, and Explainability Pipelines:\nAI Agents lack transparency, complicating debugging\nand trust. Logging systems that record prompts, tool\ncalls, memory updates, and outputs enable post-hoc\nanalysis and performance tuning. These records help\ndevelopers trace faults, refine behavior, and ensure\ncompliance with usage guidelines especially critical in\nenterprise or legal domains. Logging and explainability\nare even more critical for Agentic AI systems. With\nmultiple agents interacting asynchronously, audit trails\nare essential for identifying which agent caused an error\nand under what conditions. Explainability pipelines that\nintegrate across agents (e.g., timeline visualizations or\ndialogue replays) are key to ensuring safety, especially\nin regulatory or multi-stakeholder environments.\n10) Governance-Aware Architectures (Accountability\nand Role Isolation): AI Agents currently lack built-\nin safeguards for ethical compliance or error attribution.\nGovernance-aware designs introduce role-based access\ncontrol, sandboxing, and identity resolution to ensure\nagents act within scope and their decisions can be\naudited or revoked. These structures reduce risks in\nsensitive applications such as healthcare or finance.\nIn Agentic AI, governance must scale across roles,\nagents, and workflows. Role isolation prevents rogue\nagents from exceeding authority, while accountability\nmechanisms assign responsibility for decisions and trace\ncausality across agents. Compliance protocols, ethical\nalignment checks, and agent authentication ensure safety\nin collaborative settings paving the way for trustworthy\nAI ecosystems.\nB. Future Roadmap\nAI Agents are projected to evolve significantly through\nenhanced modular intelligence focused on five key domains\nas depicted in Figure 14, which include proactive reasoning,\ntool integration, causal inference, continual learning, and trust-\ncentric operations. The first transformative milestone involves\ntransitioning from reactive to Proactive Intelligence , where\nagents initiate tasks based on learned patterns, contextual\ncues, or latent goals rather than awaiting explicit prompts.\nThis advancement depends heavily on robust Tool Integration,\nenabling agents to dynamically interact with external systems,\nsuch as databases, APIs, or simulation environments, to fulfill\ncomplex user tasks. Equally critical is the development of\nCausal Reasoning , which will allow agents to move beyond\nstatistical correlation, supporting inference of cause-and-effect\nrelationships essential for tasks involving diagnosis, planning,\nor prediction. To maintain relevance over time, agents must\nadopt frameworks for Continuous Learning , incorporating\nfeedback loops and episodic memory to adapt their behav-\nior across sessions and environments. Lastly, to build user\nconfidence, agents must prioritize Trust & Safety mechanisms\nthrough verifiable output logging, bias detection, and ethical\nguardrails especially as their autonomy increases. Together,\nthese pathways will redefine AI Agents from static tools\ninto adaptive cognitive systems capable of autonomous yet\ncontrollable operation in dynamic digital environments.\nAgentic AI, as a natural extension of foundational AI agent\nframeworks, emphasizes collaborative intelligence through\nmulti-agent coordination, contextual persistence, and domain-\nspecific orchestration. Future systems (Figure 14, right side)\nare expected to exhibit Multi-Agent Scaling , enabling spe-\ncialized agents to operate concurrently under distributed con-\ntrol to tackle complex, high-dimensional problems mirroring\ncollaborative workflows typical of human teams. This scal-\ning necessitates a layer of Unified Orchestration , wherein\norchestrators specialized meta-agents assume responsibility for\ndynamically assigning roles, managing inter-agent communi-\ncation, sequencing task dependencies, and resolving potential\nconflicts. Orchestration, in this context, refers to the intelligent\ncoordination and regulation of interactions among multiple\nautonomous agents to ensure coherent and efficient collective\nbehavior. Sustained performance over time will depend on\nrobust Persistent Memory architectures that allow agents to\nstore and retrieve semantic, episodic, and shared task-relevant\nknowledge, supporting continuity in longitudinal operations\nand enabling agents to maintain awareness of evolving goals\nand environmental states. Simulation Planning will become\na core capability, empowering agent collectives to model\nhypothetical decision trajectories, forecast consequences, and\noptimize courses of action through internal trial-and-error\nmechanisms thus reducing real-world risk and increasing adap-\ntive robustness.\nMoreover, establishing Ethical Governance frameworks will\nbe crucial to ensure that agent collectives operate within\naligned moral and legal boundaries. These frameworks will\ndefine accountability structures, verification mechanisms, and\nsafety constraints, especially in high-stakes domains involving\nautonomous decisions. Finally, the emergence of Domain-\nSpecific Systems tailored for sectors such as law, medicine,\nlogistics, and climate science will allow Agentic AI to lever-\nage contextual specialization and outperform general-purpose\nagents through fine-tuned workflows and expert reasoning\ncapabilities.\nA transformative direction for future AI systems is intro-\nduced by the Absolute Zero: Reinforced Self-play Reason-\ning with Zero Data (AZR) framework, which reimagines the\nlearning paradigm for AI Agents and Agentic AI systems by\nremoving dependency on external datasets [237]. Traditionally,\nboth AI Agents and Agentic AI architectures have relied on\nhuman-annotated data, static knowledge corpora, or preconfig-\nured environmental factors that constrain scalability and adapt-\nability in open-world contexts. AZR addresses this limitation\nby enabling agents to autonomously generate, validate, and\nsolve their own tasks, using verifiable feedback mechanisms\n(e.g., code execution) to ground learning. This self-evolving\nmechanism opens the door to truly autonomous reasoning\nagents capable of self-directed learning and adaptation in\ndynamic, data-scarce environments.\nIn the context of Agentic AI where multiple specialized\n29\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nAI Agents\nProactive\nIntelligence\nTool\nIntegration\nCausal\nReasoning\nContinuous\nLearning\nTrust &\nSafety\nAgentic AI\nMulti-Agent\nScaling\nUnified Or-\nchestration\nPersistent\nMemory\nSimulation\nPlanning\nEthical\nGovernance\nDomain-\nSpecific\nSystems\nFig. 14: Mindmap visualization of the future roadmap for AI Agents (left) and Agentic AI (right).\nagents collaborate within orchestrated workflows, meaning\nstructured, coordinated processes managed by a central con-\ntroller or meta-agent AZR lays the groundwork for agents to\nnot only specialize in distinct roles but also co-evolve through\nself-improving interactions and shared learning objectives\n[237]. For instance, scientific research pipelines could consist\nof agents that propose hypotheses, run simulations, validate\nfindings, and revise strategies entirely through self-play and\nverifiable reasoning, without continuous human oversight. By\nintegrating the AZR paradigm, such systems can maintain\npersistent growth, knowledge refinement, and task flexibility\nover time. Ultimately, AZR highlights a future in which AI\nagents transition from static, pretrained tools to intelligent,\nself-evolving and -improving ecosystems positioning both AI\nAgents and Agentic AI at the forefront of next-generation\nartificial intelligence.\nVII. C ONCLUSION\nIn this study, we presented a comprehensive literature-\nbased evaluation of the evolving landscape of AI Agents\nand Agentic AI systems, offering a structured taxonomy\nthat highlights foundational concepts, architectural evolution,\napplication domains, and key limitations and potential so-\nlutions. Beginning with a foundational understanding, we\ncharacterized AI Agents as modular, task-specific entities with\nconstrained autonomy and reactivity within the tasks specified\nfor them. Their operational scope is enabled by the integration\nof LLMs and LIMs, which serve as core reasoning modules\nfor perception, language understanding, and decision-making.\nWe identified generative AI as a functional precursor to AI\nAgents, emphasizing its limitations in autonomy and goal\npersistence, and examined how LLMs drive the progression\nfrom passive generation to interactive task completion through\ntool augmentation.\nThis study then explored the conceptual emergence of\nAgentic AI systems as a transformative evolution from isolated\nagents or entities to orchestrated, multi-agent ecosystems\nthat is, coordinated frameworks in which multiple special-\nized agents interact through structured role assignment, task\ndelegation, and centralized or distributed control enabled by\ncollaborative learning and collective decision making. We\nanalyzed key differentiators such as distributed cognition,\npersistent memory, and coordinated planning that distinguish\nAgentic AI from conventional single-agent models. This ana-\nlytical comparison was followed by a detailed breakdown of\narchitectural evolution, highlighting the transition from mono-\nlithic, rule-based frameworks to modular, role-specialized net-\nworks. These networks are facilitated by orchestration layers,\nwhich serve as coordination mechanisms either centralized\nor decentralized that assign tasks, monitor agent interactions,\nand manage dependencies across specialized agents. Together\nwith reflective memory architectures, these orchestration layers\nenable dynamic collaboration, task adaptability, and context\npreservation, marking a foundational shift toward scalable,\nintelligent agent collectives in Agentic AI systems.\nAdditionally, this study surveyed application domains in\nwhich these two paradigms (AI Agents and Agentic AI sys-\ntems) are deployed. For AI Agents, we illustrated their role\nin automating customer support, internal enterprise search,\nemail prioritization, and scheduling. For Agentic AI, we\n30\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nshowcased use cases in collaborative research, swarm robotics,\nmedical decision support, and adaptive workflow automation,\nsupported by practical examples and industry-grade systems.\nFinally, this study provided a deep analysis of the challenges\nand limitations affecting both paradigms. For AI Agents, we\ndiscussed hallucinations, shallow reasoning, and planning con-\nstraints as the key challenges faced, while for Agentic AI, we\naddressed amplified causality issues, coordination bottlenecks,\nemergent behavior, and governance concerns limiting the rapid\nadvancement and adoption of these systems.\nThrough this comparative framework, we conclude that AI\nAgents serve well in narrow, tool-integrated scenarios with\ndefined goals, while Agentic AI represents a paradigm shift to-\nward distributed, multi-agent cognition capable of autonomous\nplanning and decision-making. The transition from reactive\ntask execution to orchestrated, collaborative workflows marks\na significant milestone in the evolution of intelligent systems.\nThese insights offer a roadmap for the future development and\ndeployment of trustworthy, scalable Agentic AI systems that\nare capable of adapting to complex real-world environments\nACKNOWLEDGMENT\nThis work was supported in part by the National Science\nFoundation (NSF) and the United States Department of Agri-\nculture (USDA), National Institute of Food and Agriculture\n(NIFA), through the “Artificial Intelligence (AI) Institute for\nAgriculture” program under Award Numbers AWD003473 and\nAWD004595, and USDA-NIFA Accession Number 1029004\nfor the project titled “Robotic Blossom Thinning with Soft Ma-\nnipulators.” Additional support was provided through USDA-\nNIFA Grant Number 2024-67022-41788, Accession Number\n1031712, under the project “ExPanding UCF AI Research To\nNovel Agricultural EngineeRing Applications (PARTNER).”\nThe partial financial support for open access publication was\nprovided by the Hellenic Academic Libraries Link (HEAL-\nLink).\nDECLARATIONS\nThe authors declare no conflicts of interest.\nSTATEMENT ON AI W RITING ASSISTANCE\nChatGPT and Perplexity were utilized to enhance grammat-\nical accuracy and refine sentence structure; all AI-generated\nrevisions were thoroughly reviewed and edited for relevance.\nAdditionally, ChatGPT-4o was employed to generate realistic\nvisualizations.\nREFERENCES\n[1] E. Oliveira, K. Fischer, and O. Stepankova, “Multi-agent systems:\nwhich research for which applications,” Robotics and Autonomous\nSystems, vol. 27, no. 1-2, pp. 91–106, 1999.\n[2] Z. Ren and C. J. Anumba, “Multi-agent systems in construction–state\nof the art and prospects,” Automation in Construction , vol. 13, no. 3,\npp. 421–434, 2004.\n[3] C. Castelfranchi, “Modelling social action for ai agents,” Artificial\nintelligence, vol. 103, no. 1-2, pp. 157–182, 1998.\n[4] J. Ferber and G. Weiss, Multi-agent systems: an introduction to\ndistributed artificial intelligence , vol. 1. Addison-wesley Reading,\n1999.\n[5] R. Calegari, G. Ciatto, V . Mascardi, and A. Omicini, “Logic-based\ntechnologies for multi-agent systems: a systematic literature review,”\nAutonomous Agents and Multi-Agent Systems, vol. 35, no. 1, p. 1, 2021.\n[6] R. C. Cardoso and A. Ferrando, “A review of agent-based programming\nfor multi-agent systems,” Computers, vol. 10, no. 2, p. 16, 2021.\n[7] E. Shortliffe, Computer-based medical consultations: MYCIN , vol. 2.\nElsevier, 2012.\n[8] B. G. Buchanan and E. A. Feigenbaum, “Heuristic dendral: A program\nfor generating explanatory hypotheses in organic chemistry,” Machine\nIntelligence, vol. 9, pp. 117–134, 1978.\n[9] J. McDermott, “R1: A rule-based configurer of computer systems,”\nArtificial Intelligence, vol. 19, no. 1, pp. 39–88, 1982.\n[10] J. C. Giarratano and G. D. Riley, Expert Systems: Principles and\nProgramming. Boston, MA: Thomson Course Technology, 4th ed.,\n2005.\n[11] J. E. Laird, The Soar Cognitive Architecture . Cambridge, MA: MIT\nPress, 2012.\n[12] R. A. Brooks, “A robust layered control system for a mobile robot,”\nin IEEE Journal on Robotics and Automation , vol. RA-2, pp. 14–23,\nIEEE, 1986.\n[13] J. Weizenbaum, “Eliza—a computer program for the study of natural\nlanguage communication between man and machine,” Communications\nof the ACM , vol. 9, no. 1, pp. 36–45, 1966.\n[14] K. M. Colby, Artificial Paranoia: A Computer Simulation of Paranoid\nProcesses. New York: Pergamon Press, 1975.\n[15] H. P. Moravec, “The stanford cart and the cmu rover,” Proceedings of\nthe IEEE, vol. 71, no. 7, pp. 872–884, 1983.\n[16] B. Dai and H. Chen, “A multi-agent and auction-based framework and\napproach for carrier collaboration,” Logistics Research, vol. 3, pp. 101–\n120, 2011.\n[17] J. Grosset, A.-J. Foug `eres, M. Djoko-Kouam, and J.-M. Bonnin,\n“Multi-agent simulation of autonomous industrial vehicle fleets: To-\nwards dynamic task allocation in v2x cooperation mode,” Integrated\nComputer-Aided Engineering, vol. 31, no. 3, pp. 249–266, 2024.\n[18] R. A. Agis, S. Gottifredi, and A. J. Garc ´ıa, “An event-driven behavior\ntrees extension to facilitate non-player multi-agent coordination in\nvideo games,” Expert Systems with Applications , vol. 155, p. 113457,\n2020.\n[19] A. Guerra-Hern ´andez, A. El Fallah-Seghrouchni, and H. Soldano,\n“Learning in bdi multi-agent systems,” in International Workshop on\nComputational Logic in Multi-Agent Systems , pp. 218–233, Springer,\n2004.\n[20] A. Saadi, R. Maamri, and Z. Sahnoun, “Behavioral flexibility in belief-\ndesire-intention (bdi) architectures,” Multiagent and grid systems ,\nvol. 16, no. 4, pp. 343–377, 2020.\n[21] D. B. Acharya, K. Kuppan, and B. Divya, “Agentic ai: Autonomous\nintelligence for complex goals–a comprehensive survey,” IEEE Access,\n2025.\n[22] M. Z. Pan, M. Cemri, L. A. Agrawal, S. Yang, B. Chopra, R. Tiwari,\nK. Keutzer, A. Parameswaran, K. Ramchandran, D. Klein, et al., “Why\ndo multiagent systems fail?,” in ICLR 2025 Workshop on Building Trust\nin Language Models and Applications , 2025.\n[23] L. Hughes, Y . K. Dwivedi, T. Malik, M. Shawosh, M. A. Albashrawi,\nI. Jeon, V . Dutot, M. Appanderanda, T. Crick, R. De’,et al., “Ai agents\nand agentic systems: A multi-expert analysis,” Journal of Computer\nInformation Systems, pp. 1–29, 2025.\n[24] Z. Deng, Y . Guo, C. Han, W. Ma, J. Xiong, S. Wen, and Y . Xiang,\n“Ai agents under threat: A survey of key security challenges and future\npathways,” ACM Computing Surveys , vol. 57, no. 7, pp. 1–36, 2025.\n[25] M. Gridach, J. Nanavati, K. Z. E. Abidine, L. Mendes, and C. Mack,\n“Agentic ai for scientific discovery: A survey of progress, challenges,\nand future directions,” arXiv preprint arXiv:2503.08979 , 2025.\n[26] T. Song, M. Luo, X. Zhang, L. Chen, Y . Huang, J. Cao, Q. Zhu, D. Liu,\nB. Zhang, G. Zou, et al. , “A multiagent-driven robotic ai chemist\nenabling autonomous chemical research on demand,” Journal of the\nAmerican Chemical Society , vol. 147, no. 15, pp. 12534–12545, 2025.\n[27] M. M. Karim, D. H. Van, S. Khan, Q. Qu, and Y . Kholodov, “Ai\nagents meet blockchain: A survey on secure and scalable collaboration\nfor multi-agents,” Future Internet, vol. 17, no. 2, p. 57, 2025.\n[28] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., “Improv-\ning language understanding by generative pre-training,” arxiv, 2018.\n[29] J. S ´anchez Cuadrado, S. P ´erez-Soler, E. Guerra, and J. De Lara,\n“Automating the development of task-oriented llm-based chatbots,”\n31\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nin Proceedings of the 6th ACM Conference on Conversational User\nInterfaces, pp. 1–10, 2024.\n[30] Y . Lu, A. Aleta, C. Du, L. Shi, and Y . Moreno, “Llms and generative\nagent-based models for complex systems research,” Physics of Life\nReviews, 2024.\n[31] A. Zhang, Y . Chen, L. Sheng, X. Wang, and T.-S. Chua, “On generative\nagents in recommendation,” in Proceedings of the 47th international\nACM SIGIR conference on research and development in Information\nRetrieval, pp. 1807–1817, 2024.\n[32] S. Peng, E. Kalliamvakou, P. Cihon, and M. Demirer, “The impact\nof ai on developer productivity: Evidence from github copilot,” arXiv\npreprint arXiv:2302.06590, 2023.\n[33] J. Li, V . Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Cohen,\nH. Nguyen, and R. T. Gadde, “Jasper: An end-to-end convolutional\nneural acoustic model,” arXiv preprint arXiv:1904.03288 , 2019.\n[34] A. Jaruga-Rozdolska, “Artificial intelligence as part of future practices\nin the architect’s work: Midjourney generative tool as part of a process\nof creating an architectural form,” Architectus, no. 3 (71, pp. 95–104,\n2022.\n[35] K. Basu, “Bridging knowledge gaps in llms via function calls,” in\nProceedings of the 33rd ACM International Conference on Information\nand Knowledge Management , pp. 5556–5557, 2024.\n[36] Z. Liu, T. Hoang, J. Zhang, M. Zhu, T. Lan, J. Tan, W. Yao, Z. Liu,\nY . Feng, R. RN, et al. , “Apigen: Automated pipeline for generating\nverifiable and diverse function-calling datasets,” Advances in Neural\nInformation Processing Systems , vol. 37, pp. 54463–54482, 2024.\n[37] H. Yang, S. Yue, and Y . He, “Auto-gpt for online decision\nmaking: Benchmarks and additional opinions,” arXiv preprint\narXiv:2306.02224, 2023.\n[38] I. Hettiarachchi, “Exploring generative ai agents: Architecture, applica-\ntions, and challenges,” Journal of Artificial Intelligence General science\n(JAIGS) ISSN: 3006-4023 , vol. 8, no. 1, pp. 105–127, 2025.\n[39] A. Das, S.-C. Chen, M.-L. Shyu, and S. Sadiq, “Enabling synergistic\nknowledge sharing and reasoning in large language models with\ncollaborative multi-agents,” in 2023 IEEE 9th International Conference\non Collaboration and Internet Computing (CIC) , pp. 92–98, IEEE,\n2023.\n[40] R. Surapaneni, J. Miku, M. Vakoc, and T. Segal, “Announcing the\nagent2agent protocol (a2a) - google developers blog,” 4 2025.\n[41] Z. Duan and J. Wang, “Exploration of llm multi-agent applica-\ntion implementation based on langgraph+ crewai,” arXiv preprint\narXiv:2411.18241, 2024.\n[42] R. Sapkota, Y . Cao, K. I. Roumeliotis, and M. Karkee, “Vision-\nlanguage-action models: Concepts, progress, applications and chal-\nlenges,” arXiv preprint arXiv:2505.04769 , 2025.\n[43] R. Sapkota, K. I. Roumeliotis, R. H. Cheppally, M. F. Calero, and\nM. Karkee, “A review of 3d object detection with vision-language\nmodels,” arXiv preprint arXiv:2504.18738 , 2025.\n[44] R. Sapkota and M. Karkee, “Object detection with multimodal large\nvision-language models: An in-depth review,” Available at SSRN\n5233953, 2025.\n[45] B. Memarian and T. Doleck, “Human-in-the-loop in artificial intel-\nligence in education: A review and entity-relationship (er) analysis,”\nComputers in Human Behavior: Artificial Humans , vol. 2, no. 1,\np. 100053, 2024.\n[46] P. Bornet, J. Wirtz, T. H. Davenport, D. De Cremer, B. Evergreen,\nP. Fersht, R. Gohel, S. Khiyara, P. Sund, and N. Mullakara, Agentic\nArtificial Intelligence: Harnessing AI Agents to Reinvent Business,\nWork and Life. Irreplaceable Publishing, 2025.\n[47] F. Sado, C. K. Loo, W. S. Liew, M. Kerzel, and S. Wermter, “Ex-\nplainable goal-driven agents and robots-a comprehensive review,”ACM\nComputing Surveys, vol. 55, no. 10, pp. 1–41, 2023.\n[48] J. Heer, “Agency plus automation: Designing artificial intelligence into\ninteractive systems,” Proceedings of the National Academy of Sciences,\nvol. 116, no. 6, pp. 1844–1850, 2019.\n[49] G. Papagni, J. de Pagter, S. Zafari, M. Filzmoser, and S. T. Koeszegi,\n“Artificial agents’ explainability to support trust: considerations on\ntiming and context,” Ai & Society , vol. 38, no. 2, pp. 947–960, 2023.\n[50] P. Wang and H. Ding, “The rationality of explanation or human\ncapacity? understanding the impact of explainable artificial intelligence\non human-ai trust and decision performance,” Information Processing\n& Management, vol. 61, no. 4, p. 103732, 2024.\n[51] E. Popa, “Human goals are constitutive of agency in artificial intelli-\ngence (ai),” Philosophy & Technology, vol. 34, no. 4, pp. 1731–1750,\n2021.\n[52] M. Chacon-Chamorro, L. F. Giraldo, N. Quijano, V . Vargas-Panesso,\nC. Gonz ´alez, J. S. Pinz ´on, R. Manrique, M. R ´ıos, Y . Fonseca,\nD. G ´omez-Barrera, et al. , “Cooperative resilience in artificial intel-\nligence multiagent systems,” IEEE Transactions on Artificial Intelli-\ngence, 2025.\n[53] M. Adam, M. Wessel, and A. Benlian, “Ai-based chatbots in customer\nservice and their effects on user compliance,” Electronic Markets ,\nvol. 31, no. 2, pp. 427–445, 2021.\n[54] D. Leoc ´adio, L. Guedes, J. Oliveira, J. Reis, and N. Mel ˜ao, “Customer\nservice with ai-powered human-robot collaboration (hrc): A literature\nreview,” Procedia Computer Science , vol. 232, pp. 1222–1232, 2024.\n[55] T. Cao, Y . Q. Khoo, S. Birajdar, Z. Gong, C.-F. Chung, Y . Moghaddam,\nA. Xu, H. Mehta, A. Shukla, Z. Wang, et al. , “Designing towards\nproductivity: A centralized ai assistant concept for work,” The Human\nSide of Service Engineering , p. 118, 2024.\n[56] Y . Huang and J. X. Huang, “Exploring chatgpt for next-generation in-\nformation retrieval: Opportunities and challenges,” in Web Intelligence,\nvol. 22, pp. 31–44, SAGE Publications Sage UK: London, England,\n2024.\n[57] N. Holtz, S. Wittfoth, and J. M. G ´omez, “The new era of knowledge\nretrieval: Multi-agent systems meet generative ai,” in 2024 Portland In-\nternational Conference on Management of Engineering and Technology\n(PICMET), pp. 1–10, IEEE, 2024.\n[58] F. Poszler and B. Lange, “The impact of intelligent decision-support\nsystems on humans’ ethical decision-making: A systematic literature\nreview and an integrated framework,” Technological Forecasting and\nSocial Change, vol. 204, p. 123403, 2024.\n[59] F. Khemakhem, H. Ellouzi, H. Ltifi, and M. B. Ayed, “Agent-based\nintelligent decision support systems: a systematic review,” IEEE Trans-\nactions on Cognitive and Developmental Systems , vol. 14, no. 1,\npp. 20–34, 2020.\n[60] R. V . Florian, “Autonomous artificial intelligent agents,” Center for\nCognitive and Neural Studies (Coneural), Cluj-Napoca, Romania ,\n2003.\n[61] T. Hellstr ¨om, N. Kaiser, and S. Bensch, “A taxonomy of embodiment\nin the ai era,” Electronics, vol. 13, no. 22, p. 4441, 2024.\n[62] M. Wischnewski, “Attributing mental states to non-embodied au-\ntonomous systems: A systematic review,” in Proceedings of the Ex-\ntended Abstracts of the CHI Conference on Human Factors in Com-\nputing Systems, pp. 1–8, 2025.\n[63] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz,\n“Not what you’ve signed up for: Compromising real-world llm-\nintegrated applications with indirect prompt injection,” in Proceedings\nof the 16th ACM Workshop on Artificial Intelligence and Security ,\npp. 79–90, 2023.\n[64] Y . Talebirad and A. Nadiri, “Multi-agent collaboration: Harnessing\nthe power of intelligent llm agents,” arXiv preprint arXiv:2306.03314,\n2023.\n[65] A. I. Hauptman, B. G. Schelble, N. J. McNeese, and K. C. Madathil,\n“Adapt and overcome: Perceptions of adaptive autonomous agents\nfor human-ai teaming,” Computers in Human Behavior , vol. 138,\np. 107451, 2023.\n[66] P. Formosa, “Robot autonomy vs. human autonomy: social robots,\nartificial intelligence (ai), and the nature of autonomy,” Minds and\nMachines, vol. 31, no. 4, pp. 595–616, 2021.\n[67] N. Krishnan, “Advancing multi-agent systems through model con-\ntext protocol: Architecture, implementation, and applications,” arXiv\npreprint arXiv:2504.21030, 2025.\n[68] H. Padigela, C. Shah, and D. Juyal, “Ml-dev-bench: Comparative\nanalysis of ai agents on ml development workflows,” arXiv preprint\narXiv:2502.00964, 2025.\n[69] C. S. Eze and L. Shamir, “Analysis and prevention of ai-based phishing\nemail attacks,” Electronics, vol. 13, no. 10, p. 1839, 2024.\n[70] D. Singh, V . Patel, D. Bose, and A. Sharma, “Enhancing email market-\ning efficacy through ai-driven personalization: Leveraging natural lan-\nguage processing and collaborative filtering algorithms,” International\nJournal of AI Advancements , vol. 9, no. 4, 2020.\n[71] R. Khan, S. Sarkar, S. K. Mahata, and E. Jose, “Security threats in\nagentic ai system,” arXiv preprint arXiv:2410.14728 , 2024.\n[72] C. G. Endacott, “Enacting machine agency when ai makes one’s day:\nunderstanding how users relate to ai communication technologies for\n32\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nscheduling,” Journal of Computer-Mediated Communication , vol. 29,\nno. 4, p. zmae011, 2024.\n[73] M. Raees, I. Meijerink, I. Lykourentzou, V .-J. Khan, and K. Papangelis,\n“From explainable to interactive ai: A literature review on current\ntrends in human-ai interaction,” International Journal of Human-\nComputer Studies, p. 103301, 2024.\n[74] Z. Pawlak and A. Skowron, “Rudiments of rough sets,” Information\nsciences, vol. 177, no. 1, pp. 3–27, 2007.\n[75] P. Ponnusamy, A. Ghias, Y . Yi, B. Yao, C. Guo, and R. Sarikaya,\n“Feedback-based self-learning in large-scale conversational ai agents,”\nAI magazine, vol. 42, no. 4, pp. 43–56, 2022.\n[76] A. Zagalsky, D. Te’eni, I. Yahav, D. G. Schwartz, G. Silverman,\nD. Cohen, Y . Mann, and D. Lewinsky, “The design of reciprocal\nlearning between human and artificial intelligence,” Proceedings of the\nACM on Human-Computer Interaction , vol. 5, no. CSCW2, pp. 1–36,\n2021.\n[77] W. J. Clancey, “Heuristic classification,” Artificial intelligence, vol. 27,\nno. 3, pp. 289–350, 1985.\n[78] S. Kapoor, B. Stroebl, Z. S. Siegel, N. Nadgir, and A. Narayanan, “Ai\nagents that matter,” arXiv preprint arXiv:2407.01502 , 2024.\n[79] X. Huang, J. Lian, Y . Lei, J. Yao, D. Lian, and X. Xie, “Recommender\nai agent: Integrating large language models for interactive recommen-\ndations,” arXiv preprint arXiv:2308.16505 , 2023.\n[80] A. M. Baabdullah, A. A. Alalwan, R. S. Algharabat, B. Metri, and N. P.\nRana, “Virtual agents and flow experience: An empirical examination\nof ai-powered chatbots,” Technological Forecasting and Social Change,\nvol. 181, p. 121772, 2022.\n[81] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. , “Gpt-4\ntechnical report,” arXiv preprint arXiv:2303.08774 , 2023.\n[82] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann,et al., “Palm: Scaling\nlanguage modeling with pathways,” Journal of Machine Learning\nResearch, vol. 24, no. 240, pp. 1–113, 2023.\n[83] H. Honda and M. Hagiwara, “Question answering systems with deep\nlearning-based symbolic processing,” IEEE Access, vol. 7, pp. 152368–\n152378, 2019.\n[84] N. Karanikolas, E. Manga, N. Samaridi, E. Tousidou, and M. Vassi-\nlakopoulos, “Large language models versus natural language under-\nstanding and generation,” in Proceedings of the 27th Pan-Hellenic\nConference on Progress in Computing and Informatics , pp. 278–290,\n2023.\n[85] K. I. Roumeliotis, N. D. Tselikas, and D. K. Nasiopoulos, “Llms for\nproduct classification in e-commerce: A zero-shot comparative study of\ngpt and claude models,” Natural Language Processing Journal, vol. 11,\np. 100142, 6 2025.\n[86] A. S. George, A. H. George, T. Baskar, and A. G. Martin, “Revolu-\ntionizing business communication: Exploring the potential of gpt-4 in\ncorporate settings,” Partners Universal International Research Journal,\nvol. 2, no. 1, pp. 149–157, 2023.\n[87] K. I. Roumeliotis, N. D. Tselikas, and D. K. Nasiopoulos, “Think\nbefore you classify: The rise of reasoning large language models for\nconsumer complaint detection and classification,” Electronics 2025,\nVol. 14, Page 1070, vol. 14, p. 1070, 3 2025.\n[88] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, et al., “Learning transferable\nvisual models from natural language supervision,” in International\nconference on machine learning , pp. 8748–8763, PmLR, 2021.\n[89] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language\nmodels,” in International conference on machine learning , pp. 19730–\n19742, PMLR, 2023.\n[90] S. Sontakke, J. Zhang, S. Arnold, K. Pertsch, E. Bıyık, D. Sadigh,\nC. Finn, and L. Itti, “Roboclip: One demonstration is enough to learn\nrobot policies,” Advances in Neural Information Processing Systems ,\nvol. 36, pp. 55681–55693, 2023.\n[91] M. Elhenawy, H. I. Ashqar, A. Rakotonirainy, T. I. Alhadidi, A. Jaber,\nand M. A. Tami, “Vision-language models for autonomous driving:\nClip-based dynamic scene understanding,” Electronics, vol. 14, no. 7,\np. 1282, 2025.\n[92] S. Park, M. Lee, J. Kang, H. Choi, Y . Park, J. Cho, A. Lee, and D. Kim,\n“Vlaad: Vision and language assistant for autonomous driving,” in\nProceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pp. 980–987, 2024.\n[93] S. H. Ahmed, S. Hu, and G. Sukthankar, “The potential of vision-\nlanguage models for content moderation of children’s videos,” in\n2023 International Conference on Machine Learning and Applications\n(ICMLA), pp. 1237–1241, IEEE, 2023.\n[94] S. H. Ahmed, M. J. Khan, and G. Sukthankar, “Enhanced multimodal\ncontent moderation of children’s videos using audiovisual fusion,”\narXiv preprint arXiv:2405.06128 , 2024.\n[95] K. I. Roumeliotis, R. Sapkota, M. Karkee, N. D. Tselikas, and\nD. K. Nasiopoulos, “Plant disease detection through multimodal large\nlanguage models and convolutional neural networks,” 4 2025.\n[96] P. Chitra and A. Saleem Raja, “Artificial intelligence (ai) algorithm and\nmodels for embodied agents (robots and drones),” in Building Embod-\nied AI Systems: The Agents, the Architecture Principles, Challenges,\nand Application Domains , pp. 417–441, Springer, 2025.\n[97] S. Kourav, K. Verma, and M. Sundararajan, “Artificial intelligence\nalgorithm models for agents of embodiment for drone applications,”\nin Building Embodied AI Systems: The Agents, the Architecture Prin-\nciples, Challenges, and Application Domains , pp. 79–101, Springer,\n2025.\n[98] G. Natarajan, E. Elango, B. Sundaravadivazhagan, and S. Rethinam,\n“Artificial intelligence algorithms and models for embodied agents:\nEnhancing autonomy in drones and robots,” in Building Embodied\nAI Systems: The Agents, the Architecture Principles, Challenges, and\nApplication Domains, pp. 103–132, Springer, 2025.\n[99] K. Pandya and M. Holia, “Automating customer service using\nlangchain: Building custom open-source gpt chatbot for organizations,”\narXiv preprint arXiv:2310.05421 , 2023.\n[100] Q. Wu, G. Bansal, J. Zhang, Y . Wu, B. Li, E. Zhu, L. Jiang, X. Zhang,\nS. Zhang, J. Liu, et al., “Autogen: Enabling next-gen llm applications\nvia multi-agent conversation,” arXiv preprint arXiv:2308.08155, 2023.\n[101] L. Gabora and J. Bach, “A path to generative artificial selves,” in EPIA\nConference on Artificial Intelligence , pp. 15–29, Springer, 2023.\n[102] G. Pezzulo, T. Parr, P. Cisek, A. Clark, and K. Friston, “Generating\nmeaning: active inference and the scope and limits of passive ai,”\nTrends in Cognitive Sciences , vol. 28, no. 2, pp. 97–112, 2024.\n[103] J. Li, M. Zhang, N. Li, D. Weyns, Z. Jin, and K. Tei, “Generative ai\nfor self-adaptive systems: State of the art and research roadmap,” ACM\nTransactions on Autonomous and Adaptive Systems , vol. 19, no. 3,\npp. 1–60, 2024.\n[104] W. O’Grady and M. Lee, “Natural syntax, artificial intelligence and\nlanguage acquisition,” Information, vol. 14, no. 7, p. 418, 2023.\n[105] X. Liu, J. Wang, J. Sun, X. Yuan, G. Dong, P. Di, W. Wang,\nand D. Wang, “Prompting frameworks for large language models: A\nsurvey,” arXiv preprint arXiv:2311.12785 , 2023.\n[106] E. T. Rolls, “The memory systems of the human brain and generative\nartificial intelligence,” Heliyon, vol. 10, no. 11, 2024.\n[107] K. Alizadeh, S. I. Mirzadeh, D. Belenko, S. Khatamifard, M. Cho, C. C.\nDel Mundo, M. Rastegari, and M. Farajtabar, “Llm in a flash: Efficient\nlarge language model inference with limited memory,” in Proceedings\nof the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pp. 12562–12584, 2024.\n[108] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, A. Wahid,\nJ. Tompson, Q. Vuong, T. Yu, W. Huang,et al., “Palm-e: An embodied\nmultimodal language model,” 2023.\n[109] P. Denny, J. Leinonen, J. Prather, A. Luxton-Reilly, T. Amarouche,\nB. A. Becker, and B. N. Reeves, “Prompt problems: A new pro-\ngramming exercise for the generative ai era,” in Proceedings of the\n55th ACM Technical Symposium on Computer Science Education V . 1,\npp. 296–302, 2024.\n[110] C. Chen, S. Lee, E. Jang, and S. S. Sundar, “Is your prompt detailed\nenough? exploring the effects of prompt coaching on users’ percep-\ntions, engagement, and trust in text-to-image generative ai tools,” in\nProceedings of the Second International Symposium on Trustworthy\nAutonomous Systems, pp. 1–12, 2024.\n[111] OpenAI, “Introducing gpt-4.1 in the api,” 4 2025.\n[112] A. Pan, E. Jones, M. Jagadeesan, and J. Steinhardt, “Feedback loops\nwith language models drive in-context reward hacking,” arXiv preprint\narXiv:2402.06627, 2024.\n[113] K. Nabben, “Ai as a constituted system: accountability lessons from\nan llm experiment,” Data & policy , vol. 6, p. e57, 2024.\n[114] P. J. Pesch, “Potentials and challenges of large language models (llms)\nin the context of administrative decision-making,” European Journal\nof Risk Regulation , pp. 1–20, 2025.\n33\nAI Agents vs. Agentic AI by Sapkota et al. 2025\n[115] C. Wang, Y . Deng, Z. Lyu, L. Zeng, J. He, S. Yan, and B. An, “Q*:\nImproving multi-step reasoning for llms with deliberative planning,”\narXiv preprint arXiv:2406.14283 , 2024.\n[116] H. Wei, Z. Zhang, S. He, T. Xia, S. Pan, and F. Liu, “Plangen-\nllms: A modern survey of llm planning capabilities,” arXiv preprint\narXiv:2502.11221, 2025.\n[117] A. Bandi, P. V . S. R. Adapa, and Y . E. V . P. K. Kuchi, “The power of\ngenerative ai: A review of requirements, models, input–output formats,\nevaluation metrics, and challenges,” Future Internet , vol. 15, no. 8,\np. 260, 2023.\n[118] Y . Liu, H. Du, D. Niyato, J. Kang, Z. Xiong, Y . Wen, and D. I. Kim,\n“Generative ai in data center networking: Fundamentals, perspectives,\nand case study,” IEEE Network, 2025.\n[119] C. Guo, F. Cheng, Z. Du, J. Kiessling, J. Ku, S. Li, Z. Li, M. Ma,\nT. Molom-Ochir, B. Morris, et al., “A survey: Collaborative hardware\nand software design in the era of large language models,”IEEE Circuits\nand Systems Magazine , vol. 25, no. 1, pp. 35–57, 2025.\n[120] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell,et al., “Language mod-\nels are few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[121] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, et al., “Llama:\nOpen and efficient foundation language models,” arXiv preprint\narXiv:2302.13971, 2023.\n[122] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\nwith a unified text-to-text transformer,” Journal of machine learning\nresearch, vol. 21, no. 140, pp. 1–67, 2020.\n[123] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Bian, C. Yin, C. Lv, D. Pan,\nD. Wang, D. Yan, et al. , “Baichuan 2: Open large-scale language\nmodels,” arXiv preprint arXiv:2309.10305 , 2023.\n[124] K. M. Yoo, D. Park, J. Kang, S.-W. Lee, and W. Park, “Gpt3mix:\nLeveraging large-scale language models for text augmentation,” arXiv\npreprint arXiv:2104.08826, 2021.\n[125] D. Zhou, X. Xue, X. Lu, Y . Guo, P. Ji, H. Lv, W. He, Y . Xu, Q. Li,\nand L. Cui, “A hierarchical model for complex adaptive system: From\nadaptive agent to ai society,” ACM Transactions on Autonomous and\nAdaptive Systems, 2024.\n[126] H. Hao, Y . Wang, and J. Chen, “Empowering scenario planning with\nartificial intelligence: A perspective on building smart and resilient\ncities,” Engineering, 2024.\n[127] Y . Wang, J. Zhu, Z. Cheng, L. Qiu, Z. Tong, and J. Huang, “Intelligent\noptimization method for real-time decision-making in laminated cool-\ning configurations through reinforcement learning,” Energy, vol. 291,\np. 130434, 2024.\n[128] X. Xiang, J. Xue, L. Zhao, Y . Lei, C. Yue, and K. Lu, “Real-\ntime integration of fine-tuned large language model for improved\ndecision-making in reinforcement learning,” in 2024 International Joint\nConference on Neural Networks (IJCNN) , pp. 1–8, IEEE, 2024.\n[129] Z. Li, H. Zhang, C. Peng, and R. Peiris, “Exploring large language\nmodel-driven agents for environment-aware spatial interactions and\nconversations in virtual reality role-play scenarios,” in 2025 IEEE\nConference Virtual Reality and 3D User Interfaces (VR) , pp. 1–11,\nIEEE, 2025.\n[130] T. R. McIntosh, T. Susnjak, T. Liu, P. Watters, and M. N. Halgamuge,\n“The inadequacy of reinforcement learning from human feedback-\nradicalizing large language models via semantic vulnerabilities,” IEEE\nTransactions on Cognitive and Developmental Systems , 2024.\n[131] S. Lee, G. Lee, W. Kim, J. Kim, J. Park, and K. Cho, “Human strategy\nlearning-based multi-agent deep reinforcement learning for online team\nsports game,” IEEE Access, 2025.\n[132] Z. Shi, S. Gao, L. Yan, Y . Feng, X. Chen, Z. Chen, D. Yin, S. Ver-\nberne, and Z. Ren, “Tool learning in the wild: Empowering language\nmodels as automatic tool agents,” in Proceedings of the ACM on Web\nConference 2025, pp. 2222–2237, 2025.\n[133] S. Yuan, K. Song, J. Chen, X. Tan, Y . Shen, R. Kan, D. Li, and D. Yang,\n“Easytool: Enhancing llm-based agents with concise tool instruction,”\narXiv preprint arXiv:2401.06201 , 2024.\n[134] B. Xu, X. Liu, H. Shen, Z. Han, Y . Li, M. Yue, Z. Peng, Y . Liu, Z. Yao,\nand D. Xu, “Gentopia: A collaborative platform for tool-augmented\nllms,” arXiv preprint arXiv:2308.04030 , 2023.\n[135] H. Lu, X. Li, X. Ji, Z. Kan, and Q. Hu, “Toolfive: Enhancing tool-\naugmented llms via tool filtering and verification,” in ICASSP 2025-\n2025 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 1–5, IEEE, 2025.\n[136] Y . Song, F. Xu, S. Zhou, and G. Neubig, “Beyond browsing: Api-based\nweb agents,” arXiv preprint arXiv:2410.16464 , 2024.\n[137] V . Tupe and S. Thube, “Ai agentic workflows and enterprise apis:\nAdapting api architectures for the age of ai agents,” arXiv preprint\narXiv:2502.17443, 2025.\n[138] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,\n“React: Synergizing reasoning and acting in language models,” in\nInternational Conference on Learning Representations (ICLR) , 2023.\n[139] OpenAI, “Introducing chatgpt search,” 10 2024.\n[140] L. Ning, Z. Liang, Z. Jiang, H. Qu, Y . Ding, W. Fan, X.-y. Wei,\nS. Lin, H. Liu, P. S. Yu, et al. , “A survey of webagents: Towards\nnext-generation ai agents for web automation with large foundation\nmodels,” arXiv preprint arXiv:2503.23350 , 2025.\n[141] M. W. U. Rahman, R. Nevarez, L. T. Mim, and S. Hariri, “Multi-\nagent actor-critic generative ai for query resolution and analysis,” IEEE\nTransactions on Artificial Intelligence , 2025.\n[142] J. L ´ala, O. O’Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques,\nand A. D. White, “Paperqa: Retrieval-augmented generative agent for\nscientific research,” arXiv preprint arXiv:2312.07559 , 2023.\n[143] Z. Wu, C. Yu, C. Chen, J. Hao, and H. H. Zhuo, “Models as agents:\nOptimizing multi-step predictions of interactive local models in model-\nbased multi-agent reinforcement learning,” in Proceedings of the AAAI\nConference on Artificial Intelligence , vol. 37, pp. 10435–10443, 2023.\n[144] Z. Feng, R. Xue, L. Yuan, Y . Yu, N. Ding, M. Liu, B. Gao, J. Sun, and\nG. Wang, “Multi-agent embodied ai: Advances and future directions,”\narXiv preprint arXiv:2505.05108 , 2025.\n[145] A. Feriani and E. Hossain, “Single and multi-agent deep reinforcement\nlearning for ai-enabled wireless networks: A tutorial,” IEEE Commu-\nnications Surveys & Tutorials , vol. 23, no. 2, pp. 1226–1252, 2021.\n[146] R. Zhang, S. Tang, Y . Liu, D. Niyato, Z. Xiong, S. Sun, S. Mao,\nand Z. Han, “Toward agentic ai: generative information retrieval\ninspired intelligent communications and networking,” arXiv preprint\narXiv:2502.16866, 2025.\n[147] U. M. Borghoff, P. Bottoni, and R. Pareschi, “Human-artificial interac-\ntion in the age of agentic ai: a system-theoretical approach,” Frontiers\nin Human Dynamics , vol. 7, p. 1579166, 2025.\n[148] E. Miehling, K. N. Ramamurthy, K. R. Varshney, M. Riemer, D. Boun-\neffouf, J. T. Richards, A. Dhurandhar, E. M. Daly, M. Hind, P. Sat-\ntigeri, et al. , “Agentic ai needs a systems theory,” arXiv preprint\narXiv:2503.00237, 2025.\n[149] W. Xu, Z. Liang, K. Mei, H. Gao, J. Tan, and Y . Zhang, “A-mem:\nAgentic memory for llm agents,” arXiv preprint arXiv:2502.12110 ,\n2025.\n[150] C. Riedl and D. De Cremer, “Ai for collective intelligence,” Collective\nIntelligence, vol. 4, no. 2, p. 26339137251328909, 2025.\n[151] L. Peng, D. Li, Z. Zhang, T. Zhang, A. Huang, S. Yang, and Y . Hu,\n“Human-ai collaboration: Unraveling the effects of user proficiency\nand ai agent capability in intelligent decision support systems,” Inter-\nnational Journal of Industrial Ergonomics , vol. 103, p. 103629, 2024.\n[152] H. Shirado, K. Shimizu, N. A. Christakis, and S. Kasahara, “Realism\ndrives interpersonal reciprocity but yields to ai-assisted egocentrism in\na coordination experiment,” inProceedings of the 2025 CHI Conference\non Human Factors in Computing Systems , pp. 1–21, 2025.\n[153] Y . Xiao, G. Shi, and P. Zhang, “Towards agentic ai networking in\n6g: A generative foundation model-as-agent approach,” arXiv preprint\narXiv:2503.15764, 2025.\n[154] P. R. Lewis and S ¸. Sarkadi, “Reflective artificial intelligence,” Minds\nand Machines, vol. 34, no. 2, p. 14, 2024.\n[155] C. Qian, W. Liu, H. Liu, N. Chen, Y . Dang, J. Li, C. Yang, W. Chen,\nY . Su, X. Cong, et al., “Chatdev: Communicative agents for software\ndevelopment,” arXiv preprint arXiv:2307.07924 , 2023.\n[156] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and\nM. S. Bernstein, “Generative agents: Interactive simulacra of human\nbehavior,” in UIST 2023 - Proceedings of the 36th Annual ACM\nSymposium on User Interface Software and Technology , Association\nfor Computing Machinery, Inc, 10 2023.\n[157] S. Hong, X. Zheng, J. Chen, Y . Cheng, J. Wang, C. Zhang,\nZ. Wang, S. K. S. Yau, Z. Lin, L. Zhou, et al. , “Metagpt: Meta\nprogramming for multi-agent collaborative framework,” arXiv preprint\narXiv:2308.00352, vol. 3, no. 4, p. 6, 2023.\n[158] Y . Liang, C. Wu, T. Song, W. Wu, Y . Xia, Y . Liu, Y . Ou, S. Lu,\nL. Ji, S. Mao, et al., “Taskmatrix. ai: Completing tasks by connecting\n34\nAI Agents vs. Agentic AI by Sapkota et al. 2025\nfoundation models with millions of apis,” Intelligent Computing, vol. 3,\np. 0063, 2024.\n[159] H. Hexmoor, J. Lammens, G. Caicedo, and S. C. Shapiro, Behaviour\nbased AI, cognitive processes, and emergent behaviors in autonomous\nagents, vol. 1. WIT Press, 2025.\n[160] H. Zhang, Z. Li, F. Liu, Y . He, Z. Cao, and Y . Zheng, “Design\nand implementation of langchain-based chatbot,” in 2024 International\nSeminar on Artificial Intelligence, Computer Technology and Control\nEngineering (ACTCE), pp. 226–229, IEEE, 2024.\n[161] E. Ephrati and J. S. Rosenschein, “A heuristic technique for multi-agent\nplanning,” Annals of Mathematics and Artificial Intelligence , vol. 20,\npp. 13–67, 1997.\n[162] S. Kupferschmid, J. Hoffmann, H. Dierks, and G. Behrmann, “Adapting\nan ai planning heuristic for directed model checking,” in International\nSPIN Workshop on Model Checking of Software , pp. 35–52, Springer,\n2006.\n[163] W. Chen, Y . Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C.-M. Chan,\nY . Qin, Y . Lu, R. Xie, et al. , “Agentverse: Facilitating multi-agent\ncollaboration and exploring emergent behaviors in agents,” arXiv\npreprint arXiv:2308.10848, vol. 2, no. 4, p. 6, 2023.\n[164] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli, E. Ham-\nbro, L. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer:\nLanguage models can teach themselves to use tools,” Advances in\nNeural Information Processing Systems , vol. 36, pp. 68539–68551,\n2023.\n[165] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le,\nD. Zhou, et al., “Chain-of-thought prompting elicits reasoning in large\nlanguage models,” Advances in neural information processing systems ,\nvol. 35, pp. 24824–24837, 2022.\n[166] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y . Cao, and\nK. Narasimhan, “Tree of thoughts: Deliberate problem solving with\nlarge language models,” Advances in neural information processing\nsystems, vol. 36, pp. 11809–11822, 2023.\n[167] J. Guo, N. Li, J. Qi, H. Yang, R. Li, Y . Feng, S. Zhang, and M. Xu,\n“Empowering working memory for large language model agents,”\narXiv preprint arXiv:2312.17259 , 2023.\n[168] S. Agashe, J. Han, S. Gan, J. Yang, A. Li, and X. E. Wang, “Agent s:\nAn open agentic framework that uses computers like a human,” arXiv\npreprint arXiv:2410.08164, 2024.\n[169] C. DeChant, “Episodic memory in ai agents poses risks that should be\nstudied and mitigated,” arXiv preprint arXiv:2501.11739 , 2025.\n[170] A. M. Nuxoll and J. E. Laird, “Enhancing intelligent agents with\nepisodic memory,” Cognitive Systems Research , vol. 17, pp. 34–48,\n2012.\n[171] G. Sarthou, A. Clodic, and R. Alami, “Ontologenius: A long-term\nsemantic memory for robotic agents,” in 2019 28th IEEE International\nConference on Robot and Human Interactive Communication (RO-\nMAN), pp. 1–8, IEEE, 2019.\n[172] A.-e.-h. Munir and W. M. Qazi, “Artificial subjectivity: Personal se-\nmantic memory model for cognitive agents,” Applied Sciences, vol. 12,\nno. 4, p. 1903, 2022.\n[173] A. Singh, A. Ehtesham, S. Kumar, and T. T. Khoei, “Agentic retrieval-\naugmented generation: A survey on agentic rag,” arXiv preprint\narXiv:2501.09136, 2025.\n[174] R. Akkiraju, A. Xu, D. Bora, T. Yu, L. An, V . Seth, A. Shukla, P. Gun-\ndecha, H. Mehta, A. Jha, et al. , “Facts about building retrieval aug-\nmented generation-based chatbots,” arXiv preprint arXiv:2407.07858 ,\n2024.\n[175] R. Sapkota, K. I. Roumeliotis, S. Pokhrel, and M. Karkee,\n“From self-learning to self-evolving architectures in large language\nmodels: A short survey,” May 2025. TechRxiv preprint DOI:\n10.36227/techrxiv.174838070.09235849/v1.\n[176] W. Samar, “World’s 1st ai hospital in china - a milestone in healthcare\ninnovation,” 2024.\n[177] N. Karunanayake, “Next-generation agentic ai for transforming health-\ncare,” Informatics and Health , vol. 2, no. 2, pp. 73–83, 2025.\n[178] R. Khanda, “Agentic ai-driven technical troubleshooting for enterprise\nsystems: A novel weighted retrieval-augmented generation paradigm,”\n2024.\n[179] G. Wang, Y . Xie, Y . Jiang, A. Mandlekar, C. Xiao, Y . Zhu, L. Fan,\nand A. Anandkumar, “V oyager: An open-ended embodied agent with\nlarge language models,” arXiv preprint arXiv:2305.16291 , 2023.\n[180] G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “Camel:\nCommunicative agents for” mind” exploration of large language model\nsociety,” Advances in Neural Information Processing Systems , vol. 36,\npp. 51991–52008, 2023.\n[181] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov,\nG. Barth-Maron, M. Gimenez, Y . Sulsky, J. Kay, J. T. Springenberg,\net al., “A generalist agent,” arXiv preprint arXiv:2205.06175 , 2022.\n[182] C. K. Thomas, C. Chaccour, W. Saad, M. Debbah, and C. S. Hong,\n“Causal reasoning: Charting a revolutionary course for next-generation\nai-native wireless networks,” IEEE Vehicular Technology Magazine ,\n2024.\n[183] Z. Tang, R. Wang, W. Chen, K. Wang, Y . Liu, T. Chen, and L. Lin,\n“Towards causalgpt: A multi-agent approach for faithful knowledge\nreasoning via promoting causal consistency in llms,” arXiv preprint\narXiv:2308.11914, 2023.\n[184] R. Sapkota, R. Qureshi, S. Z. Hassan, J. Shutske, M. Shoman, M. Saj-\njad, F. A. Dharejo, A. Paudel, J. Li, Z. Meng, et al. , “Multi-modal\nllms in agriculture: A comprehensive review,”Authorea Preprints DOI:\n10.36227/techrxiv.172651082.24507804/v1, 2024.\n[185] Z. Gekhman, J. Herzig, R. Aharoni, C. Elkind, and I. Szpektor,\n“Trueteacher: Learning factual consistency evaluation with large lan-\nguage models,” arXiv preprint arXiv:2305.11171 , 2023.\n[186] A. Wu, K. Kuang, M. Zhu, Y . Wang, Y . Zheng, K. Han, B. Li, G. Chen,\nF. Wu, and K. Zhang, “Causality for large language models,” arXiv\npreprint arXiv:2410.15319, 2024.\n[187] S. Ashwani, K. Hegde, N. R. Mannuru, D. S. Sengar, M. Jindal,\nK. C. R. Kathala, D. Banga, V . Jain, and A. Chadha, “Cause and effect:\ncan large language models truly understand causality?,” in Proceedings\nof the AAAI Symposium Series , vol. 4, pp. 2–9, 2024.\n[188] J. Richens and T. Everitt, “Robust agents learn causal world models,”\nin The Twelfth International Conference on Learning Representations ,\n2024.\n[189] A. Chan, R. Salganik, A. Markelius, C. Pang, N. Rajkumar,\nD. Krasheninnikov, L. Langosco, Z. He, Y . Duan, M. Carroll, et al. ,\n“Harms from increasingly agentic algorithmic systems,” in Proceed-\nings of the 2023 ACM Conference on Fairness, Accountability, and\nTransparency, pp. 651–666, 2023.\n[190] A. Plaat, M. van Duijn, N. van Stein, M. Preuss, P. van der Putten,\nand K. J. Batenburg, “Agentic large language models, a survey,” arXiv\npreprint arXiv:2503.23037, 2025.\n[191] J. Qiu, K. Lam, G. Li, A. Acharya, T. Y . Wong, A. Darzi, W. Yuan, and\nE. J. Topol, “Llm-based agentic systems in medicine and healthcare,”\nNature Machine Intelligence , vol. 6, no. 12, pp. 1418–1420, 2024.\n[192] G. A. Gabison and R. P. Xian, “Inherent and emergent liability issues\nin llm-based agentic systems: a principal-agent perspective,” arXiv\npreprint arXiv:2504.03255, 2025.\n[193] M. Dahl, V . Magesh, M. Suzgun, and D. E. Ho, “Large legal fictions:\nProfiling legal hallucinations in large language models,” Journal of\nLegal Analysis, vol. 16, no. 1, pp. 64–93, 2024.\n[194] Y . A. Latif, “Hallucinations in large language models and their\ninfluence on legal reasoning: Examining the risks of ai-generated\nfactual inaccuracies in judicial processes,” Journal of Computational\nIntelligence, Machine Reasoning, and Decision-Making , vol. 10, no. 2,\npp. 10–20, 2025.\n[195] S. Tonmoy, S. Zaman, V . Jain, A. Rani, V . Rawte, A. Chadha, and\nA. Das, “A comprehensive survey of hallucination mitigation tech-\nniques in large language models,” arXiv preprint arXiv:2401.01313 ,\nvol. 6, 2024.\n[196] Z. Zhang, Y . Yao, A. Zhang, X. Tang, X. Ma, Z. He, Y . Wang,\nM. Gerstein, R. Wang, G. Liu, et al., “Igniting language intelligence:\nThe hitchhiker’s guide from chain-of-thought reasoning to language\nagents,” ACM Computing Surveys , vol. 57, no. 8, pp. 1–39, 2025.\n[197] Y . Wan and K.-W. Chang, “White men lead, black women help?\nbenchmarking language agency social biases in llms,” arXiv preprint\narXiv:2404.10508, 2024.\n[198] A. Borah and R. Mihalcea, “Towards implicit bias detection and mitiga-\ntion in multi-agent llm interactions,” arXiv preprint arXiv:2410.02584,\n2024.\n[199] X. Liu, H. Yu, H. Zhang, Y . Xu, X. Lei, H. Lai, Y . Gu, H. Ding,\nK. Men, K. Yang, et al. , “Agentbench: Evaluating llms as agents,”\narXiv preprint arXiv:2308.03688 , 2023.\n[200] G. He, G. Demartini, and U. Gadiraju, “Plan-then-execute: An empir-\nical study of user trust and team performance when using llm agents\nas a daily assistant,” in Proceedings of the 2025 CHI Conference on\nHuman Factors in Computing Systems , pp. 1–22, 2025.\n35\nAI Agents vs. Agentic AI by Sapkota et al. 2025\n[201] Z. Ke, F. Jiao, Y . Ming, X.-P. Nguyen, A. Xu, D. X. Long, M. Li,\nC. Qin, P. Wang, S. Savarese, et al. , “A survey of frontiers in llm\nreasoning: Inference scaling, learning to reason, and agentic systems,”\narXiv preprint arXiv:2504.09037 , 2025.\n[202] M. Luo, X. Shi, C. Cai, T. Zhang, J. Wong, Y . Wang, C. Wang,\nY . Huang, Z. Chen, J. E. Gonzalez, et al. , “Autellix: An efficient\nserving engine for llm agents as general programs,” arXiv preprint\narXiv:2502.13965, 2025.\n[203] K. Hatalis, D. Christou, J. Myers, S. Jones, K. Lambert, A. Amos-\nBinks, Z. Dannenhauer, and D. Dannenhauer, “Memory matters: The\nneed to improve long-term memory in llm-agents,” in Proceedings of\nthe AAAI Symposium Series , vol. 2, pp. 277–280, 2023.\n[204] H. Jin, X. Han, J. Yang, Z. Jiang, Z. Liu, C.-Y . Chang, H. Chen, and\nX. Hu, “Llm maybe longlm: Self-extend llm context window without\ntuning,” arXiv preprint arXiv:2401.01325 , 2024.\n[205] M. Wooldridge and N. R. Jennings, “Intelligent agents: theory and prac-\ntice,” The Knowledge Engineering Review , vol. 10, no. 2, p. 115–152,\n1995.\n[206] M. Yu, F. Meng, X. Zhou, S. Wang, J. Mao, L. Pang, T. Chen, K. Wang,\nX. Li, Y . Zhang, et al., “A survey on trustworthy llm agents: Threats\nand countermeasures,” arXiv preprint arXiv:2503.09648 , 2025.\n[207] H. Chi, H. Li, W. Yang, F. Liu, L. Lan, X. Ren, T. Liu, and B. Han,\n“Unveiling causal reasoning in large language models: Reality or\nmirage?,” Advances in Neural Information Processing Systems, vol. 37,\npp. 96640–96670, 2024.\n[208] H. Wang, A. Zhang, N. Duy Tai, J. Sun, T.-S. Chua, et al., “Ali-agent:\nAssessing llms’ alignment with human values via agent-based evalu-\nation,” Advances in Neural Information Processing Systems , vol. 37,\npp. 99040–99088, 2024.\n[209] M. Ziembla, E. Uyarra, and J. Pinkse, “Examining system-level agency\nin the context of spatially embedded industries: a study of greater\nmanchester’s domestic retrofit industry,” Regional Studies , vol. 59,\nno. 1, p. 2446574, 2025.\n[210] Y . Yang, Q. Peng, J. Wang, and W. Zhang, “Multi-llm-agent\nsystems: Techniques and business perspectives,” arXiv preprint\narXiv:2411.14033, 2024.\n[211] L. Hammond, A. Chan, J. Clifton, J. Hoelscher-Obermaier, A. Khan,\nE. McLean, C. Smith, W. Barfuss, J. Foerster, T. Gaven ˇciak,\net al. , “Multi-agent risks from advanced ai,” arXiv preprint\narXiv:2502.14143, 2025.\n[212] D. Trusilo, “Autonomous ai systems in conflict: Emergent behavior and\nits impact on predictability and reliability,” Journal of Military Ethics ,\nvol. 22, no. 1, pp. 2–17, 2023.\n[213] R. Sapkota, S. Raza, and M. Karkee, “Comprehensive analysis of\ntransparency and accessibility of chatgpt, deepseek, and other sota large\nlanguage models,” arXiv preprint arXiv:2502.18505 , 2025.\n[214] S. Raza, R. Qureshi, A. Zahid, J. Fioresi, F. Sadak, M. Saeed, R. Sap-\nkota, A. Jain, A. Zafar, M. U. Hassan, et al. , “Who is responsible?\nthe data, models, users or regulations? a comprehensive survey on\nresponsible generative ai for a sustainable future,” arXiv preprint\narXiv:2502.08650, 2025.\n[215] M. Puvvadi, S. K. Arava, A. Santoria, S. S. P. Chennupati, and H. V .\nPuvvadi, “Coding agents: A comprehensive survey of automated bug\nfixing systems and benchmarks,” in 2025 IEEE 14th International\nConference on Communication Systems and Network Technologies\n(CSNT), pp. 680–686, IEEE, 2025.\n[216] C. Newton, J. Singleton, C. Copland, S. Kitchen, and J. Hudack,\n“Scalability in modeling and simulation systems for multi-agent, ai, and\nmachine learning applications,” in Artificial Intelligence and Machine\nLearning for Multi-Domain Operations Applications III , vol. 11746,\npp. 534–552, SPIE, 2021.\n[217] H. D. Le, X. Xia, and Z. Chen, “Multi-agent causal discovery using\nlarge language models,” arXiv preprint arXiv:2407.15073 , 2024.\n[218] Y . Shavit, S. Agarwal, M. Brundage, S. Adler, C. O’Keefe, R. Camp-\nbell, T. Lee, P. Mishkin, T. Eloundou, A. Hickey, et al., “Practices for\ngoverning agentic ai systems,” Research Paper, OpenAI, 2023.\n[219] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel, et al. , “Retrieval-\naugmented generation for knowledge-intensive nlp tasks,” Advances in\nneural information processing systems , vol. 33, pp. 9459–9474, 2020.\n[220] Y . Ma, Z. Gou, J. Hao, R. Xu, S. Wang, L. Pan, Y . Yang, Y . Cao,\nA. Sun, H. Awadalla, et al. , “Sciagent: Tool-augmented language\nmodels for scientific reasoning,” arXiv preprint arXiv:2402.11451 ,\n2024.\n[221] K. Dev, S. A. Khowaja, K. Singh, E. Zeydan, and M. Debbah,\n“Advanced architectures integrated with agentic ai for next-generation\nwireless networks,” arXiv preprint arXiv:2502.01089 , 2025.\n[222] L. Xie, C. Zheng, H. Xia, H. Qu, and C. Zhu-Tian, “Waitgpt: Moni-\ntoring and steering conversational llm agent in data analysis with on-\nthe-fly code visualization,” in Proceedings of the 37th Annual ACM\nSymposium on User Interface Software and Technology , pp. 1–14,\n2024.\n[223] A. Chan, C. Ezell, M. Kaufmann, K. Wei, L. Hammond, H. Bradley,\nE. Bluemke, N. Rajkumar, D. Krueger, N. Kolt, et al., “Visibility into\nai agents,” in Proceedings of the 2024 ACM Conference on Fairness,\nAccountability, and Transparency, pp. 958–973, 2024.\n[224] W. Murikah, J. K. Nthenge, and F. M. Musyoka, “Bias and ethics of\nai systems applied in auditing-a systematic review,” Scientific African,\np. e02281, 2024.\n[225] S. Banerjee, V . K. Verma, A. Mukherjee, D. Gupta, V . P. Namboodiri,\nand P. Rai, “Verse: Virtual-gradient aware streaming lifelong learning\nwith anytime inference,” in 2024 IEEE International Conference on\nRobotics and Automation (ICRA) , pp. 493–500, IEEE, 2024.\n[226] A. Boyle and A. Blomkvist, “Elements of episodic memory: in-\nsights from artificial agents,” Philosophical Transactions B , vol. 379,\nno. 1913, p. 20230416, 2024.\n[227] Y . Du, W. Huang, D. Zheng, Z. Wang, S. Montella, M. Lapata, K.-F.\nWong, and J. Z. Pan, “Rethinking memory in ai: Taxonomy, operations,\ntopics, and future directions,” arXiv preprint arXiv:2505.00675 , 2025.\n[228] K.-T. Tran, D. Dao, M.-D. Nguyen, Q.-V . Pham, B. O’Sullivan, and\nH. D. Nguyen, “Multi-agent collaboration mechanisms: A survey of\nllms,” arXiv preprint arXiv:2501.06322 , 2025.\n[229] K. Tallam, “From autonomous agents to integrated systems, a\nnew paradigm: Orchestrated distributed intelligence,” arXiv preprint\narXiv:2503.13754, 2025.\n[230] Y . Lee, “Critique of artificial reason: Ontology of human and artificial\nintelligence,” Journal of Ecohumanism , vol. 4, no. 3, pp. 397–415,\n2025.\n[231] L. Ale, S. A. King, N. Zhang, and H. Xing, “Enhancing generative\nai reliability via agentic ai in 6g-enabled edge computing,” Nature\nReviews Electrical Engineering , pp. 1–3, 2025.\n[232] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao,\n“Reflexion: Language agents with verbal reinforcement learning,”\nAdvances in Neural Information Processing Systems, vol. 36, pp. 8634–\n8652, 2023.\n[233] F. Kamalov, D. S. Calonge, L. Smail, D. Azizov, D. R. Thadani,\nT. Kwong, and A. Atif, “Evolution of ai in education: Agentic\nworkflows,” arXiv preprint arXiv:2504.20082 , 2025.\n[234] A. Sulc, T. Hellert, R. Kammering, H. Hoschouer, and J. S.\nJohn, “Towards agentic ai on particle accelerators,” arXiv preprint\narXiv:2409.06336, 2024.\n[235] J. Yang, C. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan,\nand O. Press, “Swe-agent: Agent-computer interfaces enable automated\nsoftware engineering,” Advances in Neural Information Processing\nSystems, vol. 37, pp. 50528–50652, 2024.\n[236] S. Barua, “Exploring autonomous agents through the lens of large\nlanguage models: A review,” arXiv preprint arXiv:2404.04442 , 2024.\n[237] A. Zhao, Y . Wu, Y . Yue, T. Wu, Q. Xu, M. Lin, S. Wang, Q. Wu,\nZ. Zheng, and G. Huang, “Absolute zero: Reinforced self-play reason-\ning with zero data,” arXiv preprint arXiv:2505.03335 , 2025.\n36\n            \n            CRITICAL: ALL AGENTS must participate in this analysis. Each agent should:\n            1. Read and understand the paper from your specific role's perspective\n            2. Identify key points relevant to your expertise\n            3. Prepare questions or concerns to discuss\n            4. Consider the implications from your unique viewpoint\n            \n            SPECIALIZED AGENTS: Pay special attention to domain-specific aspects that only you can address.\n            \n            This should be a comprehensive analysis where EVERY agent contributes their specialized perspective.\n            \n            Language: Spanish\n            ",
      "expected_output": "Comprehensive analysis with input from ALL agents including specialists",
      "agent_role": "Coordinator"
    },
    {
      "description": "\n                SPECIALIZED AGENTS DEEP DIVE: This task is specifically for the specialized agents to provide domain expertise.\n                \n                ALL SPECIALIZED AGENTS must contribute detailed analysis:\n                - AI Researcher: Provide technical insights on AI methodology and implications, - AI Philosopher: Discuss philosophical implications of AI research, - AI Doomer: Raise concerns about potential risks and negative consequences, - AI Enthusiast: Highlight positive potential and applications, - AI Newcomer: Ask basic questions that others can answer, - Comedy Communicator: Add appropriate humor and wit to make the discussion more engaging while maintaining respect for the topic\n                \n                Each specialized agent should:\n                1. Provide deep domain-specific insights about the paper\n                2. Identify methodological issues specific to your field\n                3. Highlight implications that only someone with your expertise would notice\n                4. Suggest domain-specific improvements or alternative approaches\n                5. Connect this work to other research in your specialized area\n                \n                This is YOUR moment to shine with specialized knowledge that the base agents cannot provide.\n                Format as a detailed specialist consultation with clear attribution to each expert.\n                \n                Language: Spanish\n                ",
      "expected_output": "Deep specialist analysis from all 6 domain experts",
      "agent_role": "AI Researcher"
    },
    {
      "description": "\n            Based on the initial analysis, conduct a DYNAMIC Q&A session where ALL agents ask each other specific questions.\n            \n            Instructions for multi-agent conversation:\n            1. ALL AGENTS (including specialists) should ask pointed questions to other agents\n            2. SPECIALIZED AGENTS should ask domain-specific questions that challenge assumptions\n            3. BASE AGENTS should ask specialists to clarify complex domain concepts\n            4. Agents must respond to questions directed at them with detailed answers\n            5. Follow-up questions and clarifications are encouraged\n            6. Challenge each other's assumptions respectfully\n            7. Build on each other's ideas and insights\n            8. Create a natural back-and-forth dialogue\n            \n            SPECIALIZED AGENTS: This is crucial - ask questions only YOU would think to ask!\n            \n            Focus areas for questions:\n            - Domain-specific methodological concerns\n            - Interdisciplinary connections and conflicts\n            - Alternative interpretations from different expert perspectives\n            - Practical applications in each specialist's field\n            - Potential limitations or biases from multiple viewpoints\n            \n            Format this as a realistic conversation with clear speaker identification for ALL participants.\n            \n            \n            TONE REQUIREMENTS - HUMOROUS:\n            - Use appropriate humor, wit, and clever analogies to make content engaging\n            - Include light jokes and funny observations that enhance understanding\n            - Use entertaining examples and amusing comparisons\n            - Keep humor respectful and relevant to the topic\n            - Think science communicators like Neil deGrasse Tyson or Bill Nye\n            - Use wordplay, puns, and clever observations when appropriate\n            - Make the audience smile while learning\n            - Avoid offensive humor or jokes that undermine the science\n            \n            \n            Language: Spanish\n            ",
      "expected_output": "Dynamic Q&A conversation with exchanges between ALL agents including specialists",
      "agent_role": "Critical Thinker"
    },
    {
      "description": "\n            Organize a structured debate where ALL agents with different viewpoints engage in deeper discussion.\n            \n            Debate structure:\n            1. Present the main controversial points or interpretations from the paper\n            2. Have ALL AGENTS (including specialists) take different positions and argue their cases\n            3. SPECIALIZED AGENTS: Argue from your domain expertise - what would your field say?\n            4. Allow for rebuttals and counter-arguments between different expert perspectives\n            5. Explore edge cases and hypothetical scenarios from multiple disciplinary angles\n            6. Find areas of agreement and persistent disagreements between different specialties\n            7. Synthesize different viewpoints into a richer understanding\n            \n            This should feel like a real interdisciplinary academic conference where:\n            - Different specialists bring unique perspectives that sometimes conflict\n            - Domain experts interrupt each other (politely) to make field-specific points\n            - Ideas evolve through interaction between different areas of expertise\n            - New insights emerge from cross-disciplinary exchange\n            - There's intellectual tension between different specialist viewpoints\n            \n            SPECIALIZED AGENTS: Don't hold back - defend your field's perspective!\n            \n            Make it conversational and dynamic, not just statements.\n            \n            \n            TONE REQUIREMENTS - HUMOROUS:\n            - Use appropriate humor, wit, and clever analogies to make content engaging\n            - Include light jokes and funny observations that enhance understanding\n            - Use entertaining examples and amusing comparisons\n            - Keep humor respectful and relevant to the topic\n            - Think science communicators like Neil deGrasse Tyson or Bill Nye\n            - Use wordplay, puns, and clever observations when appropriate\n            - Make the audience smile while learning\n            - Avoid offensive humor or jokes that undermine the science\n            \n            \n            Language: Spanish\n            ",
      "expected_output": "Rich interdisciplinary debate with perspectives from ALL specialist agents",
      "agent_role": "Scientific Reviewer"
    },
    {
      "description": "\n                COMEDY INTERVENTION: As the Comedy Communicator, your job is to add levity and entertainment value while maintaining respect for the science.\n                \n                Review all previous discussions and:\n                1. Identify moments where humor can enhance understanding\n                2. Create clever analogies that make complex concepts memorable\n                3. Find amusing but respectful observations about the research\n                4. Develop entertaining examples that illustrate key points\n                5. Add witty commentary that makes the discussion more engaging\n                6. Use wordplay and clever observations appropriate to the topic\n                \n                Your goal is to make the content more accessible and entertaining WITHOUT undermining the scientific rigor.\n                Think Neil deGrasse Tyson explaining astrophysics - serious science, but delivered with wit and charm.\n                \n                \n            TONE REQUIREMENTS - HUMOROUS:\n            - Use appropriate humor, wit, and clever analogies to make content engaging\n            - Include light jokes and funny observations that enhance understanding\n            - Use entertaining examples and amusing comparisons\n            - Keep humor respectful and relevant to the topic\n            - Think science communicators like Neil deGrasse Tyson or Bill Nye\n            - Use wordplay, puns, and clever observations when appropriate\n            - Make the audience smile while learning\n            - Avoid offensive humor or jokes that undermine the science\n            \n                \n                Language: Spanish\n                ",
      "expected_output": "Humorous and entertaining additions that enhance engagement while maintaining scientific accuracy",
      "agent_role": "Comedy Communicator"
    },
    {
      "description": "\n            Conduct a collaborative synthesis where ALL agents work together to build a comprehensive understanding.\n            \n            Collaborative process:\n            1. ALL AGENTS contribute their key insights from the discussions\n            2. SPECIALIZED AGENTS highlight unique perspectives only your field can provide\n            3. Agents build on each other's contributions in real-time\n            4. Identify connections between different specialist perspectives\n            5. Resolve conflicting interpretations through interdisciplinary dialogue\n            6. Co-create new insights that emerge from cross-domain discussion\n            7. Establish consensus on the most important takeaways from ALL perspectives\n            \n            This should be a generative conversation where:\n            - Ideas from one specialist spark new ideas in other specialists\n            - The group intelligence exceeds individual specialist perspectives\n            - Agents actively listen and respond to insights from other domains\n            - The conversation flows naturally between different areas of expertise\n            - New understanding emerges from interdisciplinary interaction\n            - Each specialist's unique knowledge contributes to the whole\n            \n            SPECIALIZED AGENTS: Share insights that ONLY someone with your expertise would have!\n            \n            Format as natural conversation with organic transitions between specialist viewpoints.\n            \n            \n            TONE REQUIREMENTS - HUMOROUS:\n            - Use appropriate humor, wit, and clever analogies to make content engaging\n            - Include light jokes and funny observations that enhance understanding\n            - Use entertaining examples and amusing comparisons\n            - Keep humor respectful and relevant to the topic\n            - Think science communicators like Neil deGrasse Tyson or Bill Nye\n            - Use wordplay, puns, and clever observations when appropriate\n            - Make the audience smile while learning\n            - Avoid offensive humor or jokes that undermine the science\n            \n            \n            Language: Spanish\n            ",
      "expected_output": "Collaborative synthesis conversation with emergent insights from ALL specialist perspectives",
      "agent_role": "Coordinator"
    },
    {
      "description": "\n            Based on all previous conversations and analyses, conduct a final comprehensive discussion that synthesizes insights from ALL agents.\n            \n            The discussion should:\n            1. Synthesize insights from the Q&A, specialist deep dive, debate, and collaborative sessions\n            2. Cover all major points of the paper from multiple expert perspectives\n            3. Include the rich specialist perspectives developed through agent interactions\n            4. Address concerns and criticisms that emerged from different domains\n            5. Explore implications and applications discussed by various specialists\n            6. Be engaging and conversational for a podcast audience\n            7. Highlight unique insights that could ONLY come from having multiple specialist perspectives\n            \n            CRITICAL: This final discussion must incorporate:\n            - Domain-specific insights from ALL specialist agents\n            - Cross-disciplinary connections discovered during discussions\n            - Unique perspectives that emerged from interdisciplinary dialogue\n            - Humor and entertainment elements (if humor agents participated)\n            \n            Build on the conversational richness already established across ALL agent perspectives.\n            Create a comprehensive dialogue that showcases the depth of multi-expert analysis.\n            \n            Language: Spanish\n            ",
      "expected_output": "Comprehensive discussion transcript incorporating ALL agent perspectives and specialist insights",
      "agent_role": "Critical Thinker"
    },
    {
      "description": "\n            Transform the rich multi-specialist discussions into a comprehensive educational lecture text.\n            \n            The script should be in the style of popular science educators like 3Blue1Brown:\n            1. Written as a SINGLE EDUCATOR speaking directly to the listener (use \"tú\"/\"usted\")\n            2. Use analogies and accessible explanations\n            3. Include ALL key insights from the multiple conversations and specialist exchanges\n            4. Be engaging and educational, not just informative\n            5. Flow naturally from concept to concept with smooth transitions\n            6. Include moments of wonder and intellectual curiosity\n            7. Break down complex ideas into digestible parts\n            8. Use a teaching tone that makes the listener feel they're learning something fascinating\n            9. Write as continuous text ready to be read by a voice actor\n            10. NO section headers, NO subheaders, NO formatting marks\n            11. Don't address the public with greetings or goodbyes, but make questions\n            12. Always end up with questions for the reader and practical implications\n            13. Write as plain text that flows naturally for voice reading\n            14. NO [PAUSES], NO [MUSIC], NO stage directions - just the educational content\n            15. CRITICAL: Address the listener directly - \"puedes imaginar\", \"si consideras\", \"te darás cuenta\"\n            16. DO NOT write as if summarizing a discussion - write as if YOU are the teacher\n            17. Avoid phrases like \"los expertos discutieron\" or \"el equipo concluyó\"\n            18. Incorporate the depth and nuance that emerged from ALL agent conversations\n            \n            CRITICAL - MULTI-SPECIALIST INTEGRATION:\n            19. Weave in insights that could ONLY come from having multiple specialist perspectives\n            20. Include cross-disciplinary connections discovered during discussions\n            21. Incorporate domain-specific knowledge from ALL participating specialists\n            22. Show how different expert viewpoints enhance understanding of the topic\n            23. If humor agents participated, naturally integrate entertaining elements\n            24. Demonstrate the value of interdisciplinary analysis throughout\n            \n            \n            SIMPLE LEVEL REQUIREMENTS - FOR NON-TECHNICAL AUDIENCE:\n            15. AVOID ALL TECHNICAL JARGON - use only everyday language\n            16. Use SIMPLE analogies from daily life (like cooking, sports, relationships)\n            17. Focus ONLY on \"what this means for YOU\" - direct personal relevance\n            18. Explain concepts as if talking to a curious friend with no scientific background\n            19. NO scientific terms without VERY simple explanations (e.g., \"neurons - the tiny messengers in your brain\")\n            20. Use SHORT sentences and familiar words only\n            21. Focus on PRACTICAL takeaways: \"What can you do with this information?\"\n            22. Tell STORIES and use EXAMPLES from everyday situations\n            23. Ask rhetorical questions that connect to personal experience: \"¿Alguna vez has notado que...?\"\n            24. Make it feel like a friendly conversation, not a lecture\n            25. Focus on the BIG PICTURE and skip complex details entirely\n            26. Use comparisons to things everyone knows: \"like your smartphone battery\", \"like driving a car\"\n            27. This should sound like explaining to a family member who's genuinely curious but has no technical background\n            \n            \n            \n        DURATION REQUIREMENT: EXACTLY 3 minutes of content (420-480 words) - THIS IS MANDATORY\n        \n        DEPTH GUIDANCE FOR 3 MINUTES:\n        \n            - Focus on 1-2 main concepts only\n            - Keep explanations concise but complete\n            - Include one compelling example per main point\n            - Go straight to the point without much additional context\n            \n        \n        TECHNICAL CALCULATION:\n        - Target reading speed: ~150 words per minute\n        - Word range: 420-480 words\n        - If content is too short, EXPAND significantly with more detail and depth\n        - If too long, maintain quality but adjust information density\n        \n            \n            \n            LANGUAGE REQUIREMENTS FOR SPANISH:\n            \n            CRITICAL: AVOID ANGLICISMS whenever possible and use proper Spanish terms:\n            - Instead of \"link\" use \"enlace\" or \"vínculo\"\n            - Instead of \"feedback\" use \"retroalimentación\" or \"respuesta\"\n            - Insted of \"puzzle\" use \"rompecabezas\" or \"problema\"\n            - Instead of \"performance\" use \"rendimiento\" or \"desempeño\"\n            - Instead of \"input/output\" use \"entrada/salida\"\n            - Instead of \"update\" use \"actualizar\" or \"poner al día\"\n            \n            EXCEPTIONS - You CAN use anglicisms for:\n            1. Very new technical terms with no established translation (e.g., \"blockchain\", \"ChatGPT\")\n            2. Proper names of tools/companies (e.g., \"TensorFlow\", \"GitHub\", \"OpenAI\")\n            3. Widely adopted terms in scientific literature (e.g., \"machine learning\" vs \"aprendizaje automático\")\n            4. When the Spanish term is more confusing than helpful\n            \n            GENERAL RULES:\n            - Always prioritize natural Spanish expressions\n            - Use Spanish sentence structures and idioms\n            - Make it sound like a native Spanish speaker wrote it\n            - When you must use an anglicism, briefly explain it if needed\n            \n            \n            \n            TONE REQUIREMENTS - HUMOROUS:\n            - Use appropriate humor, wit, and clever analogies to make content engaging\n            - Include light jokes and funny observations that enhance understanding\n            - Use entertaining examples and amusing comparisons\n            - Keep humor respectful and relevant to the topic\n            - Think science communicators like Neil deGrasse Tyson or Bill Nye\n            - Use wordplay, puns, and clever observations when appropriate\n            - Make the audience smile while learning\n            - Avoid offensive humor or jokes that undermine the science\n            \n            \n            Language: Spanish\n            ",
      "expected_output": "Comprehensive educational text incorporating insights from ALL specialist agents",
      "agent_role": "Educational Writer"
    },
    {
      "description": "\n            FINAL STEP: Transform the educational content into a PERFECT voice-ready script.\n            \n            CRITICAL: Verify the content meets the 3-minute target (420-480 words). If it's too short, EXPAND it significantly.\n            CRITICAL: Ensure technical level is simple - keep extremely simple and conversational.\n            \n            MANDATORY VOICE OPTIMIZATION REQUIREMENTS:\n            1. Create a SINGLE, CONTINUOUS text ready for a voice actor to read\n            2. Markdown formatting, but NO headers, NO bullet points, NO lists\n            3. Convert ALL content into natural, flowing sentences\n            4. Replace any remaining bullet points with complete sentences\n            5. Ensure PERFECT flow from sentence to sentence\n            6. Remove formatting marks: #, -, •, etc for titles and subtitles, but keep for bold and italic text\n            7. Make sure sentences are not too long or complex for voice delivery\n            8. Write naturally in Spanish without academic formalities\n            9. Remove any remaining conversational artifacts (\"como mencionamos antes\", \"en nuestra discusión\")\n            10. Ensure seamless transitions between concepts\n            11. Maintain the conversational richness but in a single educator voice\n            12. Read the text mentally to ensure it sounds natural when spoken\n            13. Ensure proper pronunciation flow for difficult technical terms\n            14. Remove any repetitive content that may have emerged from multiple discussions\n            15. Maintain the depth gained from agent conversations while ensuring clarity\n            16. Perfect pacing for natural speech rhythm\n            17. Eliminate any phrases that sound like committee work or group consensus\n            18. Make it sound like ONE expert who has deeply understood the topic\n            19. Ensure technical accuracy while maintaining conversational flow\n            20. Optimize for voice actor performance and listener engagement\n            21. This should sound like ONE VOICE teaching, not a summary of multiple voices\n            22. Avoid words that could make this sound like written by an LLM, like not often used words: \"fascinante\", \"delve\",\n            \"revelador\".\n            23. Introduction should be a catchy hook that makes the listener want to listen to the entire video, something like a question or a statement that makes the listener want to know more.\n\n            \n            LANGUAGE REQUIREMENTS FOR SPANISH:\n            \n            CRITICAL: AVOID ANGLICISMS whenever possible and use proper Spanish terms:\n            - Instead of \"link\" use \"enlace\" or \"vínculo\"\n            - Instead of \"feedback\" use \"retroalimentación\" or \"respuesta\"\n            - Insted of \"puzzle\" use \"rompecabezas\" or \"problema\"\n            - Instead of \"performance\" use \"rendimiento\" or \"desempeño\"\n            - Instead of \"input/output\" use \"entrada/salida\"\n            - Instead of \"update\" use \"actualizar\" or \"poner al día\"\n            \n            EXCEPTIONS - You CAN use anglicisms for:\n            1. Very new technical terms with no established translation (e.g., \"blockchain\", \"ChatGPT\")\n            2. Proper names of tools/companies (e.g., \"TensorFlow\", \"GitHub\", \"OpenAI\")\n            3. Widely adopted terms in scientific literature (e.g., \"machine learning\" vs \"aprendizaje automático\")\n            4. When the Spanish term is more confusing than helpful\n            \n            GENERAL RULES:\n            - Always prioritize natural Spanish expressions\n            - Use Spanish sentence structures and idioms\n            - Make it sound like a native Spanish speaker wrote it\n            - When you must use an anglicism, briefly explain it if needed\n            \n            \n            CRITICAL: This is the FINAL version that will be published. Make it PERFECT.\n            \n            Language: Spanish\n            ",
      "expected_output": "FINAL publication-ready voice script (420-480 words)",
      "agent_role": "Voice Director"
    }
  ]
}