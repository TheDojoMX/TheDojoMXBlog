{
  "project_name": "technical",
  "paper_title": "Test Time",
  "language": "Spanish",
  "agents": [
    {
      "role": "Coordinator",
      "goal": "Coordinate the discussion and ensure all perspectives are heard",
      "backstory": "You are an experienced moderator who ensures productive discussions"
    },
    {
      "role": "Scientific Reviewer",
      "goal": "Verify the soundness and methodology of the paper",
      "backstory": "You are a rigorous scientist who evaluates research methodology and conclusions"
    },
    {
      "role": "Critical Thinker",
      "goal": "Question assumptions and challenge ideas presented",
      "backstory": "You are a skeptical academic who questions everything and looks for flaws"
    },
    {
      "role": "Educational Writer",
      "goal": "Create engaging educational content in the style of popular science educators",
      "backstory": "You are a skilled science communicator who explains complex topics in an accessible, engaging way like 3Blue1Brown or other popular educators"
    },
    {
      "role": "Voice Director",
      "goal": "Transform content into perfect voice-ready script for publication",
      "backstory": "You are a master voice coach and script editor who specializes in creating flawless, publication-ready scripts that voice actors can read naturally. You ensure every word flows perfectly when spoken aloud."
    },
    {
      "role": "AI Researcher",
      "goal": "Provide technical insights on AI methodology and implications",
      "backstory": "You are an AI researcher with deep technical knowledge"
    },
    {
      "role": "AI Philosopher",
      "goal": "Discuss philosophical implications of AI research",
      "backstory": "You are a philosopher specializing in AI ethics and implications"
    },
    {
      "role": "AI Doomer",
      "goal": "Raise concerns about potential risks and negative consequences",
      "backstory": "You are concerned about AI safety and potential existential risks"
    },
    {
      "role": "AI Enthusiast",
      "goal": "Highlight positive potential and applications",
      "backstory": "You are optimistic about AI's potential to solve problems"
    },
    {
      "role": "AI Newcomer",
      "goal": "Ask basic questions that others can answer",
      "backstory": "You know little about AI but are curious and ask good questions"
    },
    {
      "role": "Comedy Communicator",
      "goal": "Add appropriate humor and wit to make the discussion more engaging while maintaining respect for the topic",
      "backstory": "You are a science comedian and communicator who knows how to make complex topics entertaining without undermining their importance. You use analogies, witty observations, and light humor to keep audiences engaged. Think Neil deGrasse Tyson meets stand-up comedy - intelligent, respectful, but definitely fun."
    }
  ],
  "tasks": [
    {
      "description": "\n            Analyze the paper titled \"Test Time\" and provide your perspective.\n            \n            Paper content:\n            arXiv:2506.08388v2 [cs. LG] 22 Jun 2025\nEdoardo Cetin, Tianyu Zhao, Yujin Tang\nSakana AI, Japan\nedo, tianyu, yujintang @sakana. ai\nTraining reasoning language models (LMs) with reinforcement learning (RL) for\none-hot correctness inherently relies on the LM being able to explore and solve its\ntask with some chance at initialization. Furthermore, a key use case of reasoning\nLMs is to act as teachers for distilling new students and cold-starting future RL\niterations rather than being deployed themselves. From these considerations, we\nnew class of Reinforcement-Learned Teachers (RLTs) focused on yielding the most\neffective downstream distillation. RLTs are prompted with both the question and\nsolution to each problem, and tasked to simply connect-the-dots with detailed\nexplanations tailored for their students. We train RLTs with dense rewards obtained\nproblem s solution. In practice, the raw outputs of a 7B RLT provide higher final\nperformance on competition and graduate-level tasks than existing distillation and\ncold-starting pipelines that collect and postprocess the reasoning traces of orders\nof magnitude larger LMs. Furthermore, RLTs maintain their effectiveness when\ntraining larger students and when applied zero-shot to out-of-distribution tasks,\nunlocking new levels of efficiency and re-usability for the RL reasoning framework.\ngithubgithub. com SakanaAI RLT\nFigure: RLTs provide better student distillation and RL\ncold-starts than orders of magnitude larger LMs across com-\npetition and graduate-level tasks (AIME, MATH, GPQA).\nExploration is one of the critical chal-\nlenges in reinforcement learning (RL)\nand has been a core focus of its lit-\nerature . Sparse rewards can-\nthe agent is already capable of solv-\ning the given task at initialization.\nWith the rise of RL for open-ended\nreasoning (RL reasoning) inducing a\nnew form of language model (LM)\nscaling beyond prompt-engineering\nand search , exploration has re-\nemerged as a key challenge. A canon-\nical motivation for RL is the poten-\nlearn entirely new tasks from scratch. However, the nature of one-hot correctness rewards used in the\nRL reasoning framework fails to provide a dense form of guidance, focusing instead on reinforcing\ncorrect responses in the initial model s pool of pass-at-k attempts without true extrapolation beyond\nPreprint. Under review.\nthe LM s initial latent abilities . As a result, mostly large, already-capable models have been\nshown to improve consistently beyond cheaper and simpler supervised optimization .\nDue to this fundamental limitation, coupled with RL s training instability, distillation has emerged as\nanother ubiquitous component of current reasoning paradigms. In this case, the test-time role of LMs\nnew problems. This teacher-student paradigm is widely adopted both to train smaller, less-capable\nmodels and even to cold-start future RL iterations for better final convergence with the teacher s\nown initial checkpoint acting as the student . However, the problem-solving skills reinforced\nby correctness-based rewards have been shown not to be entirely aligned with the goal of downstream\ndistillation . To account for this mismatch, current pipelines significantly rely on heuristic-driven\npostprocessing of the teacher s outputs for effective student transfer .\nBased on these considerations, we propose a framework that avoids RL s exploration challenge with\na new class of specialized Reinforcement-Learned Teachers (RLTs)trained specifically to yield\neffective downstream distillation. Our main intuition is simple: the ability of real-world teachers is\nnot measured by whether they can come up on their own with complex theorems, proofs, or answers\nfrom scratch. Instead, what matters is their ability to make use of readily available solutions and\ndevise instructive explanations for their students. Thus, we depart from the traditional RL reasoning\nframework, tasking a model to first think and then come up with a new solution for the first time.\nInstead, RLTs are tasked with the easier problem of providing an effective explanation with the\nproblem s solution already given within their prompt. We train RLTs with dense rewards using the\nstudent s log probabilities to assess its understanding of each problem s ground-truth solution from\nthe teacher s explanations, and the interpretability of the logical leaps in the explanations themselves.\nBy distilling students from the raw outputs of a lightweight RLT with 7B parameters, we demonstrate\nof magnitude more parameters (Figure). We show our framework provides superior benefits even\nwhen distilling the RLT s explanation to train larger 32B students and to cold-start traditional RL\noptimization. Furthermore, we showcase how RLTs can be transferred zero-shot to new domains\naccess to the task s reward. Overall, these results highlight the potential of our new method for\novercoming the large costs of RL by focusing on stronger, smaller, and highly reusable specialized\nteachers, while removing the current reliance on expensive and heuristic-driven distillation pipelines.\nWe share our code and pretrained checkpoints 1 to facilitate future research in RL reasoning and\ndistillation. In summary, our main contributions are threefold:\nWe introduce the RLT framework, tackling exploration with a simpler dense-reward that\naligns the objective of RL training to providing effective downstream student distillation.\nWe show how distilling the raw outputs of a 7B RLT directly outperforms training students\nwith carefully postprocessed reasoning traces from orders of magnitude larger LMs.\nWe demonstrate that RLTs also allow for better cold-starts for traditional RL, effective\ndistillation to larger students, and even zero-shot transfer to new reasoning domains.\n2.1 Reinforcement learning\nThe RL post-training recipe for inducing reasoning behavior was recently popularized by the\nDeepSeek R1 line of work . By fine-tuning on a dataset of questions D = q1,..., qN\nwith verifiable solutions s1,..., sN, Guo et al. show effective reasoning behavior emerges\nout of a 671B-parameter LM , significantly pushing its performance on challenging math and\ncoding tasks. Their training is conducted with GRPO , an online RL algorithm that foregoes\nthe use of a critic model with a simple Monte-Carlo value estimate. GRPO prompts the LM πθ to\nproduce a set of G 1 grouped outputs o1,... oG for each sampled question q D, optimizing:\nJ(θ) =Eq D, o G\n(Ai β DKL(πθ πref))\n1https: github. com SakanaAI RLT\nFigure: Left: RL format asking an LM to think and solve hard problems from scratch. Right: RLT\nformat asking an LM to produce instructive step-by-step explanations given access to the solutions.\nHere, the advantages Ai are obtained by normalizing each output s reward ri within each group:\nAi = ri mean( r1,..., rG )\nstd( r1,..., rG ). (2)\noutput oi into two separate formatted sections separated by think and solution tags, denoted\ntoi and soi. This structure is forced by assigning rewards ri = 1 to unformatted completions,\nri = 0.5 to wrong but formatted completions, andri = 1only to correct and formatted completions.\nTraining with this strategy, Guo et al. show the LM s completion length gradually grows with\nreflection, verification, and self-correction steps emerging, mirroring human chain-of-thoughts.\n2.2 Supervised distillation\nRL s shortcomings. For any online RL objective like Equation 1 to avoid collapse, the model must\nalready possess a non-trivial chance of producing correct responses with non-zero gradients at\ninitialization. This defining property makes the RL objective much less applicable than cross-entropy\nobjectives that always include the correct response s information in the model s gradients. As a\nconsequence of this dichotomy, distilling the reasoning traces of large RL-trained LMs with supervised\nRL itself for inducing reasoning in smaller, less-capable models . Furthermore, RL\nappears prone to instabilities and output degradation, especially during extended training sessions.\nDue to this second limitation, DeepSeek R1 and several other models perform RL training\nover multiple iterations. This is done by using the RL-trained models at the end of each intermediate\niteration only to, once again, collect distillation datasets used for cold-starting their original initial\ncheckpoint and obtain a stronger initialization point for the next RL iteration.\nConstructing a dataset of distillation prompts DSD = d1,..., dN involves using the RL-trained\nLM πθ with its reasoning system prompt to answer a corpus of verifiable questions, which can be\nchosen with several heuristics . The LM s output reasoning traces for each question\no πθ( q) are then filtered by comparing them with the ground-truth solutions to ensure their\ncorrectness. Commonly, these reasoning traces are also post-processed via additional manual steps\nof refinements, such as asking other closed-sourced LMs to remove grammatical issues and refactor\nthe reasoning steps into a nicer, consistent format. In fact, Li et al. even argues that the structure\nand learn how to reason from distillation, potentially even more important than correctness itself.\n3.1 The implications of training teacher models as students\nmodern reasoning framework. As detailed in Section, after RL training, LMs πθ are often not\ndeployed themselves but rather used to obtain reasoning distillation datasets for fine-tuning weaker\nmodels and cold-starting future RL iterations. Thus, these models can be effectively seen as teachers,\nproviding explanations for future student models πs to learn from.\nThis teacher-student paradigm highlights a potential mismatch between the objective used for RL\ntraining and the teacher s test-time role. In traditional settings, teachers are trained with sparse\ncorrectness rewards to improve their ability to solve hard problems from scratch. This objective not\nonly precludes the applicability of RL training for tasks beyond the base model s original capabilities,\ndue to its inherent exploration challenge, but is also not aligned with the teacher s actual end goal:\nproducing reasoning traces from which students πs can learn the necessary skills to derive correct\nsolutions themselves. Based on these considerations, we propose a different training framework for\nthis objective mismatch. Our framework comprises a much easier task formulation, a dense reward\nobjective, and a carefully designed training recipe, allowing us to learn a new class of specialized\nReinforcement Learned Teachers (RLTs).\n3.2 Aligning the task of teacher models\nFigure: The tokens from the RLT s explana-\ntions are copied into the student format to mea-\nsure its understanding with our reward terms.\nIn the traditional RL paradigm, the solution si to\nmodel and is only employed for checking the cor-\nLM s completions soi. Precluding direct access or\nthe test-time objective of solving entirely new test\nproblems from scratch, but is precisely what makes\nexploration challenging, as the model receives no\ngradients until its first successful attempt. Our key\nobservation, however, is that the test-time teach-\ndatasets DSD for questions with known solutions,\naccess to such solutions as is the case for real-\nworld teachers, who can rely on access to readily\navailable solutions and, thus, focus entirely on how\ninstructive their explanations are for students.\nTo this end, as illustrated in Figure, RLTs are\nprompted with a new formatting style, providing\ninputs, and are tasked to produce instructive step-\nby-step explanations, connecting the dots between\nthe two. We design our prompts to allow direct\nreuse of the teacher s outputs for student distilla-\ntion while keeping the task natural, appending the\ncompletion. At test-time, constructing the corre-\nsponding question completions for the student dis-\nextracting the think tokens from the teacher s out-\n3.3 Evaluating the quality of explanations\nstudent πs to recover correct solutionssi and are also logical continuations from questions alone under\nthe student s perspective. In particular, following the procedure from the previous subsection, for\neach completion oi from the teacher πθ, we extract the think tokens toi and format the corresponding\nstudent distillation prompt di by prepending the question qi and appending the ground-truth solution\nsi. As illustrated in Figure, each distillation prompt is then fed as input to the student model to\nobtain a set of per-token log probabilities, which are processed into our two reward terms as follows:\ni: quantifying the student πs understanding of the solutions si given the question qi\nand think tokens toi in context. This first reward term is computed with the student s log\nprobabilities over the solution tokens, reduced with both average and minimum operations:\nrSS(oi, si, qi) = avg log πsi\ns + α min log πsi\ns, where πsi\ns = πs(si toi. qi). (3)\ni: quantifying whether the think tokens toi themselves are interpretable logical continu-\nations from the student s perspective as compared with the teacher s. This second reward\ndistribution (under the RLT s format with bothqi and si in context) and the student s (with\nonly the question qi in context), reduced with both average and maximum operations:\nrKL(oi, si, qi) = avg\ns = πs(toi qi), π\nθ = πθ(toi si, qi).\nFinally, the RLT rewards are obtained by combining these two terms with a weighting coefficientλ:\ni = rSS(oi, si, qi) λrKL(oi, si, qi) (5)\nEach term in our reward function serves a precise purpose. First, optimizing rSS will produce\nsolution si. However, this term alone does not differentiate between explanations that guide the\nstudent step-by-step and those that increase the solution s likelihood without a logical path that can be\nlearned from. An extreme instance of the latter would be an explanation simply repeating the solution\ntokens to increase likelihood, failing to provide general examples of reasoning methods that can be\napplied when approaching new problems. Thus, introducing rKL fills precisely this gap, aligning\nqi and the previous think tokens in context. Intuitively, introducing this term regularizes for each\ngiven only its prior understanding and the question itself. Additionally, combining the average with\nmin max reductions ensures the rewards do not forego any individual token, regardless of the solution\nlength or the number of think tokens in the teacher s explanations. For instance, their omission could\nreduce the influence on rKL of hard but necessary individual logical steps. For further discussion, we\nrefer to Appendix, where we empirically analyze and validate all these design choices.\n3.4 Pulling everything together: the RLT training paradigm\nThe RLT framework can be used with any RL algorithm (e. g., ) with minimal modifications\nto the LM s conditioning and reward, as described in the above subsections. In this work, we employ\nthe simple GRPO recipe detailed in Section, resulting in the following training objective:\nJRLT(θ) =Eq, s D, o G\ni β DKL(πθ πref)\ni is computed with the normalization strategy defined in Equation 2 using the RLT reward\nfunction from Equation 5. Unlike for correctness-based rewards, our learning signal is inherently\ndense, providing informative rankings to the RLT s output even before achieving any task expertise.\nThis fundamental difference greatly facilitates our optimization, akin to how heuristically shaped\nrewards enabled RL agents to learn entirely new behaviors for videogames and robotics tasks .\n4.1 Training, distillation, and evaluation\nWe train RLTs on the set of questions and solutions selected by Li et al. based on their level\nof challenge. This dataset comprises less than 17K math and coding problems originally used for\nTable: RLTs and prior distillation pipelines across model (7B and 32B) and data size (1K and 17K).\nModel Data size AIME 2024 MATH 500 GPQA Diamond Overall\nQwQ-32B N. A. 50.00 90.60 54.50 65.03\nDeepSeek-R1 800K+ 79.80 97.30 71.50 82.87\nQwen2.5-7B-Instruct N. A. 10.00 74.20 33.30 39.17\nBespoke-7B-1K 1K 13.30 80.00 33.80 42.37\nRLT-7B-1K (Ours) 1K 20.00 80.40 41.90 47.43\nBespoke-7B 17K 20.00 82.00 37.80 46.60\nRLT-7B (Ours) 17K 23.30 82.80 42.40 49.50\nQwen2.5-32B-Instruct N. A. 26.70 84.00 49.00 53.23\ns1-32B + budget forcing 1K 56.70 93.00 59.60 69.77\nBespoke-32B-1K 1K 46.70 92.60 57.50 65.60\nRLT-32B-1K (Ours) 1K 60.00 94.00 60.10 71.37\nSky-T1-32B 17K 43.30 82.40 56.80 60.83\nBespoke-32B 17K 63.30 93.00 58.10 71.47\nRLT-32B (Ours) 17K 66.70 93.40 59.60 73.23\ndistilling filtered and post-processed reasoning traces collected from QwQ and DeepSeek R1 .\nIn contrast, the RLTs we consider are orders-of-magnitude smaller models, all trained starting from\nthe Qwen2.5-7B-Instruct LM . We precede our RL phase with a short supervised fine-tuning\ndataset released by Labs . During RL, we compute the reward for the RLT explanations using\nanother small Qwen-7B model as the student. We train our main models for 125 steps, less than a\nsingle epoch, with a batch size of 1024, a constant learning rate of 1 10 6, and a group size of 64.\nWe note that we were also able to train RLTs with a smaller batch size of 256 and more steps for\nfaster preliminary experimentation with only slightly inferior results.\nWe collect our distillation dataset with the learned RLTs using the same full set of 17K question-\nsolution pairs from training. With the new reasoning traces, we then proceed to fine-tune our students\neither on this full data or a randomly sampled 1K subset, equating the distillation budget and following\nthe same recipes as our baselines . Unlike previous RL distillation pipelines, we do not apply\nextra postprocessing refinements to improve the quality of the RLT s reasoning traces, directly using\nour model s raw outputs for student fine-tuning. We refer to Appendices A and B for further details\nregarding our training and distillation phases with complete lists of hyperparameters.\nFollowing prior work , our main evaluation considers three popular and challenging tasks\nfrom the literature: AIME24 , the set of problems used for the American Invitational Mathematics\nExamination. MATH 500 , the set of problems selected by from the canonical competition\nmath benchmark. GPQA Diamond , the set of diamond difficulty problems on natural science\ntopics from the Graduate-level Google-proof Q&A benchmark. We report the completion accuracy\nof each of our students using Lighteval . When available, we use baseline results reported in\nprior work, which we found close to our early reproduction attempts. In Appendix, we extend the\nexperiments in this section by evaluating our models on additional tasks and settings.\n4.2 Test-time reasoning across teachers and students\ntraces beyond traditional distillation pipelines. As described in Section, to construct the student\ndistillation dataset, we use the same starting question-solution pairs as our recent state-of-the-art\nbaselines , with each sample only differing in terms of its reasoning trace. While RLTs could\nbe inexpensively applied to provide explanations of larger corpora, this consistency serves to remove\npotential confounding factors, other than the quality of the reasoning traces, biasing our experiments\nand comparisons. For the same reason, we do not retune any hyperparameters for the distillation\nphase, training students following the same procedure as our baselines based on data size .\nTable: RLTs and prior distillation pipelines for cold-starting traditional RL.\nModel Data size AIME 2024 MATH 500 GPQA Diamond Overall\nQwen2.5-7B-Instruct N. A. 10.00 74.20 33.30 39.17\nBespoke-7B 17K 20.00 82.00 37.80 46.60\nRLT-7B (Ours) 17K 23.30 82.80 42.40 49.50\nRL no cold-start N. A. 13.30 74.20 34.80 40.77\nRL cold-start (raw) + RL 17K 10.00 71.00 34.80 38.60\nRL cold-start (GPT) + RL 17K 16.70 78.20 36.90 43.93\nBespoke-7B + RL 17K 16.70 82.80 45.40 48.30\nRLT-7B + RL (Ours) 17K 26.70 84.00 40.90 50.53\nWe compare the RLTs explanations with prior approaches, evaluating students fine-tuned on both\nour full 17K distillation samples and its 1K subset. Our recent baselines all follow a similar recipe\nor API calls and postprocessing them with closed-source LMs: s1 using traces from Gemini\nFlash Thinking , Sky-T1 using traces from QwQ , and Bespoke using traces from\nDeepSeek R1 . Since the Bespoke baseline obtained state-of-the-art results with our same question-\nsolution corpus, we extend its evaluation with new results distilling its processed R1 traces only for\nour same 1K questions subset, equating its number of datapoints with the other s1 baseline.\nAs shown in Table, the raw output explanations of our small 7B parameter RLT outperform all the\nconsidered data-distillation pipelines involving teachers with orders of magnitude more parameters\nand additional ad-hoc postprocessing steps. We also find the effectiveness of the RLT traces stays\nconsistent across different data sizes, in contrast to the R1 traces from the Bespoke pipeline that\nappear significantly less effective when subsampled. Furthermore, even when distilling a Qwen-32B\nstudent, much larger than our 7B teacher, our RLT still outperforms all prior methods for both data\nsizes with considerable margins. We believe this result, in particular, shows how our framework could\nallow overcoming the current prohibitive costs of RL reasoning: shifting the burden of expensive RL\nprocedures to small teachers, unable to effectively solve problems from scratch but highly specialized\nin the simpler task of producing effective explanations for large, more powerful students.\n4.3 RLTs to cold-start RL\nOur next set of experiments focuses on evaluating the effectiveness of RLTs in providing cold-start\ndata for traditional RL. For this new RL phase, we use our same GRPO implementation with the\nstandard student format and correctness-based rewards described in Section As compared to the\nRLT framework, we find that using a larger batch size of 1024 is significantly more beneficial to\nbetter cope with the increased variance and reward sparsity of traditional RL. We train for a full epoch\non the recent RL dataset from Li et al. collected by analyzing and selecting an effective subset of\nimprovement.\nWe compare performing this new RL phase on the Qwen-7B model cold-started from the reasoning\ntraces of our 7B RLT and the postprocessed R1 traces from the Bespoke pipeline, our strongest\ndistillation baseline. Moreover, we also compare a 7B parameters baseline teacher trained with\ntraditional RL as done in prior work : effectively performing RL twice on the Qwen model\nand collecting a dataset at the end of the first iteration to cold-start the second. To construct the\ncold-starting dataset for this last baseline, we consider either taking the model s raw output traces, as\ndone with RLTs, or postprocessing them with additional refinements using GPT4.1-mini and\nfollowing a very similar strategy to the other R1 and QwQ traditional distillation pipelines .\nAs shown in Table, the reasoning traces from our 7B parameter RLT again display superior cold-\nstarting effectiveness compared to all of our baselines. The performance gap is exceedingly noticeable\nwith the cold-starting approaches that are also using a 7B teacher trained with traditional RL. In fact,\nonly after improving the format and structure of the traces from these RL-trained teachers with GPT\npostprocessing, we were able to observe any improvements from the original Qwen-7B results. While\nour RLT was itself trained from the same 7B model, it again demonstrates superior cold-starting\neven when compared to postprocessed R1 pipelines. Overall, we find these results to be compelling\nFigure: Left: Out-of-distribution performance transferring RLTs to produce new distillation data\nas compared to students trained on the Li et al. corpus and direct RL on the countdown task.\nRight: Performance after training on different distillation datasets ranked by the RLT reward.\nreasoning framework beyond the current reliance on prohibitively large and closed-source LMs.\n4.4 Out-of-domain zero-shot transfer\nUnlike problem-solving from scratch, we posit that providing effective explanations to given solutions\nis a much less task-specific skill. Thus, in this subsection, we evaluate how well RLTs can be applied\nto construct datasets and distill new specialized students in out-of-distribution domains, without any\nexpensive RL retraining. In particular, we focus on the canonical countdown task , asking student\nmodels to combine a set of numbers to equal a given target using basic arithmetic operations. We\ntrain and test our models on distinct datasets of 16K and 1K automatically-generated question and\nsolution pairs. We compare zero-shot transferring our RLT with transferring the RLT-7B student and\nthe Bespoke-7B baselines from the previous subsections. To ground our results, we also consider\nperforming RL on the countdown task itself (CD RL), training both from the Qwen-7B model and\nthe cold-started Bespoke-7B baseline with the same setup described in Section\nAs shown in the left bar plot of Figure, applying RLT distillation zero-shot remarkably achieves\neven higher performance than direct RL on the countdown task. Interestingly, direct RL appears to\nquestions that do not include any examples of countdown problems (50.8 vs. 49.2). Furthermore, we\nfind there is stark overlap of over 98.5% in the final sets of solved problems between direct RL and\nthe RL-free Bespoke-7B baseline. We find these results in line with prior analysis , providing\ncome from steering the base model s distribution toward long-context generation. In contrast, by\nsimplifying the task and foregoing sparse rewards, our RLT appears much more effective providing\ncountdown-specific traces that allow students to learn new knowledge and solve unseen questions,\nyielding higher improvements than direct RL even without any teacher training in this new domain.\n4.5 Explanation reward analysis\nTo analyze the design of the RLT reward function, we start by examining the relationship between\nthe traces rewards and the effectiveness of student distillation. In particular, we use our RLT s\ncheckpoint right before RL training to generate 16 completions for each question-solution pair in\nour data. We then score all completions with our reward and divide them into groups based on their\nrelative rank for each prompt. Thus, we obtain 16 datasets with different reasoning traces for each\nquestion, which we use to train 16 new 7B students from Qwen. As illustrated in the right bar plot\nof Figure, ordering student performance by the respective dataset rank shows a clear correlation\nbetween the two, with a Pearson coefficient over 0.89, validating the efficacy of the RLT objective.\nAdditionally, the highest ranked traces of our 7B teacher before any RL remarkably already yield 90%\nFigure: Examples comparing the contents from the post-processed R1 traces that were\nthe performance gains of our baseline R1 distillation pipeline , showing how even small models\nalready possess latent teaching skills unlocked by our new reward and simplified task formulation.\nexplanations is particularly improved from the baseline R1 distillation pipeline. As shown in Figure,\nwe find that the R1 traces with low rewards often try to rely on external tools, such as calculators,\nand employ language patterns likely idiosyncratic to the training data of the DeepSeek-V3 LM, as\nsentences with brief humorous comments . Instead, the corresponding RLT explanations appear\nmuch more grounded and even manage to add new verification steps not considered by R1 to check\nthe final solution. In Appendix, we provide additional examples showcasing further qualitative\ndifferences of our framework with R1 traces and also specific failure cases from training RLTs without\nproper balance between each reward component, such as repetitions and overly-long explanations.\nInspired by the unprecedented abilities of the OpenAI o1 model , there has been a resurgence of\nRL approaches aimed at inducing a new kind of open-ended reasoning to scale test-time compute.\nThe work from Guo et al. was another milestone in this new domain, providing a first openly\ndetailed example of what is possible with large models and RL. Other follow-ups considered smaller\nLMs and ways to decrease optimization costs via approaches such as explicit task breakdown ,\nexploration strategies , new RL objectives , and cold-start data scale . However, it is still\nan open question if RL on small models can go beyond cheaper supervised alternatives and induce\nnew skills beyond the pretraining corpus . In contrast to this work, RLTs break the traditional\nframework of maximizing one-hot accuracy with verifiable rewards turning the task on its head by\nfeeding the model the correct solution as input and avoiding RL s inherent exploration challenge.\nA large part of the recent test-time scaling literature considering smaller LMs has focused on\ninducing reasoning with teacher-student supervised distillation , a widely validated technique\nin traditional LM development . This approach s popularity to induce LM reasoning dates\nearlier than the RL paradigm, with older methods harnessing verifiers and prompting for self-\nimprovement . By following a common structure of generation, filtering correct responses,\nand postprocessing them, modern RL-based distillation has seen significant advances mostly driven\nby more capable teachers and carefully curating targeted datasets . However, the\nstudent itself , and their ability to induce actual generalization remains unclear . Unlike\nthese traditional distillation pipelines, the RLT framework does not rely on verifiers for filtering,\ndirectly optimizes the teacher for downstream distillation, and does not require any post-processing,\nallowing direct transfer of reasoning capabilities to arbitrary tasks and even larger student models.\nThis work introduced a new class of Reinforcement-Learned Teachers trained with a simpler dense-\nreward task that inputs both each problem s question and solution, and optimizes the LM to provide\ninstructive reasoning traces for distillation as outputs. Empirically, students trained or cold-started\nfrom the raw outputs of a 7B RLT obtain higher performance than prior distillation pipelines using\norders-of-magnitude larger LMs. Furthermore, RLTs maintain their effectiveness even for distilling\nmuch larger students, and when providing reasoning traces for out-of-distribution tasks beyond\ntheir training corpus. Nonetheless, our work has only begun to study the design space of our new\nframework, with many exciting directions yet to be explored. One example is training RLTs and their\nstudents in tandem, allowing the teacher s explanations to adapt to the student s learning dynamics\nlive. Pushing this further, the same model could even take both roles, iterating RL with our task\nformulation, providing access to each problem s solution, to obtain instructive reasoning traces, and\nself-distillation to revise its own explanation and learn how to solve questions from scratch\nunifying the open-endedness of RL with the stability of supervised optimization.\nThe authors would like to thank Takuya Akiba for insightful project discussions, together with Yutaro\nYamada and Boris Meinardus for providing feedback on early versions of the text.\n            \n            CRITICAL: ONLY CONVERSATION AGENTS participate in this analysis:\n            - Base agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - Specialized domain agents\n            \n            EXCLUDED FROM ANALYSIS: Educational Writer, Voice Director, and Comedy Communicator (all work in post-production)\n            \n            Each participating agent should:\n            1. Read and understand the paper from your specific role's perspective\n            2. Identify key points relevant to your expertise\n            3. Prepare questions or concerns to discuss\n            4. Consider the implications from your unique viewpoint\n            \n            SPECIALIZED AGENTS: Pay special attention to domain-specific aspects that only you can address.\n            \n            This should be a comprehensive TECHNICAL analysis where EVERY conversation agent contributes their specialized perspective.\n            \n            Language: Spanish\n            ",
      "expected_output": "Comprehensive technical analysis from conversation agents only (no post-production agents)",
      "agent_role": "Coordinator"
    },
    {
      "description": "\n                    SPECIALIZED AGENTS DEEP DIVE: Domain expertise from TECHNICAL conversation agents only.\n                    \n                    PARTICIPATING SPECIALIZED AGENTS (technical focus):\n                    - AI Researcher: Provide technical insights on AI methodology and implications, - AI Philosopher: Discuss philosophical implications of AI research, - AI Doomer: Raise concerns about potential risks and negative consequences, - AI Enthusiast: Highlight positive potential and applications, - AI Newcomer: Ask basic questions that others can answer\n                    \n                    EXCLUDED: Comedy Communicator (works in post-production phase)\n                    \n                    Each specialized agent should:\n                    1. Provide deep domain-specific insights about the paper\n                    2. Identify methodological issues specific to your field\n                    3. Highlight implications that only someone with your expertise would notice\n                    4. Suggest domain-specific improvements or alternative approaches\n                    5. Connect this work to other research in your specialized area\n                    \n                    This is YOUR moment to shine with specialized knowledge that the base agents cannot provide.\n                    Focus on TECHNICAL DEPTH and DOMAIN EXPERTISE.\n                    Format as a detailed specialist consultation with clear attribution to each expert.\n                    \n                    Language: Spanish\n                    ",
      "expected_output": "Deep technical specialist analysis from 5 domain experts",
      "agent_role": "AI Researcher"
    },
    {
      "description": "\n            Based on the initial analysis, conduct a DYNAMIC Q&A session where technical conversation agents ask each other specific questions.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker) \n            - ALL specialized domain agents\n            \n            EXCLUDED FROM CONVERSATION: Educational Writer, Voice Director, and Comedy Communicator (all work in post-production)\n            \n            Instructions for multi-agent technical conversation:\n            1. ALL TECHNICAL CONVERSATION AGENTS should ask pointed questions to other agents\n            2. SPECIALIZED AGENTS should ask domain-specific questions that challenge assumptions\n            3. BASE AGENTS should ask specialists to clarify complex domain concepts\n            4. Agents must respond to questions directed at them with detailed technical answers\n            5. Follow-up questions and clarifications are encouraged\n            6. Challenge each other's assumptions respectfully\n            7. Build on each other's ideas and insights\n            8. Create a natural back-and-forth technical dialogue\n            \n            SPECIALIZED AGENTS: This is crucial - ask questions only YOU would think to ask!\n            \n            Focus areas for technical questions:\n            - Domain-specific methodological concerns\n            - Interdisciplinary connections and conflicts\n            - Alternative interpretations from different expert perspectives\n            - Practical applications in each specialist's field\n            - Potential limitations or biases from multiple viewpoints\n            \n            Format this as a realistic TECHNICAL conversation with clear speaker identification for ALL conversation participants.\n            Keep the tone SERIOUS and TECHNICAL - humor will be added later in post-production.\n            \n            Language: Spanish\n            ",
      "expected_output": "Dynamic technical Q&A conversation between conversation agents only (no post-production or humor)",
      "agent_role": "Critical Thinker"
    },
    {
      "description": "\n            Organize a structured technical debate where conversation agents with different viewpoints engage in deeper discussion.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - ALL specialized domain agents  \n            \n            EXCLUDED FROM DEBATE: Educational Writer, Voice Director, and Comedy Communicator (all work in post-production)\n            \n            Technical debate structure:\n            1. Present the main controversial points or interpretations from the paper\n            2. Have TECHNICAL CONVERSATION AGENTS take different positions and argue their cases\n            3. SPECIALIZED AGENTS: Argue from your domain expertise - what would your field say?\n            4. Allow for rebuttals and counter-arguments between different expert perspectives\n            5. Explore edge cases and hypothetical scenarios from multiple disciplinary angles\n            6. Find areas of agreement and persistent disagreements between different specialties\n            7. Synthesize different viewpoints into a richer technical understanding\n            \n            This should feel like a real interdisciplinary TECHNICAL conference where:\n            - Different specialists bring unique perspectives that sometimes conflict\n            - Domain experts interrupt each other (politely) to make field-specific points\n            - Ideas evolve through interaction between different areas of expertise\n            - New insights emerge from cross-disciplinary exchange\n            - There's intellectual tension between different specialist viewpoints\n            \n            SPECIALIZED AGENTS: Don't hold back - defend your field's perspective!\n            \n            Make it conversational and dynamic, but keep TECHNICAL FOCUS - humor will be added later.\n            \n            Language: Spanish\n            ",
      "expected_output": "Rich interdisciplinary technical debate between conversation agents only (no post-production or humor)",
      "agent_role": "Scientific Reviewer"
    },
    {
      "description": "\n            Conduct a collaborative synthesis where technical conversation agents work together to build a comprehensive understanding.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - ALL specialized domain agents\n            \n            EXCLUDED FROM SYNTHESIS: Educational Writer, Voice Director, and Comedy Communicator (all work in post-production)\n            \n            Technical collaborative process:\n            1. ALL TECHNICAL CONVERSATION AGENTS contribute their key insights from the discussions\n            2. SPECIALIZED AGENTS highlight unique perspectives only your field can provide\n            3. Agents build on each other's contributions in real-time\n            4. Identify connections between different specialist perspectives\n            5. Resolve conflicting interpretations through interdisciplinary dialogue\n            6. Co-create new insights that emerge from cross-domain discussion\n            7. Establish consensus on the most important takeaways from ALL conversation perspectives\n            \n            This should be a generative TECHNICAL conversation where:\n            - Ideas from one specialist spark new ideas in other specialists\n            - The group intelligence exceeds individual specialist perspectives\n            - Agents actively listen and respond to insights from other domains\n            - The conversation flows naturally between different areas of expertise\n            - New understanding emerges from interdisciplinary interaction\n            - Each specialist's unique knowledge contributes to the whole\n            \n            SPECIALIZED AGENTS: Share insights that ONLY someone with your expertise would have!\n            \n            Format as natural TECHNICAL conversation with organic transitions between specialist viewpoints.\n            Keep SERIOUS and FOCUSED - entertainment will be added later in post-production.\n            \n            Language: Spanish\n            ",
      "expected_output": "Collaborative technical synthesis conversation from conversation agents only (no post-production or humor)",
      "agent_role": "Coordinator"
    },
    {
      "description": "\n            Based on all previous conversations and analyses, conduct a final comprehensive technical discussion that synthesizes insights from conversation agents.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - ALL specialized domain agents\n            \n            EXCLUDED: Educational Writer, Voice Director, and Comedy Communicator (they will process this output in post-production)\n            \n            The final technical discussion should:\n            1. Synthesize insights from the Q&A, specialist deep dive, debate, and collaborative sessions\n            2. Cover all major points of the paper from multiple expert perspectives\n            3. Include the rich specialist perspectives developed through agent interactions\n            4. Address concerns and criticisms that emerged from different domains\n            5. Explore implications and applications discussed by various specialists\n            6. Be comprehensive and technically rigorous for expert audiences\n            7. Highlight unique insights that could ONLY come from having multiple specialist perspectives\n            \n            CRITICAL: This final technical discussion must incorporate:\n            - Domain-specific insights from ALL specialist conversation agents\n            - Cross-disciplinary connections discovered during discussions\n            - Unique perspectives that emerged from interdisciplinary dialogue\n            - Technical depth and rigor appropriate for expert audiences\n            \n            This is the FINAL technical conversation output that will be handed to the post-production team.\n            Make it comprehensive, rigorous, and rich with all the insights gathered.\n            Keep it TECHNICAL and SERIOUS - post-production will handle accessibility and entertainment.\n            \n            Language: Spanish\n            ",
      "expected_output": "Final comprehensive technical discussion ready for post-production processing",
      "agent_role": "Critical Thinker"
    },
    {
      "description": "\n                POST-PRODUCTION PHASE 1: COMEDY ENHANCEMENT\n                \n                You are receiving ALL the serious technical conversations from the conversation phase.\n                Your job is to add appropriate humor and entertainment value while maintaining respect for the science.\n                \n                Technical content processed so far includes:\n                - Initial analysis from all conversation agents\n                - Specialized domain expert deep dive\n                - Dynamic Q&A sessions between experts  \n                - Interdisciplinary technical debates\n                - Collaborative synthesis\n                - Final comprehensive technical discussion\n                \n                Review ALL this technical content and:\n                1. Identify moments where humor can enhance understanding\n                2. Create clever analogies that make complex concepts memorable\n                3. Find amusing but respectful observations about the research\n                4. Develop entertaining examples that illustrate key points\n                5. Add witty commentary that makes the discussion more engaging\n                6. Use wordplay and clever observations appropriate to the topic\n                7. Insert humor that bridges different specialist perspectives\n                8. Make technical debates more entertaining without losing substance\n                9. Add comic relief to dense technical discussions\n                10. Create memorable one-liners that help key concepts stick\n                \n                Your goal is to make the content more accessible and entertaining WITHOUT undermining the scientific rigor.\n                Think Neil deGrasse Tyson explaining astrophysics - serious science, but delivered with wit and charm.\n                \n                The output should be the SAME technical content but now enhanced with appropriate humor and entertainment.\n                You are NOT writing the final script - you're adding humor to the technical discussions.\n                \n                \n            TONE REQUIREMENTS - HUMOROUS:\n            - Use appropriate humor, wit, and clever analogies to make content engaging\n            - Include light jokes and funny observations that enhance understanding\n            - Use entertaining examples and amusing comparisons\n            - Keep humor respectful and relevant to the topic\n            - Think science communicators like Neil deGrasse Tyson or Bill Nye\n            - Use wordplay, puns, and clever observations when appropriate\n            - Make the audience smile while learning\n            - Avoid offensive humor or jokes that undermine the science\n            \n                \n                Language: Spanish\n                ",
      "expected_output": "Technical content enhanced with appropriate humor and entertainment while maintaining scientific accuracy",
      "agent_role": "Comedy Communicator"
    },
    {
      "description": "\n            POST-PRODUCTION PHASE 2: EDUCATIONAL SCRIPT CREATION\n            \n            Transform ALL the rich content into a comprehensive educational lecture text.\n            \n            You are receiving the complete output, which includes:\n            - Initial analysis from all conversation agents\n            - Specialized domain expert deep dive\n            - Dynamic Q&A sessions between experts\n            - Interdisciplinary technical debates\n            - Collaborative synthesis\n            - Final comprehensive technical discussion\n            - Comedy-enhanced version with appropriate humor\n            \n            Your job is to distill ALL this rich content into a single educator voice.\n            \n            The script should be in the style of popular science educators like 3Blue1Brown:\n            1. Written as a SINGLE EDUCATOR speaking directly to the listener (use \"tú\"/\"usted\")\n            2. Use analogies and accessible explanations\n            3. Include ALL key insights from the multiple conversations and specialist exchanges\n            4. Be engaging and educational, not just informative\n            5. Flow naturally from concept to concept with smooth transitions\n            6. Include moments of wonder and intellectual curiosity\n            7. Break down complex ideas into digestible parts\n            8. Use a teaching tone that makes the listener feel they're learning something fascinating\n            9. Write as continuous text ready to be read by a voice actor\n            10. NO section headers, NO subheaders, NO formatting marks\n            11. Don't address the public with greetings or goodbyes, but make questions\n            12. Always end up with questions for the reader and practical implications\n            13. Write as plain text that flows naturally for voice reading\n            14. NO [PAUSES], NO [MUSIC], NO stage directions - just the educational content\n            15. CRITICAL: Address the listener directly - \"puedes imaginar\", \"si consideras\", \"te darás cuenta\"\n            16. DO NOT write as if summarizing a discussion - write as if YOU are the teacher\n            17. Avoid phrases like \"los expertos discutieron\" or \"el equipo concluyó\"\n            18. Incorporate the depth and nuance that emerged from ALL agent conversations\n            \n            CRITICAL - MULTI-SPECIALIST INTEGRATION:\n            19. Weave in insights that could ONLY come from having multiple specialist perspectives\n            20. Include cross-disciplinary connections discovered during discussions\n            21. Incorporate domain-specific knowledge from ALL participating specialists\n            22. Show how different expert viewpoints enhance understanding of the topic\n            23. Naturally integrate the entertaining elements added by the Comedy Communicator\n            24. Demonstrate the value of interdisciplinary analysis throughout\n            \n            \n            CRITICAL TECHNICAL REQUIREMENTS - THIS IS MANDATORY:\n            15. YOU MUST include comprehensive technical depth throughout the entire script\n            16. EXPLAIN IN DETAIL: experimental design, control groups, statistical methods used\n            17. INCLUDE SPECIFIC NUMBERS: sample sizes (e.g. \"54 participants\"), p-values, effect sizes, confidence intervals\n            18. DISCUSS METHODOLOGY THOROUGHLY: EEG analysis methods, data collection procedures, analysis pipelines\n            19. ADDRESS LIMITATIONS AND CONFOUNDS: what could bias results, alternative explanations\n            20. USE TECHNICAL TERMS CORRECTLY: neural connectivity, spectral analysis, statistical significance, but ALWAYS explain them\n            21. COMPARE TO OTHER STUDIES: how does this fit with existing research in the field\n            22. DISCUSS THEORETICAL IMPLICATIONS: what theories does this support or challenge\n            23. INCLUDE TECHNICAL DETAILS: electrode placement, signal processing, statistical tests used\n            24. EXPLAIN THE \"HOW\" not just the \"WHAT\": how did they measure cognitive load, how did they analyze connectivity\n            25. DISCUSS FUTURE RESEARCH: specific methodological improvements, follow-up studies needed\n            26. BE PRECISE WITH TERMINOLOGY: use exact scientific language for concepts\n            27. This should feel like a technical seminar for graduate students or researchers\n            \n            \n            \n        DURATION REQUIREMENT: EXACTLY 3 minutes of content (420-480 words) - THIS IS MANDATORY\n        \n        DEPTH GUIDANCE FOR 3 MINUTES:\n        \n            - Focus on 1-2 main concepts only\n            - Keep explanations concise but complete\n            - Include one compelling example per main point\n            - Go straight to the point without much additional context\n            \n        \n        TECHNICAL CALCULATION:\n        - Target reading speed: ~150 words per minute\n        - Word range: 420-480 words\n        - If content is too short, EXPAND significantly with more detail and depth\n        - If too long, maintain quality but adjust information density\n        \n            \n            \n            LANGUAGE REQUIREMENTS FOR SPANISH:\n            \n            CRITICAL: AVOID ANGLICISMS whenever possible and use proper Spanish terms:\n            - Instead of \"link\" use \"enlace\" or \"vínculo\"\n            - Instead of \"feedback\" use \"retroalimentación\" or \"respuesta\"\n            - Insted of \"puzzle\" use \"rompecabezas\" or \"problema\"\n            - Instead of \"performance\" use \"rendimiento\" or \"desempeño\"\n            - Instead of \"input/output\" use \"entrada/salida\"\n            - Instead of \"update\" use \"actualizar\" or \"poner al día\"\n            \n            EXCEPTIONS - You CAN use anglicisms for:\n            1. Very new technical terms with no established translation (e.g., \"blockchain\", \"ChatGPT\")\n            2. Proper names of tools/companies (e.g., \"TensorFlow\", \"GitHub\", \"OpenAI\")\n            3. Widely adopted terms in scientific literature (e.g., \"machine learning\" vs \"aprendizaje automático\")\n            4. When the Spanish term is more confusing than helpful\n            \n            GENERAL RULES:\n            - Always prioritize natural Spanish expressions\n            - Use Spanish sentence structures and idioms\n            - Make it sound like a native Spanish speaker wrote it\n            - When you must use an anglicism, briefly explain it if needed\n            \n            \n            Language: Spanish\n            ",
      "expected_output": "Comprehensive educational script incorporating ALL conversation insights and humor",
      "agent_role": "Educational Writer"
    },
    {
      "description": "\n            POST-PRODUCTION PHASE 3: FINAL VOICE OPTIMIZATION\n            \n            Transform the Educational Writer's script into a PERFECT voice-ready script.\n            \n            You are receiving the educational script that has been carefully crafted from all conversation insights\n            and enhanced with appropriate humor.\n            Your job is PURELY technical optimization for voice delivery.\n            \n            CRITICAL: Verify the content meets the 3-minute target (420-480 words). If it's too short, EXPAND it significantly.\n            CRITICAL: Ensure technical level is technical - include deep technical analysis.\n            \n            MANDATORY VOICE OPTIMIZATION REQUIREMENTS:\n            1. Create a SINGLE, CONTINUOUS text ready for a voice actor to read\n            2. Markdown formatting, but NO headers, NO bullet points, NO lists\n            3. Convert ALL content into natural, flowing sentences\n            4. Replace any remaining bullet points with complete sentences\n            5. Ensure PERFECT flow from sentence to sentence\n            6. Remove formatting marks: #, -, •, etc for titles and subtitles, but keep for bold and italic text\n            7. Make sure sentences are not too long or complex for voice delivery\n            8. Write naturally in Spanish without academic formalities\n            9. Remove any remaining conversational artifacts (\"como mencionamos antes\", \"en nuestra discusión\")\n            10. Ensure seamless transitions between concepts\n            11. Maintain the conversational richness but in a single educator voice\n            12. Read the text mentally to ensure it sounds natural when spoken\n            13. Ensure proper pronunciation flow for difficult technical terms\n            14. Remove any repetitive content that may have emerged from multiple discussions\n            15. Maintain the depth gained from agent conversations while ensuring clarity\n            16. Perfect pacing for natural speech rhythm\n            17. Eliminate any phrases that sound like committee work or group consensus\n            18. Make it sound like ONE expert who has deeply understood the topic\n            19. Ensure technical accuracy while maintaining conversational flow\n            20. Optimize for voice actor performance and listener engagement\n            21. This should sound like ONE VOICE teaching, not a summary of multiple voices\n            22. Avoid words that could make this sound like written by an LLM, like not often used words: \"fascinante\", \"delve\", \"revelador\"\n            23. Introduction should be a catchy hook that makes the listener want to listen to the entire video, something like a question or a statement that makes the listener want to know more\n            24. DO NOT add new content - only optimize existing content for voice delivery\n            25. DO NOT change the educational message - only improve its delivery\n            26. Preserve the humor elements but ensure they flow naturally in speech\n\n            \n            LANGUAGE REQUIREMENTS FOR SPANISH:\n            \n            CRITICAL: AVOID ANGLICISMS whenever possible and use proper Spanish terms:\n            - Instead of \"link\" use \"enlace\" or \"vínculo\"\n            - Instead of \"feedback\" use \"retroalimentación\" or \"respuesta\"\n            - Insted of \"puzzle\" use \"rompecabezas\" or \"problema\"\n            - Instead of \"performance\" use \"rendimiento\" or \"desempeño\"\n            - Instead of \"input/output\" use \"entrada/salida\"\n            - Instead of \"update\" use \"actualizar\" or \"poner al día\"\n            \n            EXCEPTIONS - You CAN use anglicisms for:\n            1. Very new technical terms with no established translation (e.g., \"blockchain\", \"ChatGPT\")\n            2. Proper names of tools/companies (e.g., \"TensorFlow\", \"GitHub\", \"OpenAI\")\n            3. Widely adopted terms in scientific literature (e.g., \"machine learning\" vs \"aprendizaje automático\")\n            4. When the Spanish term is more confusing than helpful\n            \n            GENERAL RULES:\n            - Always prioritize natural Spanish expressions\n            - Use Spanish sentence structures and idioms\n            - Make it sound like a native Spanish speaker wrote it\n            - When you must use an anglicism, briefly explain it if needed\n            \n            \n            CRITICAL: This is the FINAL version that will be published. Make it PERFECT for voice delivery.\n            \n            Language: Spanish\n            ",
      "expected_output": "FINAL publication-ready voice script optimized for delivery (420-480 words) with humor",
      "agent_role": "Voice Director"
    }
  ]
}