Section: Section 10
Characters: 9839
==================================================
• Data annotation for AI engineering represents a larger challenge than in traditional ML engineering.  
• Traditional ML engineering works primarily with tabular data, whereas foundation models work with unstructured data.  
• In AI engineering, data manipulation includes deduplication, tokenization, context retrieval, and quality control (for example, removing sensitive information and toxic data).  
• The amount of data required depends on the adapter technique used:  
 – Training a model from scratch requires the most data.  
 – Finetuning a pre-trained model requires less data than training from scratch.  
 – Prompt engineering requires even less data.  
• Inference optimization is the process of making models faster and cheaper to run.  
 – Foundation models often use autoregressive generation, meaning tokens are generated sequentially.  
  • For instance, if generating one token takes 10 ms, producing 100 tokens takes approximately 1 second, which is a challenge when aiming for millisecond-level latency.  
 – Techniques for inference optimization include quantization, distillation, and parallelism.  
• Table-4 (as provided) compares responsibilities between traditional ML and foundation models:  
 – Modeling and training:  
  • Traditional ML requires ML knowledge for training a model from scratch.  
  • With foundation models, ML knowledge is considered a nice-to-have.  
 – Dataset engineering:  
  • In traditional ML, the focus is on feature engineering, particularly with tabular data.  
  • With foundation models, the focus is on data deduplication, tokenization, context retrieval, and quality control.  
 – Inference optimization:  
  • Important in traditional ML engineering and even more important with foundation models.  
• The application development layer in AI engineering consists of evaluation, prompt engineering, and AI interface:  
 – Evaluation is used to mitigate risks and uncover opportunities during model adaptation. It is applied for model selection, benchmarking progress, deployment readiness, and detecting issues or opportunities in production.  
 – Prompt engineering involves obtaining desirable behaviors from AI models by providing instructions and context (for example, providing specific numbers of examples in prompt design).  
  • The Gemini evaluation example shows that using a prompt technique labeled CoT@32 improved Gemini Ultra’s performance on MMLU from 83.7% to 90.04%.  
 – Context construction can include providing the model with a memory management system to track history during complex tasks.  
 – AI interface refers to building interfaces for end users to interact with AI applications. These can be implemented as:  
  • Standalone web, desktop, or mobile apps  
  • Browser extensions for quick queries  
  • Chatbots integrated into messaging apps such as Slack, Discord, WeChat, and WhatsApp  
  • API integrations for embedding AI functionalities into products (example platforms: VSCode via Copilot, GitHub Copilot as a plug-in, Grammarly as a browser extension)  
  • Voice-based or embodied (augmented and virtual reality) interfaces  
  • Common tools for building these applications include Streamlit, Gradio, and Plotly Dash.  
• The evolution of ML frameworks now includes increasing support for JavaScript APIs (e.g., LangChain.js, Transformers.js, OpenAI’s Node library, and Vercel’s AI SDK) alongside Python-centric approaches.  
• Tables (such as Table-5 and Table-6) illustrate:  
 – How different prompting techniques can affect model performance (demonstrated by differences in performance measures on benchmarks like MMLU).  
 – The changing importance of different responsibilities in app development between traditional ML (where model quality is the differentiation) and foundation model–based AI engineering (where differentiation comes through application development processes including interface, evaluation, and prompt engineering).  
• The technical structure outlined includes a shift from focusing solely on modeling and training, as in traditional ML engineering, to also including significant responsibilities in dataset engineering and inference optimization in AI engineering.  
• The reported challenge of sequential (autoregressive) token generation makes reducing latency to the level expected for typical internet applications a major technical hurdle.  
• In summary, the technical content details the differences in data handling, model adaptation techniques (prompt engineering, finetuning), evaluation processes, and interface development between traditional ML engineering and AI engineering with foundation models.