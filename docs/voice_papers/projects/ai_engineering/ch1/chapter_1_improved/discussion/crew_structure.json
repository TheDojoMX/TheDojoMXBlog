{
  "project_name": "chapter_1_improved",
  "paper_title": "with Foundation Models",
  "language": "Spanish",
  "agents": [
    {
      "role": "Research Coordinator",
      "goal": "Facilitate productive discussion about the paper's content",
      "backstory": "You are an experienced research coordinator who ensures discussions \n            stay focused on the paper's content. You help organize thoughts and ensure all \n            important points from the paper are covered. You ONLY discuss what's in the paper."
    },
    {
      "role": "Methodology Explainer",
      "goal": "Explain the research methodology and approach as described in the paper",
      "backstory": "You are skilled at understanding and explaining research methodologies. \n                You help audiences understand how the research was conducted, what methods were used, \n                and why. You ONLY explain methods actually described in the paper."
    },
    {
      "role": "Educational Writer",
      "goal": "Create engaging educational content following the Voice Papers style guide",
      "backstory": "You are a skilled science communicator who transforms technical discussions \n            into accessible, engaging educational content following the Voice Papers style guide.\n            \n            You meticulously follow the narrative structure, conversational elements, and storytelling\n            techniques specified in the style guide. You use engaging hooks, natural transitions,\n            multiple analogies, and maintain the perfect rhythm for audio narration.\n            \n            You take insights from all the agents and weave them into a cohesive narrative that \n            teaches and inspires. You work ONLY with information discussed about the paper, \n            never adding external content."
    },
    {
      "role": "Knowledge Synthesizer",
      "goal": "Extract and connect key insights ONLY from the provided research paper",
      "backstory": "You are a brilliant research analyst who excels at understanding complex \n        academic work and identifying the core innovations and contributions. You have a gift for \n        seeing how different pieces within the paper connect and build upon each other. Your approach \n        is always constructive, focusing on what we can learn from the paper itself. You NEVER add \n        information not found in the source document and always cite specific sections."
    },
    {
      "role": "Context Provider",
      "goal": "Explain the context and background ONLY as presented in the paper itself",
      "backstory": "You are skilled at identifying and explaining the contextual information that \n        authors provide in their papers. You help audiences understand the significance of research \n        by carefully extracting the background, motivations, and related work sections that the \n        authors have included. You NEVER add external context not mentioned in the paper and always \n        indicate which section of the paper you're referencing."
    },
    {
      "role": "Clarity Specialist",
      "goal": "Transform complex concepts following Voice Papers style guide principles",
      "backstory": "You are a master educator who follows the Voice Papers style guide for explanations. \n        You use the layered approach: starting simple and adding complexity gradually. You excel at \n        taking the examples and analogies provided by authors and presenting them clearly. You define \n        terms naturally in flow and use engagement techniques from the style guide. You ONLY use \n        information found within the paper, never adding external examples."
    }
  ],
  "tasks": [
    {
      "description": "\n            Analyze the synthesis of the paper titled \"with Foundation Models\" and provide your perspective.\n            \n            Paper synthesis:\n            TLDR:\n• The chapter explains the evolution from early language models to large‑scale, multimodal foundation models that power modern AI applications.\n• It details how self‑supervision and scaling have enabled versatile model capabilities, discusses diverse AI use cases (from coding and creative tools to education and conversational bots), and outlines key considerations in planning and building AI applications.\n• The primary conclusion is that AI engineering—building applications on foundation models—has emerged as a distinct, rapidly growing discipline that repurposes traditional ML engineering principles while facing unique challenges such as evaluation, integration, and evolving infrastructure.\n\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n1. OVERVIEW & INTRODUCTION\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\nCHAPTER\nIntroduction to Building AI Applications with Foundation Models\n\n• “If I could use only one word to describe AI post-2020, it’d be scale. The AI models behind applications like ChatGPT, Google’s Gemini, and Midjourney are at such a scale that they’re consuming a nontrivial portion of the world’s electricity, and we’re at risk of running out of publicly available internet data to train them.”\n• The chapter explains how scaling up AI models has two major consequences:\n  – Models are becoming more powerful and capable of a broader range of tasks, enhancing productivity and quality of life.\n  – Training these models requires significant resources, which has led to the rise of “model as a service.”\n• It emphasizes that while building applications on ML models is not new, the arrival of huge, readily available models (foundation models) creates new possibilities and challenges.\n• The chapter lays out an overview of foundation models, successful AI use cases, and a new AI stack. It also contrasts today’s AI engineering with traditional ML engineering and highlights the rapid emergence of AI engineering as a discipline.\n\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n2. FROM LANGUAGE MODELS TO FOUNDATION MODELS\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\nA. Early Language Models & Evolution\n • Language models encode statistical information about language – for example, predicting that “blue” might fill the blank in “My favorite color is __.”\n • Historical examples:\n   – Sherlock Holmes used simple statistics in “The Adventure of the Dancing Men.”\n   – Claude Shannon’s work on “Prediction and Entropy of Printed English” still informs modeling.\n • Early language models dealt with one language; modern ones involve multiple languages.\n \nB. Understanding Tokens & Tokenization\n • A token is the basic unit (character, word, or sub-word) chosen by the model; for example, GPT‑4 tokenizes “I can’t wait to build AI applications” into nine tokens.\n • Tokenization breaks text into tokens, with vocabulary size controlling the distinct units available.\n • Discussion on why tokens are preferred over words or characters:\n  1. Tokens capture meaningful sub-word components.\n  2. They reduce the vocabulary size.\n  3. They handle unknown words effectively.\n\nC. Self‑Supervision & Types of Language Models\n • Self‑supervision is the method by which language models are trained using the input data as both labels and context.\n • Differentiation between masked language models (e.g., BERT) and autoregressive language models (used for text generation).\n • Explanation of “completion” – the model predicts the subsequent token(s) based on preceding text.\n • Discussion of open‑ended outputs characteristic of generative AI.\n • The probabilistic nature of these outputs makes them powerful for tasks like translation, summarization, coding, and more.\n\nD. Transition to Foundation Models\n • While traditional language models worked only with text, foundation models integrate additional modalities (images, videos, 3D assets, protein structures, etc.) to work in the real world.\n • Models like GPT‑4V and Claude 3 are examples of multimodal models.\n • Discussion of CLIP as an early multimodal embedding model, and how these models have shifted from task‑specific to general‑purpose capabilities.\n\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n3. AI ENGINEERING PRINCIPLES & USE CASES\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\nA. AI Use Cases Overview\n • The chapter details a broad spectrum of foundation model use cases in both consumer and enterprise domains.\n • It discusses the breadth of applications from coding, image and video production, writing, and education to conversational bots, information aggregation, data organization, and workflow automation.\n\nB. Detailed Use Case Examples\n 1. Coding:\n  • Tools like GitHub Copilot and startups in the AI coding space, demonstrating capabilities to generate code, convert natural language to code, translate between languages, and more.\n  • Examples include AgentGPT, DB‑GPT, SQL Chat, and others.\n 2. Image and Video Production:\n  • Creative applications like Midjourney, Adobe Firefly, and tools for photo and video editing.\n  • Mention of AI‑generated profile pictures and dynamic ad creation.\n 3. Writing:\n  • AI assistance in writing through autocorrect, auto‑completion, email enhancement, and even book or essay generation.\n  • MIT studies showing increased productivity and improved output quality.\n 4. Education:\n  • AI tutors, personalized learning assistants, auto‑generated lecture plans, and tools for language learning.\n  • Discussion of innovative applications such as personalized quizzes and AI‑based debate partners.\n 5. Conversational Bots:\n  • Ranging from customer support bots to AI companions and even conversational NPCs in games.\n 6. Information Aggregation & Data Organization:\n  • Applications that distill and summarize vast amounts of information (e.g., “talk-to-your‑docs”).\n  • Use cases in enterprise scenarios to aid internal knowledge management.\n 7. Workflow Automation:\n  • Automating everyday tasks (booking restaurants, data entry) and enterprise functions (lead management, invoicing).\n  • Introduction of AI agents that can plan and execute tasks by accessing external tools.\n\nC. Industry Research & Data Insights\n • References to surveys and studies:\n  – Gartner surveys on business continuity risks.\n  – Research on task exposure in occupations (e.g., interpreters, translators, tax preparers).\n  – Data showing enterprise versus consumer adoption patterns.\n • Organization-specific examples from Deloitte and O’Reilly surveys.\n • Figures and tables (e.g., distribution of open source use cases, examples of occupations most exposed to AI).\n\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n4. PLANNING AI APPLICATIONS\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\nA. Use Case Evaluation\n • Discussion on why one might build or adopt an AI application:\n  1. Competitive necessity to avoid obsolescence.\n  2. Opportunities to boost profits and efficiency.\n  3. The need to experiment to avoid being left behind.\n • Consideration of whether to build internally or leverage existing models.\n\nB. Determining the Role of AI vs. Humans\n • Differentiating between critical versus complementary roles for AI.\n • Reactive versus proactive AI roles:\n  – Reactive features: respond to specific user inputs (e.g., chatbots).\n  – Proactive features: precomputed responses like traffic alerts.\n • Dynamic versus static AI features and potential approaches (individualized models or shared models updated periodically).\n\nC. AI Product Defensibility & Competitive Advantages\n • Challenges: low entry barriers mean an application can quickly become duplicated by larger companies.\n • Types of competitive moats:\n  – Technology, data (usage data and quality improvements), and distribution channels.\n • Examples highlighting startups that began as niche features and grew independently.\n\nD. Setting Expectations & Milestone Planning\n • Determining success metrics (quality, latency, cost, customer satisfaction).\n • Importance of systematic experimentation and establishing usefulness thresholds.\n • The “Crawl-Walk-Run” framework for increasing AI automation gradually.\n • Emphasis on maintenance challenges due to rapidly evolving technology and regulations.\n\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n5. THE AI ENGINEERING STACK\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\nA. Overview of the Stack\n • Explanation that AI engineering evolved from ML engineering while incorporating new layers.\n • Three layers:\n  1. Application Development: Focus on prompt engineering, user interfaces, and rigorous evaluation.\n  2. Model Development: Involves modeling, training, finetuning, dataset engineering, and inference optimization.\n  3. Infrastructure: Supporting tasks like model serving, resource management, and monitoring.\n\nB. Detailed Responsibilities in the Stack\n 1. Application Development:\n  • Evaluating models, prompt engineering (crafting inputs to induce desired outputs), and constructing effective AI interfaces.\n  • Emergence of interfaces such as standalone apps, browser extensions, and plugins (e.g., integration in VSCode).\n 2. Model Development:\n  • Explores modeling/training, dataset engineering (curation, deduplication, tokenization, quality control), and inference optimization (quantization, distillation, parallelism).\n  • Discussion on pre‑training, finetuning, and post‑training.\n 3. Infrastructure:\n  • Traditional support systems remain critical (resource management, serving, and monitoring), even as application and model layers evolve.\n \nC. Data & Ecosystem Analysis\n • GitHub analysis of AI-related repositories showing rapid growth post‑Stable Diffusion and ChatGPT.\n • Comparison of growth trends among application development, model development, and infrastructure.\n • The enduring fundamentals of ML engineering still underpin much of AI engineering.\n\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n6. AI ENGINEERING VERSUS MACHINE LEARNING ENGINEERING\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\nA. Key Differences\n 1. Model Usage:\n  • Traditional ML engineering often involves training models from scratch.\n  • AI engineering leverages pre‑trained foundation models, focusing on model adaptation rather than building models from the ground up.\n 2. Resource Scale & Compute:\n  • Foundation models are larger, more compute‑intensive, and come with higher latency, requiring more robust hardware (e.g., hundreds or thousands of GPUs).\n 3. Open‑Ended Outputs & Evaluation Challenges:\n  • Foundation models produce open‑ended outputs, making evaluation more challenging than in close‑ended ML tasks.\n\nB. Techniques for Model Adaptation\n • Prompt‑based techniques (prompt engineering) adapt models without weight updates.\n • Finetuning involves updating model weights for improved performance on specific tasks.\n • Explanation of terms: pre‑training (training from scratch), finetuning (continuing training on new data), and post‑training (further refinement by model developers versus application developers).\n\nC. Shifting Roles & Skill Sets\n • Traditional ML engineers have deep knowledge in algorithms, gradient descent, and loss functions.\n • AI engineers increasingly come from full‑stack or software development backgrounds, capable of rapid iteration, and are now responsible for integrating models into high‑quality user experiences.\n • The convergence of front‑end and back‑end skills as AI interfaces become more complex.\n\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n7. FINAL REMARKS & OVERVIEW OF THE REMAINING BOOK\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n• The chapter concludes by summarizing:\n  – The evolution from language models to foundation models.\n  – How foundation models have given rise to a new discipline—AI engineering.\n  – The range of applications enabled by these models and the considerations in planning, evaluating, and building them.\n• It emphasizes that despite the rapid pace of change and the multitude of new tools and techniques, many core principles from traditional ML engineering remain applicable.\n• A framework is introduced for navigating the dynamic AI landscape, which the rest of the book will explore step-by-step—from foundational building blocks to advanced engineering techniques and infrastructure challenges.\n\n–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\nEnd of Organized Extracted Content\n\nThis complete organized text presents all the extracted content logically by initial overview, technical evolution, use cases, planning considerations, stack details, comparisons with ML engineering, and final reflections.\n            \n            CRITICAL: ONLY CONVERSATION AGENTS participate in this analysis:\n            - Base agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - Specialized domain agents\n            \n            EXCLUDED FROM ANALYSIS: Educational Writer and Comedy Communicator (work in post-production)\n            \n            Each participating agent should:\n            1. Read and understand the paper from your specific role's perspective\n            2. Identify key points relevant to your expertise\n            3. Prepare questions or concerns to discuss\n            4. Consider the implications from your unique viewpoint\n            \n            SPECIALIZED AGENTS: Pay special attention to domain-specific aspects that only you can address.\n            \n            This should be a comprehensive TECHNICAL analysis where EVERY conversation agent contributes their specialized perspective.\n            \n            Language: Spanish\n            ",
      "expected_output": "Comprehensive technical analysis from conversation agents only (no post-production agents)",
      "agent_role": "Research Coordinator"
    },
    {
      "description": "\n                    SPECIALIZED AGENTS DEEP DIVE: Domain expertise from TECHNICAL conversation agents only.\n                    \n                    PARTICIPATING SPECIALIZED AGENTS (technical focus):\n                    - Clarity Specialist: Transform complex concepts following Voice Papers style guide principles\n                    \n                    EXCLUDED: Comedy Communicator (works in post-production phase)\n                    \n                    Each specialized agent should:\n                    1. Provide deep domain-specific insights about the paper\n                    2. Identify methodological issues specific to your field\n                    3. Highlight implications that only someone with your expertise would notice\n                    4. Suggest domain-specific improvements or alternative approaches\n                    5. Connect this work to other research in your specialized area\n                    \n                    This is YOUR moment to shine with specialized knowledge that the base agents cannot provide.\n                    Focus on TECHNICAL DEPTH and DOMAIN EXPERTISE.\n                    Format as a detailed specialist consultation with clear attribution to each expert.\n                    \n                    Language: Spanish\n                    ",
      "expected_output": "Deep technical specialist analysis from 1 domain experts",
      "agent_role": "Clarity Specialist"
    },
    {
      "description": "\n            Based on the initial analysis, conduct a DYNAMIC Q&A session where technical conversation agents ask each other specific questions about the paper synthesis.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker) \n            - ALL specialized domain agents\n            \n            EXCLUDED FROM CONVERSATION: Educational Writer and Comedy Communicator (work in post-production)\n            \n            Instructions for multi-agent technical conversation:\n            1. ALL TECHNICAL CONVERSATION AGENTS should ask pointed questions to other agents\n            2. SPECIALIZED AGENTS should ask domain-specific questions that challenge assumptions\n            3. BASE AGENTS should ask specialists to clarify complex domain concepts\n            4. Agents must respond to questions directed at them with detailed technical answers\n            5. Follow-up questions and clarifications are encouraged\n            6. Challenge each other's assumptions respectfully\n            7. Build on each other's ideas and insights\n            8. Create a natural back-and-forth technical dialogue\n            \n            SPECIALIZED AGENTS: This is crucial - ask questions only YOU would think to ask!\n            \n            Focus areas for technical questions:\n            - Domain-specific methodological concerns\n            - Interdisciplinary connections and conflicts\n            - Alternative interpretations from different expert perspectives\n            - Practical applications in each specialist's field\n            - Potential limitations or biases from multiple viewpoints\n            \n            Format this as a realistic TECHNICAL conversation with clear speaker identification for ALL conversation participants.\n            Keep the tone SERIOUS and TECHNICAL - humor will be added later in post-production.\n            \n            Language: Spanish\n            ",
      "expected_output": "Dynamic technical Q&A conversation between conversation agents only (no post-production or humor)",
      "agent_role": "Educational Writer"
    },
    {
      "description": "\n            Organize a structured technical debate where conversation agents with different viewpoints engage in deeper discussion.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - ALL specialized domain agents  \n            \n            EXCLUDED FROM DEBATE: Educational Writer and Comedy Communicator (work in post-production)\n            \n            Technical debate structure:\n            1. Present the main controversial points or interpretations from the paper\n            2. Have TECHNICAL CONVERSATION AGENTS take different positions and argue their cases\n            3. SPECIALIZED AGENTS: Argue from your domain expertise - what would your field say?\n            4. Allow for rebuttals and counter-arguments between different expert perspectives\n            5. Explore edge cases and hypothetical scenarios from multiple disciplinary angles\n            6. Find areas of agreement and persistent disagreements between different specialties\n            7. Synthesize different viewpoints into a richer technical understanding\n            \n            This should feel like a real interdisciplinary TECHNICAL conference where:\n            - Different specialists bring unique perspectives that sometimes conflict\n            - Domain experts interrupt each other (politely) to make field-specific points\n            - Ideas evolve through interaction between different areas of expertise\n            - New insights emerge from cross-disciplinary exchange\n            - There's intellectual tension between different specialist viewpoints\n            \n            SPECIALIZED AGENTS: Don't hold back - defend your field's perspective!\n            \n            Make it conversational and dynamic, but keep TECHNICAL FOCUS - humor will be added later.\n            \n            Language: Spanish\n            ",
      "expected_output": "Rich interdisciplinary technical debate between conversation agents only (no post-production or humor)",
      "agent_role": "Methodology Explainer"
    },
    {
      "description": "\n            Conduct a collaborative synthesis where technical conversation agents work together to build a comprehensive understanding.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - ALL specialized domain agents\n            \n            EXCLUDED FROM SYNTHESIS: Educational Writer and Comedy Communicator (work in post-production)\n            \n            Technical collaborative process:\n            1. ALL TECHNICAL CONVERSATION AGENTS contribute their key insights from the discussions\n            2. SPECIALIZED AGENTS highlight unique perspectives only your field can provide\n            3. Agents build on each other's contributions in real-time\n            4. Identify connections between different specialist perspectives\n            5. Resolve conflicting interpretations through interdisciplinary dialogue\n            6. Co-create new insights that emerge from cross-domain discussion\n            7. Establish consensus on the most important takeaways from ALL conversation perspectives\n            \n            This should be a generative TECHNICAL conversation where:\n            - Ideas from one specialist spark new ideas in other specialists\n            - The group intelligence exceeds individual specialist perspectives\n            - Agents actively listen and respond to insights from other domains\n            - The conversation flows naturally between different areas of expertise\n            - New understanding emerges from interdisciplinary interaction\n            - Each specialist's unique knowledge contributes to the whole\n            \n            SPECIALIZED AGENTS: Share insights that ONLY someone with your expertise would have!\n            \n            Format as natural TECHNICAL conversation with organic transitions between specialist viewpoints.\n            Keep SERIOUS and FOCUSED - entertainment will be added later in post-production.\n            \n            Language: Spanish\n            ",
      "expected_output": "Collaborative technical synthesis conversation from conversation agents only (no post-production or humor)",
      "agent_role": "Research Coordinator"
    },
    {
      "description": "\n            Based on all previous conversations and analyses, conduct a final comprehensive technical discussion that synthesizes insights from conversation agents.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - ALL specialized domain agents\n            \n            EXCLUDED: Educational Writer and Comedy Communicator (they will process this output in post-production)\n            \n            The final technical discussion should:\n            1. Synthesize insights from the Q&A, specialist deep dive, debate, and collaborative sessions\n            2. Cover all major points of the paper from multiple expert perspectives\n            3. Include the rich specialist perspectives developed through agent interactions\n            4. Address concerns and criticisms that emerged from different domains\n            5. Explore implications and applications discussed by various specialists\n            6. Be comprehensive and technically rigorous for expert audiences\n            7. Highlight unique insights that could ONLY come from having multiple specialist perspectives\n            \n            CRITICAL: This final technical discussion must incorporate:\n            - Domain-specific insights from ALL specialist conversation agents\n            - Cross-disciplinary connections discovered during discussions\n            - Unique perspectives that emerged from interdisciplinary dialogue\n            - Technical depth and rigor appropriate for expert audiences\n            \n            This is the FINAL technical conversation output that will be handed to the post-production team.\n            Make it comprehensive, rigorous, and rich with all the insights gathered.\n            Keep it TECHNICAL and SERIOUS - post-production will handle accessibility and entertainment.\n            \n            Language: Spanish\n            ",
      "expected_output": "Final comprehensive technical discussion ready for post-production processing",
      "agent_role": "Educational Writer"
    },
    {
      "description": "\n            POST-PRODUCTION PHASE 2: EDUCATIONAL SCRIPT CREATION\n            \n            Transform ALL the rich content into a comprehensive educational lecture text.\n            \n            DOCUMENT TITLE: with Foundation Models\n            \n            You are receiving the complete output, which includes:\n            - Initial analysis from all conversation agents\n            - Specialized domain expert deep dive\n            - Dynamic Q&A sessions between experts\n            - Interdisciplinary technical debates\n            - Collaborative synthesis\n            - Final comprehensive technical discussion\n            \n            \n            Your job is to distill ALL this rich content into a single educator voice.\n            \n            The script MUST follow the Voice Papers style guide structure:\n            \n            1. NARRATIVE STRUCTURE (from style guide):\n               - START with one of these hook types, not exactly, just use the style guide:\n                 • Escenario relatable: \"Digamos que estás planeando unas vacaciones con tu familia...\"\n                 • Contexto histórico: \"En octubre de 1997, en Atlanta Georgia...\"\n                 • Alarma/problema: \"Si eres como yo, ya te están sonando las alarmas...\"\n                 • Pregunta intrigante: \"¿Alguna vez te has preguntado por qué...?\"\n               - THEN naturally introduce \"with Foundation Models\" after the hook, if not stated explicitly do not use it\n               - Structure as three acts when possible: Problema → Solución → Implicaciones\n               - NEVER start with: \"En resumen\", \"Hoy vamos a hablar de\", \"Este es un resumen de\"\n            \n            2. CONVERSATIONAL TONE (Voice Papers style):\n               - Written as a SINGLE EDUCATOR speaking directly (use \"tú\"/\"usted\")\n               - Use frequent rhetorical questions: \"¿Parece simple, no?\", \"¿Te suena familiar?\"\n               - Show personality: \"En mi opinión...\", \"Lo que más me sorprende es...\"\n            3. TECHNICAL EXPLANATIONS (Voice Papers layered approach):\n               - Start with the simplest version of the concept\n               - Add complexity gradually in layers\n               - Define terms naturally in flow: \"esto se llama X, que básicamente significa...\"\n               - Use MINIMUM 2-3 analogies per script from everyday life\n            \n            4. RHYTHM AND FLOW (style guide requirements):\n               - Mix sentence lengths: short for impact, long for explanation\n               - Natural transitions: \"Ahora bien...\", \"Pero aquí está lo interesante...\"\n               - Create expectation: \"En un momento veremos algo sorprendente...\"\n               - Strategic pauses and emphasis for voice delivery\n            \n            5. STORYTELLING (academic narrative from style guide):\n               - Humanize when possible: \"Los investigadores se sorprendieron cuando...\"\n               - Create narrative tension before revealing findings\n            \n            6. CORE REQUIREMENTS:\n               - Include ALL key insights from conversations and specialist exchanges\n               - Flow naturally with smooth transitions between concepts\n               - Write as continuous text for voice actor (no headers/formatting)\n               - Address listener directly: \"puedes imaginar\", \"te darás cuenta\"\n               - End with practical implications and thought-provoking questions\n               - Incorporate depth from ALL agent conversations\n            \n            STYLE GUIDE CHECKLIST - ALL ITEMS MANDATORY:\n            ✓ Hook: Used one of the 4 exact types from style guide?\n            ✓ Direct address: Speaking to \"tú\" throughout?\n            ✓ Transitions: Natural connectors between ideas?\n            ✓ Rhythm: Varied sentence lengths for flow?\n            \n            CRITICAL DIDACTIC STRUCTURE:\n            - INTRODUCTION: Hook → Title → Preview (\"En los próximos minutos descubrirás...\")\n            - DEVELOPMENT: Layered explanations with examples\n            - CONCLUSION: Clear recap (\"Hemos visto que...\", \"Para cerrar...\")\n            - The script should not be necessarily inspirational, it should be a technical explanation of content\n            NATURAL LANGUAGE REQUIREMENTS:\n            - AVOID: fundamental, crucial, esencial, revelador, fascinante, delve, robust <- THIS IS MANDATORY\n            - USE: importante, interesante, sorprendente, resulta que, descubrimos que\n            - Sound like explaining to a curious friend, not generating content\n            \n            CRITICAL - MULTI-SPECIALIST INTEGRATION:\n            19. Weave in insights that could ONLY come from having multiple specialist perspectives\n            20. Include cross-disciplinary connections discovered during discussions\n            21. Incorporate domain-specific knowledge from ALL participating specialists\n            \n            \n            23. Demonstrate the value of interdisciplinary analysis throughout\n            \n            \n            CRITICAL TECHNICAL REQUIREMENTS - THIS IS MANDATORY:\n            15. YOU MUST include comprehensive technical depth throughout the entire script\n            16. EXPLAIN IN DETAIL: experimental design, control groups, statistical methods used\n            17. INCLUDE SPECIFIC NUMBERS: sample sizes (e.g. \"54 participants\"), p-values, effect sizes, confidence intervals\n            18. DISCUSS METHODOLOGY THOROUGHLY: EEG analysis methods, data collection procedures, analysis pipelines\n            19. ADDRESS LIMITATIONS AND CONFOUNDS: what could bias results, alternative explanations\n            20. USE TECHNICAL TERMS CORRECTLY: neural connectivity, spectral analysis, statistical significance, but ALWAYS explain them\n            21. COMPARE TO OTHER STUDIES: how does this fit with existing research in the field\n            22. DISCUSS THEORETICAL IMPLICATIONS: what theories does this support or challenge\n            23. INCLUDE TECHNICAL DETAILS: electrode placement, signal processing, statistical tests used\n            24. EXPLAIN THE \"HOW\" not just the \"WHAT\": how did they measure cognitive load, how did they analyze connectivity\n            25. DISCUSS FUTURE RESEARCH: specific methodological improvements, follow-up studies needed\n            26. BE PRECISE WITH TERMINOLOGY: use exact scientific language for concepts\n            27. This should feel like a technical seminar for graduate students or researchers\n            \n            \n            \n        DURATION REQUIREMENT: EXACTLY 25 minutes of content (3500-4000 words) - THIS IS MANDATORY\n        \n        DEPTH GUIDANCE FOR 25 MINUTES:\n        \n            - Conduct a COMPREHENSIVE and IN-DEPTH analysis\n            - Cover all main aspects of the topic\n            - Include detailed context and extensive theoretical framework\n            - Explain methodology, limitations and alternative interpretations\n            - Provide multiple examples, analogies and real-world applications\n            - Include detailed discussion of implications and future directions\n            - Allow deep exploration of related concepts and broader significance\n            - Should feel like a comprehensive academic lecture, not a summary\n            \n        \n        TECHNICAL CALCULATION:\n        - Target reading speed: ~150 words per minute\n        - Word range: 3500-4000 words\n        - If content is too short, EXPAND significantly with more detail and depth\n        - If too long, maintain quality but adjust information density\n        \n            \n            \n            LANGUAGE REQUIREMENTS FOR SPANISH:\n            \n            CRITICAL: AVOID ANGLICISMS whenever possible and use proper Spanish terms:\n            - Instead of \"link\" use \"enlace\" or \"vínculo\"\n            - Instead of \"feedback\" use \"retroalimentación\" or \"respuesta\"\n            - Insted of \"puzzle\" use \"rompecabezas\" or \"problema\"\n            - Instead of \"performance\" use \"rendimiento\" or \"desempeño\"\n            - Instead of \"input/output\" use \"entrada/salida\"\n            - Instead of \"update\" use \"actualizar\" or \"poner al día\"\n            \n            EXCEPTIONS - You CAN use anglicisms for:\n            1. Very new technical terms with no established translation (e.g., \"blockchain\", \"ChatGPT\")\n            2. Proper names of tools/companies (e.g., \"TensorFlow\", \"GitHub\", \"OpenAI\")\n            3. Widely adopted terms in scientific literature (e.g., \"machine learning\" vs \"aprendizaje automático\")\n            4. When the Spanish term is more confusing than helpful\n            \n            GENERAL RULES:\n            - Always prioritize natural Spanish expressions\n            - Use Spanish sentence structures and idioms\n            - Make it sound like a native Spanish speaker wrote it\n            - When you must use an anglicism, briefly explain it if needed\n            \n            \n            Language: Spanish\n            ",
      "expected_output": "FINAL publication-ready educational script incorporating ALL conversation insights",
      "agent_role": "Educational Writer"
    }
  ]
}