Section: Section 1
Characters: 9963
==================================================
The section begins by setting the stage for modern AI by emphasizing “scale” as the defining characteristic of AI post-2020. It opens with an observation that today’s massive AI models – those behind applications like ChatGPT, Google’s Gemini, and Midjourney – operate at a scale that demands significant electricity and relies on vast amounts of publicly available internet data. This focus on scale is central to the discussion because it has two critical consequences. First, the power and versatility of these models are increasing at a rapid pace, enabling a broader range of applications that enhance productivity, drive economic value, and improve quality of life. Second, however, the high resource requirements (massive datasets, compute, and specialized talent) mean that only a few organizations can build such models; thus, these organizations provide “models as a service,” lowering the barrier to entry for others who want to build AI applications without creating models from scratch.

The narrative then shifts to contextualize these developments within the broader history of AI and machine learning. The section contrasts the new generation of AI engineering – marked by readily available, large-scale models (termed “foundation models”) – with traditional machine learning, which, while having powered applications such as product recommendations, fraud detection, and churn prediction, did not operate at the same unprecedented scale. This distinction is important because while many principles remain consistent, the advent of foundation models introduces new possibilities and challenges that will be explored further in the book.

The content goes on to lay out the structure of the chapter: an overview of foundation models, examples of successful AI use cases (which highlight current strengths and limitations), and an introduction to the new AI stack. This stack is described in terms of both continuity with traditional methods and the new roles and responsibilities now demanded of AI engineers compared to traditional ML engineers.

A significant portion of the section is devoted to the evolution of language models. It traces the journey from early language models – which encoded statistical information about individual languages – to today’s large language models (LLMs) that rely on self-supervision. The discussion highlights historical milestones, such as how Sherlock Holmes in “The Adventure of the Dancing Men” indirectly used statistical reasoning to decode messages and how Claude Shannon’s work in the early 1950s on language statistics (e.g., entropy) laid the groundwork for modern techniques.

Delving into technical details, the section explains that the basic operational unit in these models is the token—which might represent a character, a word, or part of a word—and the process of breaking text into tokens is called tokenization. For instance, GPT-4 tokenizes “I can’t wait to build AI applications” into nine tokens, with an average token length roughly equivalent to three-quarters of a word. Vocabulary size, which is the complete set of tokens that a model understands, is highlighted with examples (Mixtral 8x7B has 32,000 tokens versus GPT-4’s 100,256 tokens), emphasizing choices made by model developers in balancing efficiency and richness of representation.

The section further distinguishes between two types of language models: masked language models and autoregressive language models. Masked language models (exemplified by BERT) predict missing tokens by considering context from both sides of the gap and are typically used in tasks like sentiment analysis or text classification. In contrast, autoregressive language models (the default focus of the book) predict the next token sequentially from prior tokens, making them better suited for generating open-ended responses and thus more popular for text generation tasks. The explanation underscores that the output generated by these models (referred to as “completions”) is inherently probabilistic – useful for tasks like translation, summarization, and even as reagents for applications like spam detection, yet not infallible.

This comprehensive examination matters because it not only maps out the technical evolution and operational mechanisms of modern AI models but also frames the challenges posed by scale and resource requirements. It sets up the discussion for later chapters that will dive deeper into the nuances of AI engineering and the emerging AI stack, thereby linking historical context to current practice and future possibilities. The text is rich with technical details, historical references, and reflections on the social and economic implications of these models, which in turn highlights both the transformative power and inherent limitations and challenges of deploying such advanced AI systems in real-world applications.