Section: Section 2
Characters: 9933
==================================================
e model might be able to complete it with: Likely spam, which turns this
language model into a spam classifier.
While completion is powerful, completion isn’t the same as engaging in a conversa!
tion. For example, if you ask a completion machine a question, it can complete what
you said by adding another question instead of answering the question. PostTraining on page 78 discusses how to make a model respond appropriately to a user’s
request.
Self-supervision
Language modeling is just one of many ML algorithms. There are also models for
object detection, topic modeling, recommender systems, weather forecasting, stock
price prediction, etc. What’s special about language models that made them the cen!
ter of the scaling approach that caused the ChatGPT moment?
The answer is that language models can be trained using self-supervision, while many
other models require supervision. Supervision refers to the process of training ML
algorithms using labeled data, which can be expensive and slow to obtain. Selfsupervision helps overcome this data labeling bottleneck to create larger datasets for
models to learn from, effectively allowing models to scale up. Here’s how.
With supervision, you label examples to show the behaviors you want the model to
learn, and then train the model on these examples. Once trained, the model can be
applied to new data. For example, to train a fraud detection model, you use examples
of transactions, each labeled with fraud or not fraud. Once the model learns from
these examples, you can use this model to predict whether a transaction is fraudulent.
The success of AI models in the 2010s lay in supervision. The model that started the
deep learning revolution, AlexNet, was supervised. It was
trained to learn how to classify over 1 million images in the dataset ImageNet. It clas!
sified each image into one of 1,000 categories such as car, balloon, or monkey.
Chapter 1: Introduction to Building AI Applications with Foundation Models
The actual data labeling cost varies depending on several factors, including the task’s complexity, the scale
(larger datasets typically result in lower per-sample costs), and the labeling service provider. For example, as
of September 2024, Amazon SageMaker Ground Truth charges 8 cents per image for labeling fewer than
50,000 images, but only 2 cents per image for labeling more than 1 million images.
This is similar to how it’s important for humans to know when to stop talking.
A drawback of supervision is that data labeling is expensive and time-consuming. If it
costs 5 cents for one person to label one image, it’d cost $50,000 to label a million
images for ImageNet.5 If you want two different people to label each image—so that
you could cross-check label quality—it’d cost twice as much. Because the world con!
tains vastly more than 1,000 objects, to expand models capabilities to work with
more objects, you’d need to add labels of more categories. To scale up to 1 million
categories, the labeling cost alone would increase to $50 million.
Labeling everyday objects is something that most people can do without prior train!
ing. Hence, it can be done relatively cheaply. However, not all labeling tasks are that
simple. Generating Latin translations for an English-to-Latin model is more expen!
sive. Labeling whether a CT scan shows signs of cancer would be astronomical.
Self-supervision helps overcome the data labeling bottleneck. In self-supervision,
instead of requiring explicit labels, the model can infer labels from the input data.
Language modeling is self-supervised because each input sequence provides both the
labels (tokens to be predicted) and the contexts the model can use to predict these
labels. For example, the sentence I love street food. gives six training samples, as
shown in Table-1.
Table-1. Training samples from the sentence I love street food. for language modeling.
Input (context)
Output (next token)
BOS
BOS, I
love
BOS, I, love
street
BOS, I, love, street
food
BOS, I, love, street, food
BOS, I, love, street, food,.
EOS
In Table-1, BOS and EOS mark the beginning and the end of a sequence.
These markers are necessary for a language model to work with multiple sequences.
Each marker is typically treated as one special token by the model. The end-ofsequence marker is especially important as it helps language models know when to
end their responses.6
The Rise of AI Engineering
In school, I was taught that model parameters include both model weights and model biases. However, today,
we generally use model weights to refer to all parameters.
It seems counterintuitive that larger models require more training data. If a model is more powerful,
shouldn’t it require fewer examples to learn from? However, we’re not trying to get a large model to match
the performance of a small model using the same data. We’re trying to maximize model performance.
Self-supervision differs from unsupervision. In self-supervised
learning, labels are inferred from the input data. In unsupervised
learning, you don’t need labels at all.
Self-supervised learning means that language models can learn from text sequences
without requiring any labeling. Because text sequences are everywhere—in books,
blog posts, articles, and Reddit comments—it’s possible to construct a massive amount of training data, allowing language models to scale up to become LLMs.
LLM, however, is hardly a scientific term. How large does a language model have to
be to be considered large? What is large today might be considered tiny tomorrow. A
model’s size is typically measured by its number of parameters. A parameter is a vari!
able within an ML model that is updated through the training process.7 In general,
though this is not always true, the more parameters a model has, the greater its
capacity to learn desired behaviors.
When OpenAI’s first generative pre-trained transformer (GPT) model came out in
June 2018, it had 117 million parameters, and that was considered large. In February
2019, when OpenAI introduced GPT-2 with 1.5 billion parameters, 117 million was
downgraded to be considered small. As of the writing of this book, a model with
billion parameters is considered large. Perhaps one day, this size will be considered
small.
Before we move on to the next section, I want to touch on a question that is usually
taken for granted: Why do larger models need more data? Larger models have more
capacity to learn, and, therefore, would need more training data to maximize their
performance.8 You can train a large model on a small dataset too, but it’d be a waste
of compute. You could have achieved similar or better results on this dataset with
smaller models.
From Large Language Models to Foundation Models
While language models are capable of incredible tasks, they are limited to text. As
humans, we perceive the world not just via language but also through vision, hearing,
touch, and more. Being able to process data beyond text is essential for AI to operate
in the real world.
Chapter 1: Introduction to Building AI Applications with Foundation Models
For this reason, language models are being extended to incorporate more data
modalities. GPT-4V and Claude 3 can understand images and texts. Some models
even understand videos, 3D assets, protein structures, and so on. Incorporating more
data modalities into language models makes them even more powerful. OpenAI
noted in their GPT-4V system card in 2023 that incorporating additional modalities
(such as image inputs) into LLMs is viewed by some as a key frontier in AI research
and development.
While many people still call Gemini and GPT-4V LLMs, they’re better characterized
as foundation models. The word foundation signifies both the importance of these
models in AI applications and the fact that they can be built upon for different needs.
Foundation models mark a breakthrough from the traditional structure of AI
research. For a long time, AI research was divided by data modalities. Natural lan!
guage processing (NLP) deals only with text. Computer vision deals only with vision.
Text-only models can be used for tasks such as translation and spam detection.
Image-only models can be used for object detection and image classification. Audioonly models can handle speech recognition (speech-to-text, or STT) and speech syn!
thesis (text-to-speech, or TTS).
A model that can work with more than one data modality is also called a multimodal
model. A generative multimodal model is also called a large multimodal model
(LMM). If a language model generates the next token conditioned on text-only
tokens, a multimodal model generates the next token conditioned on both text and
image tokens, or whichever modalities that the model supports, as shown in
Figure-3.
Figure-3. A multimodal model can generate the next token using information from
both text and visual tokens.
The Rise of AI Engineering
Just like language models, multimodal models need data to scale up. Self-supervision
works for multimodal models too. For example, OpenAI used a variant of selfsupervision called natural language supervision to train their language-image model
CLIP (OpenAI, 2021). Instead of manually generating labels for each image, they
found (image, text) pairs that co-occurred on the internet. They were able to generate
a dataset of 400 million (image, text) pairs, which was 400 times larger than Image!
Net, without manual labeling cost. This dataset enabled CLIP to become the first
model that could generalize to multiple image classification tasks without requiring
additional training.
This book uses the term foundation models to refer to both large
language models and large multimodal models.
Note that CLIP isn’t a generative model—it wasn’t trained to generate open-ended
outputs. CLIP is an embedding model, trained to produce joint embeddings of both
texts and images. Introduction to Embedding on page 134 discusses embeddings in
detail.