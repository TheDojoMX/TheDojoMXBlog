Section: Section 8
Characters: 9876
==================================================
Section 8 of the paper “with Foundation Models” provides a deep dive into the practical nuances of developing AI products built on foundation models and lays out a framework for planning, engineering, and maintaining such systems. It begins by emphasizing the critical need to assess your starting point: evaluating off‐the‐shelf capabilities to determine the additional work required to reach your product’s target functionality. For instance, if the foundation model already handles 30% of a task but your goal is 60%, then the additional engineering required is significantly lower compared to starting from scratch. The section stresses that after such evaluations, goals might need to be adjusted because some projects may prove too resource intensive relative to their return.

A central theme is the “last mile challenge” in AI product development. While building an engaging demo is relatively fast (sometimes over a weekend), transitioning from an initial proof-of-concept to a robust, end-to-end product can take months or even years. Real-world examples are cited: in the UltraChat paper, the jump from 0 to 60 on performance is straightforward, but advancing even the next few percentage points (from 60 to 100) becomes exponentially harder. LinkedIn’s experience is also used to illustrate this phenomenon—achieving initial success took only one month, but refining their product to surpass 95% quality required four additional months. This incremental improvement, often entailing tedious work on product kinks and addressing issues like model hallucinations, is a core insight that product planners must recognize.

Further, the section discusses the rapid evolution of the AI landscape, noting that many positive changes—longer context lengths, improved outputs, and decreasing inference costs—are happening concurrently. Figure-11 illustrates how inference costs have rapidly dropped over time alongside performance improvements on benchmarks like MMLU. Yet, these advances also bring friction: the dynamic nature of model performance means that a technology investment may quickly be outmatched by its competitors. This challenges developers not only to continuously run cost-benefit analyses on technological choices but also to remain agile in adapting to rapidly shifting APIs, toolchains, and regulatory environments.

Regulatory and operational challenges receive substantial attention. The authors warn that regulations, such as GDPR or national security mandates impacting compute resources, can impose sudden barriers. There is also a discussion about intellectual property concerns: if a product is built on models trained with third-party data, it raises questions regarding long-term ownership of the resulting IP. These legal and compliance risks underscore the importance of thorough planning and establishing infrastructure for versioning and evaluation.

The latter part of the section introduces the “AI Engineering Stack,” a conceptual framework for understanding the building blocks of modern AI applications. This stack comprises three layers:

1. Application Development – This is where accessible, off-the-shelf foundation models are employed to create applications. Emphasis is placed on designing effective prompts, context provision, and user interfaces, along with a need for rigorous evaluation of the application’s performance.

2. Model Development – This layer focuses on the tooling required for model training, fine-tuning, and inference optimization, including dataset engineering. Despite foundation models offering impressive base capabilities, fine-tuning and experimentation (with models, prompts, sampling variables, etc.) remain essential to tailor them to specific use cases.

3. Infrastructure – The foundational layer entails managing compute resources, model serving, data management, and monitoring. The argument is made that while model and application layers have evolved with the advent of foundation models, the core infrastructural needs remain consistent with traditional practices.

Evidence of the rapid evolution in the community is drawn from an analysis of GitHub repositories sharing AI-related projects (each with at least 500 stars). The cumulative growth—illustrated in Figure-15—shows that since the introduction of models like Stable Diffusion and ChatGPT, there has been a pronounced surge in repositories focused on applications and application development, even though the broader infrastructure layer has grown more modestly. This trend supports the idea that while the ecosystem remains vibrant and innovative, the underlying engineering challenges and principles continue to align with traditional ML engineering despite the new opportunities presented by foundation models.

Overall, the insights and findings from this section highlight several significant implications and debates:
• The initial allure of foundation models is tempered by the reality that a “cool demo” is only the beginning—substantial, often painstaking work is required to convert demos into viable, market-ready products.
• Continuous innovation in AI, including faster inference, better outputs, and evolving APIs, means that products must be designed with a future-proof mindset. Developers need to plan for ongoing maintenance and rapid shifts in technology.
• The potential benefits of using foundation models (reduced development time, powerful off-the-shelf capabilities, and easier integration of multimodal data) are counterbalanced by new challenges, such as regulatory compliance, shifting market dynamics, and the risk that underlying model improvements could render custom-built layers obsolete.
• From an engineering perspective, the distinction and overlap between AI engineering and traditional ML engineering are clarified. While many companies merge these roles, the emergence of specialized AI engineering job titles reflects the need for new skills in adapting and fine-tuning foundation models.
• The discussion also reinforces that systematic experimentation—whether with hyperparameters in traditional ML or with prompts, retrieval algorithms, and sampling variables in foundation models—is key to optimizing performance while ensuring cost and latency targets are met.

In summary, Section 8 not only maps out practical advice on evaluating and planning AI projects but also situates these challenges within a broader context of fast-paced technological change and evolving regulatory landscapes. By detailing the layered engineering stack, providing empirical examples from industry experiences, and carefully weighing the trade-offs inherent in using foundation models, the section offers a comprehensive and nuanced roadmap for successfully building and maintaining AI applications in a rapidly evolving landscape.