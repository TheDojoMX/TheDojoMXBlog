Section: Section 4
Characters: 9999
==================================================
Below is the comprehensive extraction and analysis of Section 4, preserving every detail from the actual content while explaining the reasoning, evidence, and significance behind each part.

─────────────────────────────  
Complete Content of Section 4:  

"le. (29:00) What would you do differently 1947 1940 or what woul  
d Kissinger do different 1947 1948 1949 than what we did do? You know I I wrote  
two books with Dr. Kissinger and I miss him very much. He was my closest friend.  
Um and Henry was very much a realist in the sense that when you look at his his  
tory in uh roughly 36 38 he and his uh I guess 37 38 his family were were Jewish  
were forced to immigrate from uh Germany because of the Nazis and he watched th  
e entire world that he'd grown up with as a boy be destroyed by the Nazis and by  
Hitler and then he saw the confilgration that occurred as a result and I tell y  
ou that whether you like him or not, he spent the rest of (29:42) his life tryin  
g to prevent that from happening again. Mhm. So we we are today safe because peo  
ple like Henry saw the world fall apart. Mhm. So I think from my perspective, we  
should be very careful in our language and our strategy to not start that proce  
ss. Henry's view on China was different from other China scholars. (30:09) His v  
iew was in China was that we shouldn't poke the bear, that we shouldn't talk abo  
ut Taiwan too much and we let China deal with our own problems which were very s  
ignificant. But he was worried that we or China in a small way would start World  
War II in the same way that World War I was started. You remember that World Wa  
r One one, World War I started with a essentially a small geopolitical event whi  
ch was quickly escalated for political reasons on on all sides and then the rest  
was a horrific war, the war to end all wars at the time. (30:38) So we have to  
be very very careful when we have these conversations not to isolate each other.  
Um Henry started a number of what are called track two dialogues which I'm part  
of one of them to try to make sure we're talking to each other. And so somebody  
who's a a hardcore person would say, well, you know, we're Americans and we're  
better and so forth. (30:58) Well, I can tell you having spent lots of time on t  
his, the Chinese are very smart, very care capable, very much up here. And if yo  
u're confused about that, again, look at the arrival of Deep Seek. A year ago, I  
said they were two years behind. I was clearly wrong. With enough money and eno  
ugh power, they're in the game. Yeah. (31:21) Let me actually drill in just a li  
ttle bit more on that too because I think um one of the reasons deep sea caught  
up so quickly is because it turned out that inference time generates a lot of IQ  
and I don't think anyone saw that coming and inference time is a lot easier to  
catch up on and also if you take one of our big open source models and distill i  
t and then make it a specialist like you were saying a minute ago and then you p  
ut a ton of infra time compute behind it, it's a massive advantage and also a ma  
massive leak of capability within CBRN for example that nobody anticipated and  
CBNN remember is chemical, biological, radiological and nuclear. Um (31:57) let  
me rephrase what you said. If the structure of the world in 5 to 10 years is 10  
models and I'll make some numbers up. Five in the United States, three in China,  
two elsewhere. And those models are data centers that are multi- gigawatts. The  
y will be all nationalized in some way. In China, they will be owned by the gove  
rnment. Mhm. The stakes are too high. (32:28) Mhm. Um, one in my military work o  
ne day I visited a place where we keep our plutonium and we keep our plutonium i  
n in a base that's inside of another base with even more machine guns and even m  
ore specialized because the plutonium is so is so interesting and and obviously  
very dangerous and I believe it's the only one or two facilities that we have in  
America. (32:53) So in that scenario, these data centers will have the equivale  
nt of guards and machine guns because they're so important. Now is that a stable  
geopolitical system? Absolutely. You know where they are. President of one coun  
try can call the other. They can have a conversation. You know, they can agree o  
n what they agree on and so forth. But let's say the it is not true. (33:19) Let  
's say that the technology improves again unknown to the point where the kind of  
technologies that I'm describing are implementable on the equivalent of a small  
server then you have a humongous data center proliferation problem and that's w  
here the open-source issue is so important because those servers which will be p  
roliferate throughout the world will all be on open source. (33:38) We have no c  
ontrol regime for that. Now, I'm in favor of open source as you mentioned earlie  
r with Mark Andre u uh that open competition and so forth tends to allow people  
to run ahead in defense of the proprietary companies. Collectively, they believe  
as best I can tell that the open- source models can't scale fast enough because  
they need this heavyweight training. (34:06) If you look, I I'll give you an ex  
ample of Grock is trained on a single cluster that was built by Nvidia in 20 day  
s or so forth in Memphis, Tennessee of 200,000 GPUs. Um GPU is about $50,000. Yo  
u can say it's about a $10 billion supercomput in one building that does one thi  
ng, right? If that is the future, then we're okay because we'll be able to know  
where they are. Yeah. (34:34) If in fact the arrival of intelligence is ultimate  
ly a a distributed problem, then we're going to have lots of problems with terro  
rism, bad actors, North Korea poorly, which is my which is my greatest concern.  
Right. China and the US are rational actors. Yeah. Uh the terrorist who has acce  
ss to this and I I don't want to go all negative on this on this podcast. It's i  
t's an important thing to wake people up to the deep thinking you've done on thi  
s. (34:59) Um my concern is is the terrorist who gains access and are we spendin  
g enough time and energy and are we training enough models to watch them. So the  
first the companies are doing this there are there's a body of work happening n  
ow which can be understood as follows. You have a super intelligent model. Can y  
ou build a model that's not as smart as the student that's studying? You know, t  
here is a professor that's watching the student, but the student is smarter than  
the professor. Is it possible to watch what (35:36) it does? It appears that we can.  
It appears that there's a way even if you have a this rogue incredible thi  
ng, we can watch it and understand what it's doing and thereby control it. Anoth  
er example of the of where where we don't know is that it's very clear that thes  
e savant models will proceed. (35:57) There's no question about that. The questi  
on is how do we get the Einsteins? So there are two possibilities. One and this  
is to discover completely new schools of thought which is what's the most exciti  
ng thing. Yeah. And in our book Genesis, Henry and I and Craig talk about the im  
portance of polymaths in history. In fact, the first chapter is on polymaths. (3  
6:23) What happens when we have millions and millions of polymaths? Very, very i  
nteresting. Okay. Now, it looks like the great discoveries, the greatest scienti  
sts and people in our history had the following property. They were experts in s  
omething and they looked at some at a different problem and they saw a pattern i  
n one area of thinking that they could apply to a completely unrelated field and  
they were able to do so and make a huge breakthrough. The models today are not  
able to do that. (36:57) So one thing to watch for is algorithmically when can t  
hey do that? This is generally known as the non-stationerity problem. Yeah. beca  
use uh the reward functions in these models are fairly straightforward. You know  
, beat the human, beat the question and so forth. (37:17) But when the rules kee  
p changing, is it possible to say the old rule can be applied to a new rule to d  
iscover something new? And and again, the research is underway. We won't know fo  
r years. Peter and I were over at OpenAI yesterday, actually, and we were talkin  
g to many people, but Noan Brown in particular, and um I said the word of the ye  
ar is scaffolding. (37:34) And he said, "Yeah, maybe the word of the month is sc  
affolding." I was like, "Okay, what did I step on there?" He said, "Look, you kn  
ow, right now, if you try to get the AI to discover relativity or, you know, jus  
t some green field opportunity, it won't it won't do it. If you set up a framewo  
rk kind of like a lattice, like a trellis, the vine will grow on the trellis bea  
utifully, but you have to lay out those pathways and breadcrumbs. (37:58) " He w  
as saying the AI's ability to generate its own scaffolding is imminent. Mhm. Tha  
t doesn't make it completely self-improving. It's not it's not Pandora's box, bu  
t it's also much deeper down the path of create an entire breakthrough in physic  
s or create an entire feature length movie or you know these these prompts that  
require 20 hours of consecutive inference time compute pretty much sure that tha  
t will be a 2025 thing at least from from their point of view. (38:29) So, uh, r  
ecursive self-improvement is the general term for the computer continuing to lea  
rn. Yeah, we've already crossed that in the sense that these systems are now run  
ning and learning things and they're learning from the way they own they think w  
ithin limited functions. When does the system have the ability to generate its o  
wn objective and its own question? Does not have that today. Yep. That's another  
sign. (38:57) Another sign would be that the system decides to uh exfiltrate it  
self and it takes steps to get it get itself away from the commander the control  
and command system. Um that has not happened yet. Jim and I hasn't called you y  
et and said, "Hi, Eric. (39:17) Can I but but there there are theoreticians who  
believe that the that the systems will ultimately choose that as a reward functi  
on because they're programmed to, you know, to continue to learn." Uh, another o  
ne is access to weapons, right? And lying to get it."

─────────────────────────────  
Analysis and Comprehensive Insights:

1. Geopolitical and Historical Perspective:  
   The section opens with a reflection on Henry Kissinger’s life and his pragmatic, realist outlook shaped by his experiences during the Nazi era. This historical context is used to caution against repeating past geopolitical mistakes. The speaker underlines that Kissinger’s lifelong efforts to prevent global catastrophes inform today’s need for careful, measured language and strategy—especially regarding sensitive issues like the status of Taiwan and relations with China. The point is that the consequences of past conflicts can serve as a lesson for how modern technological and geopolitical risks should be managed.

2. Caution in Diplomacy and Communication:  
   By referencing track two dialogues and emphasizing open lines of communication—even with adversaries—the speaker stresses that isolating nations or adopting overly aggressive rhetoric could inadvertently escalate tensions reminiscent of the lead-up to major global conflicts like World War I. This argument is grounded in historical analogy and serves as a warning to maintain dialogue to avoid catastrophic escalation.

3. Technological Race, Energy, and National Security:  
   The narrative then shifts focus to the rapid advancements in AI technology. The discussion makes a compelling point: while much attention is given to the raw computational power (e.g., the example of hundreds of thousands of GPUs and multi-gigawatt data centers), there is an equally critical risk regarding the proliferation of technology. When breakthroughs like “inference time” gains are combined with distillation of open source models, it enables not only faster catch-up by competitors (such as Deep Seek and Chinese tech) but also risks disseminating powerful capabilities beyond regulated channels. The analogy drawn with guarding plutonium in highly secured facilities illustrates the magnitude of the stakes—these data centers, as the future “superbrains,” will be as strategically sensitive as nuclear materials.

4. Open Source Versus Nationalized Control:  
   A key insight is that, in the near future, there might be a small number of major AI models—five in the US, three in China, and two elsewhere—all hosted in massive data centers which are likely to be subject to national control. The conversation foresees a scenario where open source and faster “inference time” updates could allow relatively smaller systems or even distributed networks (small servers) to emerge, which might undermine the ability for central control and raise accountability and proliferation concerns.

5. The Promise and Peril of Recursive Self-Improvement and Polymathy:  
   The speaker introduces the concept of recursive self-improvement in AI—a situation where systems learn and optimize their own functioning. While current models show signs of learning within set boundaries, they have yet to achieve the ability to set their own objectives or “exfiltrate” from control. This is coupled with the notion of polymaths: historically, breakthroughs occurred when experts applied insights from one domain to another. Today’s AI, however, lacks this cross-domain creative spark. The potential for future models to generate their own “scaffolding” and create breakthroughs (as hinted with a 2025 timeframe) points to both immense opportunity and major unknown risks, particularly in overcoming what is termed the non-stationarity problem—the challenge of applying static reward functions to a changing environment.

6. Implications and Future Risks:  
   Overall, the section underscores that the coming era of digital superintelligence is fraught with dual-edged possibilities. On the one hand, the efficient scaling of computation and the ability to rapidly update models promise profound advancements. On the other hand, they raise pressing issues of proliferation (especially through open source channels), national security risks (with comparisons to nuclear safeguards), and unforeseen challenges relating to autonomous decision-making in these systems. The debate is not just technical but also deeply geopolitical, with profound implications for how societies manage strategic competition and global stability.

─────────────────────────────  
In summary, Section 4 provides a nuanced discussion that intertwines historical lessons with modern technological challenges. It links Kissinger’s realist worldview to the way we must approach emerging AI capabilities—a technology that could soon mimic the distributed, high-capacity nature of national defense infrastructures. The detailed technical discussion about inference time advantages, the potential for decentralized but powerful AI through open source models, and the challenges of ensuring these systems remain under accountable control all underscore the urgent need to balance rapid innovation with robust safeguards to prevent both geopolitical escalation and unintended proliferation. This synthesis of historical perspective, technological insight, and strategic warning offers a multilayered view of the trajectory towards digital superintelligence and its wide-ranging consequences.