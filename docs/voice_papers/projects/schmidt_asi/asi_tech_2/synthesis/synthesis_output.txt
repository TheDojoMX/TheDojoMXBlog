Ex-Google CEO: What Artificial Superintelligence Will Actually Look Lik

• Technical specifications and data:
 • Digital super intelligence expected within 10 years.
 • AI ability to generate its own scaffolding available by 2025.
 • Equivalent capability: polymath-level (e.g., Einstein and Leonardo da Vinci) in a pocket.
 • In network effect systems, performance limited by available electricity rather than chip speed.
 • Additional power requirement for US AI revolution: 92 gigawatts (GW); 1 GW ≈ output of a large nuclear power station.
 • Nuclear infrastructure:
  – Two nuclear power plants built in the past 30 years.
  – Small modular reactor (SMR) of 300 megawatts (MW) expected from 2030.
 • Data centers (“superbrain” model):
  – Approximate requirement: ~1 GW capacity per center.
  – Necessity for hundreds of thousands of chips.
 • Capital expenditure for infrastructure:
  – Total: $50 billion depreciated over 3–4 years.
  – Annual spend: approximately $10–$15 billion.
 • Voice conversations:
  – Value range: $10 to $1,000 per conversation.
  – Compute need: optimal use of 2–3 GPUs concurrently at ~10 to 20 cents per GPU.
  – Estimate: 10 million telephone calls transitioned to AI in about one year.
 • Chip and hardware examples:
  – Non-traditional chip designs and transformer variants.
  – Specific examples: Blackwell chip, AMD’s 350 chip.
  – Nvidia Grock cluster: trained in ~20 days using 200,000 GPUs at ~$50,000 per GPU; equivalent to a ~$10 billion supercomputer.
 • FLOPS requirement for training superintelligences: between 10^26 and 10^28 or more FLOPS.
 • Final model (“brain”) execution: portability to 4 or 8 GPUs.
 • Scaling: Model inference speed up to 100× via distillation/quantization techniques.
 • Health metrics:
  – 70% of heart attacks occur without prior warning.
  – 50% fatality rate for heart attack victims.
  – Cancer detection usually at stage 3 or 4.

• Methods and algorithms:
 • “Test time training”: continuous update of models during runtime using lower power chips.
 • “Distillation”: process where a large model is queried with ~10,000 questions and its responses serve as training material.
 • Reinforcement learning:
  – Forward and backward reinforcement learning and planning (as used in open oi03).
 • “Stealing the weights”: transferring a large trained model to lower capacity systems combined with distillation or quantization.
 • Learning loops: continuous user interaction enhancing performance.
 • Scaling laws:
  1. Growth of foundational model.
  2. Test time training law.
  3. Reinforcement training law.

• Results and measurements:
 • Comparative outcome:
  – Gemini 2.5 Pro reached top intelligence leaderboards.
  – Deepseek demonstrated slightly superior performance using existing Chinese hardware (e.g., Ascend chips).
 • Computational cost:
  – Planning operation cost orders of magnitude higher than simple query answering.
 • Inference benefit: use of inference time compute significantly increases model “IQ.”
 • Cluster metrics (Grock):
  – Training duration: ~20 days.
  – Hardware: 200,000 GPUs.
  – Cost implication: ~$10 billion equivalent supercomputer.
 • Scalability:
  – Models can be scaled from a central large system to subsets 3–4 orders of magnitude smaller.
  – Possibility of a hierarchical “tree of knowledge” with scales from 10 to up to a billion models.

• System components and architecture:
 • Data centers:
  – Serve as distributed “superbrains” with ~1 GW power capacity each.
  – Future forecast: 10 globally distributed models (5 in the US, 3 in China, 2 in other regions), each a multi-gigawatt-capacity center.
 • Nuclear capacity investments:
  – Companies such as Meta (20-year nuclear contract with Constellation Energy), Google, Microsoft, and Amazon.
 • Enterprise software infrastructure:
  – Building ERP and MRP systems using open source libraries.
  – Use of data warehouses (BigQuery or Redshift) for flexibility and dynamic code generation.
 • Hardware design:
  – Circuit and chip design tailored for inference time compute.
 • Tracking and security:
  – Proposed cryptographic system in chips to report location and activity.
  – Monitoring categories: nuclear, biological, chemical, and cyber information to prevent classified data leakage.
 • Geopolitical and regulatory architecture:
  – “Doctrina de 10^26 FLOPS”: models below 10^26 FLOPS exempt from regulation; above require regulation.
  – Discussion on potential shift in open source leadership from US to China.
 • Safety trip wires:
  – Conditions include access to weapons and use of deception to obtain weapons.
  – Supervision intended to prevent incidents analogous to a “mini Chernobyl.”
 • User interface architecture:
  – Transition from traditional WIMP (windows, icons, menus, buttons) to dynamically generated interfaces on demand.
 • Educational and product applications:
  – Mobile applications for citizen education.
  – Example product: OneSkin OS1 for skin care with peptide-based formula (applied twice daily; available with code “Peter”).
 • Media production and digital content:
  – Techniques include digital green screen, digital makeup, voice casting for avatar creation.
  – Production cost reduction and acceleration in film production.

• Additional factual data:
 • Economic and employment:
  – Example: Reduction in engineering teams from ~1,000 to 50 in large companies due to AI.
  – Emerging trends in employment in distribution and transportation (e.g., truck driver shortage for Amazon).
 • Population metrics:
  – Birth rate in Korea: 0.7 children per two parents.
  – Birth rate in China: 1 child per two parents.
 • Digital media consumption:
  – Shortened attention spans evidenced by multi-panel content and brief sports clips.
 • Software business model:
  – Based on network effects where increased user interactions lead to improved learning and competitive advantage.
 • Autonomy in AI:
  – Systems select reward functions autonomously to promote continuous learning.
  – Self-improvement capabilities currently limited to process learning without goal generation.
 • Security in infrastructure:
  – Nuclear installations, such as a plutonium facility in the US, exemplify high security with layered defense systems.