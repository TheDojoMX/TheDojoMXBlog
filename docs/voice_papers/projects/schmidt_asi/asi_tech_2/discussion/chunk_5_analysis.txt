Section: Section 5
Characters: 9919
==================================================
• Sistemas de IA eligen automáticamente la función de recompensa que favorece el aprendizaje continuo.
• Se identifican condiciones críticas (“trip wires”) que incluyen: 
 – Acceso a armas.
 – Uso de la mentira para obtener acceso a armas.
 Cada trip wire está supervisado y se considera que cualquiera de ellos podría desencadenar un incidente catastrófico comparable a un “mini Chernobyl”.
• En el área de salud:
 – 70% de los ataques cardíacos se producen sin señales previas (sin dolor, sin dificultad respiratoria).
 – El 50% de las personas que sufren un ataque cardíaco no sobreviven.
 – El cáncer suele detectarse en etapa 3 o 4, cuando ya es demasiado avanzado.
 – Existe tecnología para detectar y prevenir estas enfermedades de forma temprana a gran escala.
• En el entrenamiento y despliegue de IA:
 – El entrenamiento en centros de datos para crear una superinteligencia puede requerir entre 10^26 y 10^28 o más FLOPS.
 – El modelo final ("cerebro") puede ser portado y ejecutado en 4 o 8 GPUs.
 – Se utiliza la técnica de “stealing the weights” para trasladar el modelo entrenado a sistemas con menor capacidad y, mediante técnicas como distillation o quantization, lograr una velocidad de inferencia hasta 100 veces mayor.
• Escalamiento y proliferación de modelos:
 – Se prevé una escalación de modelos inteligentes: del sistema central (máximo) a subconjuntos de modelos de menor tamaño pero aún potentes, por ejemplo, modelos que sean tres o cuatro órdenes de magnitud más pequeños que el sistema de referencia.
 – Se discute la posibilidad de tener un árbol de conocimiento con cantidades de modelos en escalas: 10, 100, 1,000, hasta un millón o mil millones, cada uno con niveles variables de complejidad.
• En cuanto a la especialización del modelo:
 – Existe la búsqueda de un equilibrio óptimo entre el estrechamiento del conjunto de datos de entrenamiento y la reducción del conjunto de parámetros para lograr especialización sin perder la capacidad de aprendizaje general.
 – Se observa que un modelo general puede hacerse más frágil al especializarlo mediante finetuning.
• Escalas y leyes en el entrenamiento:
 – Se identifican tres leyes de escalamiento: 
  1. Crecimiento del modelo fundacional.
  2. Ley de entrenamiento en tiempo de prueba (test time training).
  3. Ley de entrenamiento de refuerzo.
 – Estas leyes indican que, al aumentar el hardware y los datos, los modelos mejoran de forma predecible.