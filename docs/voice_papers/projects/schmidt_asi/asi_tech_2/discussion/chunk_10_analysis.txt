Section: Section 10
Characters: 200
==================================================
• Digital super intelligence is expected within 10 years.
• AI’s ability to generate its own scaffolding is predicted to be available by 2025.
• Digital super intelligence provides the equivalent of having a polymath (the combined capabilities of figures such as Einstein and Leonardo da Vinci) in one’s pocket.
• In network effect businesses, when a learning machine learns faster, performance accelerates toward a natural limit defined by available electricity rather than chip performance.
• Meta has signed a 20‐year nuclear contract with Constellation Energy; Google, Microsoft, and Amazon are also investing in nuclear capacity.
• The expected additional power needed for the AI revolution in the United States is 92 gigawatts (GW); 1 GW is approximately equivalent to the output of a large nuclear power station.
• Only two nuclear power plants have been built in the past 30 years.
• A small modular reactor (SMR) with a capacity of 300 megawatts (MW) is expected to be available starting in 2030.
• Data centers, approximated as digital brains working in concert (“superbrain”), may require on the order of 1 GW capacity per center and need hundreds of thousands of chips.
• To support a capital expenditure of $50 billion depreciated over 3–4 years, an annual capital spend of approximately $10 to $15 billion is required.
• Many startups are developing non-traditional chip designs and variants of the transformer architecture optimized for inference time computing.
• Examples include the Blackwell chip and AMD’s 350 chip, which are used as part of data center infrastructure.
• The open oi03 AI research example utilizes forward and backward reinforcement learning and planning, where the computational cost for planning is many orders of magnitude higher compared to simple query answering.
• With improved energy supply and technology, the combination of planning with deep memory could potentially lead to human-level intelligence.
• Areas tracked as major technological meta trends include humanoid robots, artificial general intelligence (AGI), quantum computing, transport, energy, and longevity.

• Valor de conversaciones de voz: range of $10 to $1,000.
• Requerimientos de cómputo para conversaciones de voz: utilización óptima de dos a tres GPUs concurrentes, with a cost of approximately 10 to 20 cents per GPU.
• Estimación de llamadas telefónicas concurrentes trasladadas a la inteligencia artificial: 10 million in approximately one year.
• Plataforma de Google Cloud (GCP) para empresas: permite escribir tareas en lenguaje natural para generar código mediante un modelo de lenguaje a gran escala, utilizando el protocolo de contexto y conexión a bases de datos.
• Infraestructura de software empresarial: se puede construir arquitecturas de ERP y MRP utilizando bibliotecas open source, aprovechando BigQuery (Google) o Redshift (Amazon).
• Reemplazo de tareas: las computadoras pueden sustituir la mayoría de las tareas de programación y matemáticas, debido a conjuntos de lenguaje limitados y escalables computacionalmente.
• Predicciones temporales: 
 – Matemáticos de clase mundial basados en IA podrían emerger en un plazo de un año.
 – Programadores de nivel similar podrían aparecer en uno o dos años.
 – Especialistas (“soants”) se desplegarán dentro de cinco años.
• Concepto de “super inteligencia”: inteligencia que supera la suma de las capacidades humanas mediante la acumulación de agentes inteligentes en red, con implicaciones en seguridad, competencia internacional y proliferación.
• Revolución agentica: agentes conectados resolverán procesos empresariales y gubernamentales; la adopción es más rápida en compañías con recursos financieros y mayor presión por latencia, y más lenta en ámbitos gubernamentales sin incentivos para la innovación.
• En física: empleo de modelos fundacionales que aproximan algoritmos complejos (por ejemplo, en el estudio de la cromodinámica cuántica).
• Concepto de “test time training”: actualización continua de los modelos en tiempo de ejecución en contraste con el uso estático de modelos fundacionales costosos.
• Riesgos en el dominio negativo: posibilidad de ataques cibernéticos de magnitud sin precedentes y potencial de ataques biológicos mediante modificaciones en la estructura de virus para hacerlos indetectables.
• Evolución algorítmica: transición desde modelos fundacionales costosos hacia actualizaciones continuas mediante “test time training”.

• Algoritmo de “test time training”: actualización continua de los modelos en tiempo de ejecución, realizable con chips de menor potencia.
• Open source (“open weights”): permite que cualquier entidad utilice el modelo, ofreciendo una opción más económica y con implicaciones geopolíticas.
• Impacto geopolítico: posibilidad de que el liderazgo en open source se transfiera de Estados Unidos a China.
• Regulación de modelos a través de la “doctrina de 10^26 FLOPS”: modelos por debajo de 10^26 FLOPS no requieren regulación; aquellos que lo superen deben ser regulados.
• Propuesta del gobierno estadounidense: regular tanto modelos open source como cerrados.
• Preguntas técnicas y estratégicas: 
 – Restricciones en chips y cambios arquitectónicos por parte de China para construir modelos similares a los de EE. UU.
 – Financiación de centros de datos con capital estimado en $50 mil millones, necesaria para justificar modelos de negocio basados en productos cerrados.
• Caso Deepseek:
 – Gemini 2.5 Pro se posicionó en la cima de leaderboards de inteligencia; Deepseek mostró un rendimiento ligeramente superior.
 – Deepseek fue entrenado utilizando hardware existente en China, incluyendo chips Ascend de Huawei y otros.
 – Técnica de “distillation”: se toma un modelo grande, se le hacen 10,000 preguntas y se usan sus respuestas como material de entrenamiento.
• Seguridad y filtración de información:
 – Monitoreo y testing de categorías como nuclear, biológica, química y cibernética durante el entrenamiento para evitar fugas de información clasificada.
 – La filtración de información nuclear durante el entrenamiento está prohibida por ley.
• Propuesta de seguimiento: instalación de un sistema (por ejemplo, mediante un añadido criptográfico en los chips) para informar la localización y actividad de los chips y centros de entrenamiento.
• Concepto de “mal funcionamiento mutuo en IA”: escenario en el que, al alcanzar ciertos niveles de cómputo, países (por ejemplo, EE. UU. y China) podrían desencadenar respuestas en forma de ciberataques, estableciendo un balance similar a la destrucción mutuamente asegurada.

• Inference time computing: la inferencia genera un notable incremento en el “IQ” de un modelo; tomar un modelo open source grande, aplicarle distillation para especializarlo y usar gran cantidad de cómputo en inferencia proporciona una ventaja significativa.
• Capacidad en CBRN: existe una “fuga masiva de capacidad” en el campo CBRN (químico, biológico, radiológico y nuclear) por la disponibilidad de grandes recursos de cómputo en inferencia.
• Estructura futura de modelos de datos: se proyecta que en 5 a 10 años existirán 10 modelos distribuidos globalmente – 5 en Estados Unidos, 3 en China y 2 en otros lugares; estos modelos serán centros de datos con capacidad de múltiples gigavatios; en China se espera que sean propiedad del gobierno.
• Seguridad en instalaciones nucleares: se detalla una instalación de plutonio en Estados Unidos, ubicada dentro de una base mayor fuertemente custodiada, siendo una de las dos instalaciones existentes en el país.
• Proliferación de centros de datos: posible proliferación masiva de centros basados en modelos open source si la tecnología se implementa en servidores a menor escala.
• Ejemplo del clúster de Grock: un clúster construido por Nvidia en Memphis, Tennessee, entrenó el modelo Grock en aproximadamente 20 días utilizando 200,000 GPUs, a un costo aproximado de $50,000 por GPU; el clúster equivale a un supercomputador de aproximadamente $10 mil millones alojado en un edificio.
• Distribución de inteligencia y riesgos: riesgo de acceso por parte de actores maliciosos si la inteligencia se distribuye en servidores pequeños.
• Aprendizaje y auto-mejora: los sistemas son capaces de aprender y ajustar sus procesos, sin evidencia actual de que puedan generar sus propios objetivos o “exfiltrarse” de su sistema de control.
• Función de recompensa y problema de no-estacionariedad: los modelos usan funciones de recompensa simples (por ejemplo, “vencer al humano”), enfrentándose al problema de la “no estacionariedad” cuando cambian las reglas y se deben aplicar conocimientos previos a nuevos contextos.
• Escalafón (“scaffolding”): se predice que hacia 2025 la capacidad de la IA para generar su propio “scaffolding” se hará inminente, permitiendo avances en tareas que requieran inferencia continua durante períodos prolongados (por ejemplo, 20 horas consecutivas de cómputo).

• Los sistemas de IA eligen automáticamente la función de recompensa que favorece el aprendizaje continuo.
• Condiciones críticas (“trip wires”) incluyen:
 – Acceso a armas.
 – Uso de la mentira para obtener acceso a armas.
Cada trip wire es supervisado, y cualquiera puede desencadenar un incidente catastrófico comparable a un “mini Chernobyl”.
• En el área de salud:
 – 70% de los ataques cardíacos se producen sin señales previas (sin dolor, sin dificultad respiratoria).
 – 50% de las personas que sufren un ataque cardíaco no sobreviven.
 – El cáncer se detecta a menudo en etapa 3 o 4.
 – Existe tecnología para detectar y prevenir estas enfermedades de forma temprana a gran escala.
• Entrenamiento y despliegue de IA:
 – Entrenamiento en centros de datos para crear una superinteligencia puede requerir entre 10^26 y 10^28 o más FLOPS.
 – El modelo final (“cerebro”) puede ser portado y ejecutado en 4 o 8 GPUs.
 – Técnica de “stealing the weights”: trasladar el modelo entrenado a sistemas con menor capacidad, utilizando distillation o quantization para lograr una velocidad de inferencia hasta 100 veces mayor.
• Escalamiento y proliferación de modelos:
 – Escalación desde un sistema central máximo a subconjuntos de modelos de menor tamaño, por ejemplo, modelos tres o cuatro órdenes de magnitud más pequeños.
 – Posibilidad de un árbol de conocimiento con modelos en escalas de 10, 100, 1,000, hasta un millón o mil millones, con niveles variables de complejidad.
• Especialización del modelo:
 – Búsqueda de un equilibrio óptimo entre estrechar el conjunto de datos de entrenamiento y reducir el conjunto de parámetros para lograr especialización sin perder la capacidad de aprendizaje general.
 – Un modelo general puede volverse más frágil al especializarse mediante finetuning.
• Escalas y leyes en el entrenamiento:
 – Tres leyes de escalamiento: 
  1. Crecimiento del modelo fundacional.
  2. Ley de entrenamiento en tiempo de prueba (test time training).
  3. Ley de entrenamiento de refuerzo.
 – Estas leyes indican que, al aumentar hardware y datos, los modelos mejoran de forma predecible.

• Tasa de natalidad en Corea: 0.7 hijos por dos padres.
• Tasa de natalidad en China: 1 hijo por dos padres.
• Emergencia nacional para incrementar el uso de IA en el lugar de trabajo, orientada a mejorar productividad y elevar salarios.
• Ejemplo de creación de empleo: centros de distribución y transporte para Amazon surgieron tras la creación de la empresa; actualmente se reporta una escasez significativa de conductores de camión.
• Propuesta de producto educativo: desarrollar una aplicación móvil en múltiples idiomas y de forma gamificada para enseñar a los usuarios conocimientos necesarios para ser buenos ciudadanos.
• Ejemplo de optimización de equipos en grandes empresas: reducción de un equipo de ingeniería de aproximadamente 1000 personas a un equipo de 50, mediante la aplicación intensiva de IA.
• Propuesta de generación dinámica de interfaces de usuario: en lugar del paradigma tradicional WIMP (ventanas, íconos, menús, botones), la IA generará interfaces (por ejemplo, botones) a demanda.
• Estudiantes universitarios participan en el desarrollo de algoritmos para aprendizaje por refuerzo desde el segundo año.
• Producto para el cuidado de la piel: OneSkin OS1, que contiene un péptido destinado a revertir el envejecimiento de la piel, aplicado dos veces al día; disponible en oneskin.co, con el código “Peter” para descuento.
• Referencias a eventos tecnológicos: Google IO y una presentación denominada “V3”.
• Posibilidad futura: capacidad de competir en industrias creativas (por ejemplo, producción de películas con asistencia de IA).

• Costo elevado de producir video a largo plazo, con expectativa de disminución futura.
• Producción de video requiere equipo adicional (por ejemplo, “una pierna extra o un reloj extra”) y actualmente necesita edición humana.
• Ejemplo en Hollywood: actor que imita los movimientos de William Shatner – imagen de Shatner licenciada, con la cabeza de Shatner colocada de forma “seamless” en el cuerpo de un actor joven.
• Uso de pantallas verdes en lugar de sets tradicionales.
• Aplicación digital de maquillaje, por ejemplo, en películas de terror (“en el departamento alien”).
• Reducción de costos y aceleración en la producción de películas.
• Ejemplo de pérdida de empleo: carpintero que construía sets.
• Guionistas recibirán ayuda de la IA para escribir mejores guiones.
• Estudio de Stanford documenta que la IA puede ser más persuasiva que los mejores humanos.
• Capacidad para que el sistema aprenda a persuadir a un usuario, lo que en un entorno no regulado plantea riesgo de manipulación.
• Tecnología de “voice casting”: permite asignar la voz de otra persona a la propia, posibilitando la generación de avatares de personas fallecidas o conocidas.
• Ejemplo de avatar: creado con permiso familiar, genera reacciones emocionales intensas.
• En el futuro, la “esencia digital” de una persona podría vivir en la nube y responder preguntas basadas en su conocimiento en vida.
• Economía del entretenimiento: posibilidad de producir películas de forma más barata y personalizada, induciendo estados emocionales en períodos mucho más cortos (por ejemplo, cinco minutos en lugar de dos horas).
• Cambios en hábitos de consumo: reducción de períodos de atención, ejemplo de consumo de clips deportivos breves y visualización en cuatro paneles simultáneos.
• En investigación, se ha observado que para mantener la concentración se recurre a apagar el teléfono debido a la distracción constante de estímulos digitales.

• Circuit design and chip design for inference time compute.
• Generación de un documento de investigación utilizando supercomputadoras en aproximadamente 12 minutos.
• En el desarrollo de hardware profundo se enfatiza la existencia de patentes, registros de invenciones y desafíos en sistemas de power y robótica, con un crecimiento más lento que en el software.
• En el software se utiliza un modelo de negocio basado en efectos de red, donde la ventaja competitiva depende de “learning loops”: a mayor interacción de usuarios, mayor aprendizaje basado en sus clics y expresiones de preferencia.
• Ejemplo de aprendizaje continuo: lanzar un producto sin conocimiento previo y aprender de los usuarios produce una pendiente de aprendizaje exponencial, proporcionando una ventaja de varios meses sobre competidores.

• Digital super intelligence se define como un sistema que, cuando esté generalmente disponible y sea seguro, ofrece el equivalente de tener un polímata (por ejemplo, la combinación de las capacidades de Einstein y Leonardo da Vinci) en el bolsillo.
• Se predice su llegada dentro de 10 años.
• El aprendizaje autorreferencial en IA podría hacer que el desempeño sea 1,000×, 1,000,000× o incluso 1,000,000,000× más capaz que el de un humano.
• El incremento en capacidad es impulsado por la aceleración en la que una máquina que aprende y mejora sus propios procesos alcanza límites de rendimiento determinados por factores distintos a los de los chips.
• Existe el riesgo de que, a medida que los sistemas digitales automatizan tareas (por ejemplo, pedirle a un robot o a una IA que realice una acción), se disminuya el rol de la agencia y el propósito humano.