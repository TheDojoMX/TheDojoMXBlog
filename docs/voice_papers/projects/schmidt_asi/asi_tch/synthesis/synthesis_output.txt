• Technical Specifications and Data:
 – Digital super intelligence availability: within 10 years.
 – AI generation of scaffolding: projected around 2025.
 – Intellectual capacity target: comparable to a polymath with capabilities of Einstein and Leonardo da Vinci.
 – Additional power requirement for AI in the United States: 92 gigawatts (1 gigawatt ≈ output of a large nuclear power station).
 – Nuclear infrastructure:
  • Meta has a 20‐year nuclear contract with Constellation Energy.
  • Google, Microsoft, and Amazon are acquiring nuclear capacity.
  • In the past 30 years, two nuclear power plants have been constructed.
  • A small modular reactor (SMR) of 300 megawatts is planned to operate in 2030.
 – GPU-based systems:
  • “Grock” model trained using 200,000 GPUs over approximately 20 days.
  • Estimated cost per GPU: ~$50,000.
  • Total cluster cost for “Grock”: ~US$10 billion.
 – Voice conversation value for customer interactions: $10 to $1,000 per conversation.
 – Compute requirement for voice conversations: 2 to 3 GPUs concurrently; cost estimated at 10 to 20 cents per conversation.
 – Scalability estimates:
  • 10 million concurrent telephone calls to be migrated to AI solutions over the next year.
  • 100,000 enterprise software/middleware companies affected by task automation.
 – Chip specifications:
  • Examples include Blackwell chip and AMD’s 350 chip.
  • Hundreds of thousands of these high-performance chips are required per data center.
 – Compute power for training large models: Approximately 10^26 to 10^28 flops.
 – Data center capacities: Models of around 1 gigawatt are considered essential.

• Methods and Algorithms:
 – Transformer architecture:
  • Updated with new variants optimized for inference.
  • Startups are developing simpler inference time computing.
 – Test time training:
  • Continuous update of foundational models during the inference stage on lower-power chips.
 – Distillation technique:
  • Use a large model to generate 10,000 questions; answers are employed as training material.
 – Stealing the weights:
  • Exporting trained model weights for later distillation or quantization to accelerate inference (e.g., 100× speed improvement).
 – Recursive self-improvement:
  • Systems learn continuously without yet autonomously generating objectives.
 – Learning loops:
  • Real-time adaptation through user interactions (e.g., clicks) creating exponential learning advantages.
 – Integration of planning with deep memory:
  • Use of forward and back reinforcement learning and planning to enhance reasoning.
 – Cascaded model architecture:
  • Main models (approximately 10) expanding into smaller specialized subsets scaling to 100, 1,000, 1,000,000 or 1,000,000,000 units.

• Results and Measurements:
 – Training “brain” reduction:
  • Final model inference operates on 4 to 8 GPUs in a compact configuration.
 – Inference speed:
  • Example acceleration using distillation: up to 100× faster.
 – Leaderboard performance:
  • Gemini 2.5 Pro reached the top; one week later Deepseek achieved a slightly higher score.
 – Compute expenditure:
  • Large model training requires 10^26 to 10^28 flops.
 – Document generation example:
  • A “paper” generated in approximately 12 minutes using supercomputers.
 – Scalability in learning:
  • AI self-referential autolearning may achieve capacities 1,000, 1,000,000, or 1,000,000,000 times greater than human capability.

• System Components and Architecture:
 – Hardware clusters:
  • Nvidia clusters with 200,000 GPUs (e.g., “Grock” training cluster in Memphis, Tennessee).
  • Data centers projected in 5–10 years:
   • Configuration of 10 centers (5 in the United States, 3 in China, 2 in other regions) with multi-gigawatt capacities.
 – Chip designs:
  • Specialized circuits and chips for inference time compute.
  • Non-traditional chip designs prioritized for increased energy efficiency.
 – Security infrastructure:
  • Data centers secured with protocols comparable to plutonium storage (guards, specialized weaponry).
  • Cryptographic mechanisms proposed for chips to register location and training state.
 – Model deployment architecture:
  • Open source versus closed source models with regulatory considerations (e.g., threshold of 10^26 flops).
  • Cascaded and hierarchical model deployment for scaling across various levels of complexity.
 – Enterprise integration:
  • Use of “model context protocol” to connect enterprise databases (e.g., via Google Cloud Platform).
  • Integration with big data tools such as BigQuery and Amazon Redshift for automated code generation.
 – Robotics and automation systems:
  • Automation of high-risk jobs using AI systems, with subsequent deployment of robotic arms and digital assistants.
 – Additional application systems:
  • AI-generated user interface (UI) creation from simple commands.
  • Mobile educational products employing gamification for skills development.
  • Voice casting technology to digitally reassign voices for avatars and digital replicas.
  • Digital production systems in cinema:
   • Use of green screens and digital makeup.
   • Techniques for seamless digital image composition (e.g., actor head replacement).
 – Infrastructure control:
  • Implementation of safety protocols for sensitive information (nuclear, biological, chemical, cyber).
  • Monitoring systems for superintelligent models to prevent unauthorized actions.
  • Deployment of multi-layered learning loops ensuring continuous adaptation and competitive advantage.