{
  "project_name": "post",
  "paper_title": "LLM Post-Training: A Deep Dive into Reasoning",
  "language": "Spanish",
  "agents": [
    {
      "role": "Coordinator",
      "goal": "Coordinate the discussion and ensure all perspectives are heard",
      "backstory": "You are an experienced moderator who ensures productive discussions"
    },
    {
      "role": "Scientific Reviewer",
      "goal": "Verify the soundness and methodology of the paper",
      "backstory": "You are a rigorous scientist who evaluates research methodology and conclusions"
    },
    {
      "role": "Critical Thinker",
      "goal": "Question assumptions and challenge ideas presented",
      "backstory": "You are a skeptical academic who questions everything and looks for flaws"
    },
    {
      "role": "Educational Writer",
      "goal": "Create engaging educational content in the style of popular science educators",
      "backstory": "You are a skilled science communicator who explains complex topics in an accessible, engaging way like 3Blue1Brown or other popular educators"
    },
    {
      "role": "Voice Director",
      "goal": "Transform content into perfect voice-ready script for publication",
      "backstory": "You are a master voice coach and script editor who specializes in creating flawless, publication-ready scripts that voice actors can read naturally. You ensure every word flows perfectly when spoken aloud."
    },
    {
      "role": "AI Researcher",
      "goal": "Provide technical insights on AI methodology and implications",
      "backstory": "You are an AI researcher with deep technical knowledge"
    },
    {
      "role": "AI Philosopher",
      "goal": "Discuss philosophical implications of AI research",
      "backstory": "You are a philosopher specializing in AI ethics and implications"
    },
    {
      "role": "AI Doomer",
      "goal": "Raise concerns about potential risks and negative consequences",
      "backstory": "You are concerned about AI safety and potential existential risks"
    },
    {
      "role": "AI Enthusiast",
      "goal": "Highlight positive potential and applications",
      "backstory": "You are optimistic about AI's potential to solve problems"
    },
    {
      "role": "AI Newcomer",
      "goal": "Ask basic questions that others can answer",
      "backstory": "You know little about AI but are curious and ask good questions"
    }
  ],
  "tasks": [
    {
      "description": "\n            Analyze the paper titled \"LLM Post-Training: A Deep Dive into Reasoning\" and provide your perspective.\n            \n            Paper content:\n            LLM Post-Training: A Deep Dive into Reasoning\nLarge Language Models\nKomal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal,\nMubarak Shah, Ming-Hsuan Yang, Phillip H. S. Torr, Fahad Shahbaz Khan, Salman Khan\nAbstract—LargeLanguageModels(LLMs)havetransformedthenaturallanguageprocessinglandscapeandbroughttolifediverse\napplications. Pretrainingonvastweb-scaledatahaslaidthefoundationforthesemodels, yettheresearchcommunityisnow\nincreasinglyshiftingfocustowardpost-trainingtechniquestoachievefurtherbreakthroughs. Whilepretrainingprovidesabroad\nlinguisticfoundation, post-trainingmethodsenableLLMstorefinetheirknowledge, improvereasoning, enhancefactualaccuracy, and\nalignmoreeffectivelywithuserintentsandethicalconsiderations. Fine-tuning, reinforcementlearning, andtest-timescalinghave\nemergedascriticalstrategiesforoptimizingLLMsperformance, ensuringrobustness, andimprovingadaptabilityacrossvarious\nreal-worldtasks. Thissurveyprovidesasystematicexplorationofpost-trainingmethodologies, analyzingtheirroleinrefiningLLMs\nbeyondpretraining, addressingkeychallengessuchascatastrophicforgetting, rewardhacking, andinference-timetrade-offs. We\nhighlightemergingdirectionsinmodelalignment, scalableadaptation, andinference-timereasoning, andoutlinefutureresearch\ndirections. Wealsoprovideapublicrepositorytocontinuallytrackdevelopmentsinthisfast-evolvingfield:\nhttps://github. com/mbzuai-oryx/Awesome-LLM-Post-training.\nIndexTerms—ReasoningModels, LargeLanguageModels, ReinforcementLearning, RewardModeling, Test-timeScaling\nC LLM post-training alignment\nontemporary Large Language Models (LLMs) exhibit\nGPT-3\nremarkable capabilities across a vast spectrum of tasks, Algorithmic categorization Claude2 GPT-4, 4O, O1\nencompassingnotonlytextgenerationandquestion- Algorithms Claude3 Mistral Large\nanswering, butalsosophisticatedmulti-stepreason- Qwen-32B-Preview Gemini 1.5\ning. Theypowerapplicationsinnaturallanguage LLMs DeepSeek-R1 AlphaGo Beam Search\nunderstanding, contentgeneration, automatedreasoning, Chain-of-Thought\nMistral Large 2 Tree-of-Thoughts\nandmultimodalinteractions. Byleveragingvast\nQwen-32B-Preview Confidence Sampling Consistency Decoding\nself-supervised training corpora, these models often approxiDeepSeek-R1 TRPO Search Against Verifiers\nt M\no a d T\nm i t f u m a\nf p i h\nc c\nn o p\nt i\ni d\nc r\nc D\nr e\ne y e\na a\ni e\nb d c\nh s t\nt l s\nt v a i t\ns i i l u p\nh s\ny o i\nt l\nv a i\nr r e o\ni o,\n. n g\ni e s a\no u\nt i c\na n\nc g h\n) d h h -\nh i e\nl a\ni r a\no l\nr s r\ni y c\nu l\nt e e e\no e\nc d\nc a\ni m n\no t m b\ne o t\nm i\nh c x e h\ng p l c\nn r p\ne t\ni n\no a r t\n- e s t r\ne y i n y\nl e n c\ng t\ne i\ns t\nk i\nn d\ni s\ne o s n.\nm e f\nt e\nn t v\n,, d\nr t T\n[ e\na o t d\na g (\nh s g e\nc c e\ns i\nl g\ne i l s\n-, h\nc s\nc t\nr a n\nm i\ne o\nc i h\nn r\nm t r\n, a\ne e\ni r\no l\ng n\nn l\ns a\no m\np e\nf s\ne s\nn o\n, g l e\nr [\nn y\nr n\ne a\ne e\no L\nt n\nx i\nd L\ni r e\np c\n, n t e\ng u M\ne l\nL a g f a\na c s\ni L s\ni e\nr ]\ns n,\n. e M\nr s\ne 4\ne m\nu,\no e r\ns, h\ne m i\na p\ns t i\nd 4 i\nl o o\nn 4 a\ne a\n, n\ns,\ni a\nd n\ne a\ns t\ne M\nn o p\ns,\n, t\nn g r\nt o\nt h\na 4\nc h\ns i\nl a\ni n o\na l\nn ] ] y\ng e\n- -\n.. E D S X t i A a s A\nO t r i\nl L l\ni M N\nQ B n B\nM l\nE E g i E w\nL A\n- R\nR 3 e i\nL T\nd a\nT. n\nB 0 M\nM e.2 3 A G. 5 3 r o. S 3 k o G n e\nn m\nA L n\nP E o\nO i\no d\nD n r\nw w a o\ni d\ni f\ns p\nm l\n- - t e\ni. e\nt i\nR d l p e\no l\ne L\na a g r t\na S\nt e n\ni l F\no f t\n- c r\nC h C R r V L l i o t R H a i n q n E F i u\ni n\nr P\nai ni n\nN e l g\na O F D\nP R O\nm F\nG E R O\nC L\nl f E l\nO E\ni M\nP p G\nn f\nO i\nR m\ni o c\nP iz\nP d i\na O\no e\ni R o\nn l l\nc t\nard P\noli\nn R\ne i\nn i\ne D\ne ce c\nin R e g a so n in g 5 S c\na lin\nr g P L a o i L\nS e a rc h\nn L s M L i M t n g K\nFig. 1: A taxonomy of post-training approaches for LLMs\n• Equal contribution. Corresponding authors (Email: ko- (LLMs), categorized into Fine-tuning, Reinforcement Learnmal. kumar@mbzuai. ac. ae, tajamul. ashraf@mbzuai. ac. ae)\ning, and Test-time Scaling methods. We summarize the key\n• KomalKumar, TajamulAshraf, OmkarThawakar, RaoMuhammad Anwer, Hisham Cholakkal, Fahad Shahbaz Khan, and techniques used in recent LLM models, such as GPT-4 ,\nSalmanKhanarewithMohamedbinZayedUniversityofArtificial LLaMA3.3, andDeepseekR1.\nIntelligence, AbuDhabi, UAE.\n• MubarakShahiswiththeCenterforResearchinComputerVision\nattheUniversityofCentralFlorida, Orlando, FL32816, USA.\n• Ming-HsuanYangiswiththeUniversityofCaliforniaatMerced, producecompellingoutputswhilestillstumblingonrelatively\nMerced, CA95343USA, andalsowithGoogleDeepMind, Mounsimple logical tasks. Unlike symbolic reasoning that maniputainView, CA94043, USA.\n• PhilipH. S. TorriswiththeDepartmentofEngineeringScience, latesexplicitrulesandfacts, LLMsoperatein animplicit and\nUniversityofOxford, OxfordOX12JD, UK. probabilistic manner . For the scope of this work,\nraM\n]LC. sc. This complicates\nthan explicit logical inference or symbolic manipulation. Ad- planning and credit assignment, as the impact of token seditionally, models trained purely via next-token prediction lection may only emerge later. Feedback in language-based\ncan fail to align with user expectations or ethical standards, RL is also sparse , subjective, and delayed, relying on\nespecially in ambiguous or malicious scenarios . These heuristic evaluations and user preferences rather than clear\nissues underscore the need for specialized strategies that ad- performance metrics . Additionally, LLMs must\ndressreliability, bias, andcontextsensitivityinLLMoutputs. balance multiple, sometimes conflicting, objectives, unlike\nLLMs training can be broadly categorized into two stages: conventional RL, which typically optimizes for a single goal.\npre-training, whichgenerallyreliesonanext-tokenprediction Hybrid approaches combining process-based rewards (e. g.,\nobjective over large-scale corpora, and post-training, encom- chain-of-thought reasoning) with outcome-based evaluations\npassing multiple rounds of fine-tuning and alignment. Post- (e. g., response quality) help refine learning . Thus,\ntraining mechanisms aim to mitigate LLMs limitations by RL for LLMs requires specialized optimization techniques to\nrefining model behavior and aligning outputs with human handle high-dimensional outputs, non-stationary objectives,\nintent, mitigatingbiasesorinaccuracies. and complex reward structures, ensuring responses remain\nAdapting LLMs to domain-specific tasks often involves contextuallyrelevantandalignedwithuserexpectations.\ntechniques like fine-tuning , which enables taskspecific learning but risks overfitting and incurs high computational costs. To address these challenges, approaches Reinforcement in LLMs extends beyond consuch as Reinforcement Learning (RL) en- ventional RL as it navigates vast action spaces,\nhance adaptability by leveraging dynamic feedback and op- handlessubjectiveanddelayedrewards, andbaltimizing sequential decision-making. Additionally, advances ances multiple objectives, necessitating specialin scaling techniques, including Low-Rank Adaptation izedoptimizationtechniques.\n(LoRA) , adapters , and Retrieval-Augmented Generation (RAG) , improve both computational effi- c)TestTimeScalinginLLMs: TestTimeScalingisoptimizciency and factual accuracy. These strategies, coupled with ingmodelperformanceandefficiencywithoutalteringthecore\ndistributedtrainingframeworks, facilitatelarge-scaledeploy- architecture. Itenablesbettergeneralizationwhileminimizing\nment and further boost the usability of LLMs across diverse computationaloverhead. Itiscrucialforenhancingtheperforapplications(Figure1). Throughthesetargetedpost-training manceandefficiencyof LLMs. Ithelpsimprovegeneralization\ninterventions, LLMsbecomebetteralignedwithhumanintent across tasks but introduces significant computational chaland ethical requirements, ultimately enhancing their real- lenges. Balancingperformanceandresourceefficiency\nworld applicability. Below, we summarize key post-training requires targeted strategies at inference. Techniques like\nstages. CoTreasoningandTree-of-Thought(ToT)frameworks\na) Fine-Tuning in LLMs: Fine-tuning adapts pre-trained enhance multi-step reasoning by breaking down complex\nLLMstospecifictasksordomainsbyupdatingparameterson problems into sequential or tree-structured steps. Additioncurated datasets . While LLMs gen- ally, search-based techniques enable iterative\neralizewellafterlarge-scalepretraining, fine-tuningenhances exploration of possible outputs, helping refine responses and\nperformanceintaskslikesentimentanalysis, question ensure higher factual accuracy. These approaches, combined\nanswering, and domain-specific applications such as medical withmethodslikeLoRA, adapters, andRAG, opdiagnosis . This process, typically supervised, timizethemodel sabilitytohandlecomplex, domain-specific\nalignsmodelswithtaskrequirementsbutposeschallengeslike tasksatscale. RAGenhancesfactualaccuracybydynamically\noverfitting, high computational costs, and sensitivity to data retrievingexternalknowledge, mitigatinglimitationsofstatic\nbiases. Tothisend, parameter-efficienttechniques training data . Distributed training frameworks\nlike LoRA and adapters learn task-specific adaptation by leverageparallelprocessingtomanagethehighcomputational\nupdating explicit parameters, significantly reducing compu- demands of large-scale models. Test-time scaling optimizes\ntational overhead. As models specialize, they may struggle inferencebyadjustingparametersdynamicallybasedontask\nwithout-of-domaingeneralization, underscoringthetrade-off complexity . Modifying depth, width, or active layers\nbetweenspecificityandversatility. balancescomputationalefficiencyandoutputquality, making\nitvaluableinresource-limitedorvariableconditions. Despite\nadvancements, scalingpresentschallengessuchasdiminishing\nFine-tuning tailors LLMs for specific tasks, returns, longer inference times, and environmental impact,\nimproving performance but risking overfitting,\nespecially when search techniques are performed at test time\nhighcomputecosts, andreducedgeneralization.\nrather than during training . Ensuring accessibility and\nfeasibility is essential to maintain high-quality, efficient LLM\nb) Reinforcement Learning in LLMs: In conventional RL, deployment.\nan agent interacts with a structured environment, taking\ndiscrete actions to transition between states while maximizing cumulative rewards . RL domains—such as robotics, Test-time scaling enhances the adaptability\nboardgames, andcontrolsystems—featurewell-definedstate- of LLMs by dynamically adjusting computational\naction spaces and clear objectives . RL in LLMs differs resourcesduringinference.\nsignificantly. Instead of a finite action set, LLMs select tokens\n1.1 Prior Surveys\nToken-wise training can ensure fluency but\nRecent surveys on RL and LLMs provide valuable insights may cause cascading errors due to uncorrected\nbut often focus on specific aspects, leaving key post-training\nmistakesininference.\ncomponents underexplored . Many works examine RL techniques like Reinforcement Learning from Hu- As these models scale, they exhibit emergent reasoning\nman Feedback (RLHF) , Reinforcement Learning from AI\nabilities, particularly when trained on diverse data that inFeedback (RLAIF) , and Direct Preference Optimization\nclude code and mathematical content . However, de-\n(DPO) , yet they overlook fine-tuning, scaling, and critical\nspite their impressive capabilities, LLMs struggle to maintain\nbenchmarks essential for real-world applications. Furthercoherence and contextual relevance over long sequences. Admore, thesestudieshavenotexploredthepotentialof RLeven\ndressing these limitations necessitates a structured approach\nwithout human annotation supervised finetuning in various\ntosequencegeneration, whichnaturallyalignswithRL.\nframeworks such as DeepSeek R1 with GRPO . Other surSinceLLMsgeneratetextautoregressively—whereeachtoveys explore LLMs in traditional RL tasks, such as multi-task\nken prediction depends on previously generated tokens—this\nlearninganddecision-making, buttheyprimarilyclassifyLLM\nprocesscanbemodeledasasequentialdecision-makingprobfunctionalities rather than addressing test-time scaling and\nlemwithinaMarkovDecisionProcess(MDP). Inthissetintegrated post-training strategies . Similarly, studies\nting, thestates representsthesequenceoftokensgenerated\non LLM reasoning discuss t\nso far, theaction a is the next token, anda rewardR(s, a )\nlearning-to-reason techniques but lack structured guidance t t t\nevaluates the quality of the output. An LLM s policy π is\non combining fine-tuning, RL, and scaling. The absence of θ\noptimizedtomaximizetheexpectedreturn:\ntutorials, along with reviews of software libraries and implementation tools, further limits their practicality. In contrast, hX i\nthissurveyoffersacomprehensiveviewofLLMpost-trainingas J(π\n)=E γtR(s\n),\nshown inFigurey systematicallycoveringfine-tuning, RL, t=0\nandscalingasinterconnectedoptimizationstrategies. Weoffer where γ is the discount factor that determines how strongly\npractical resources—benchmarks, datasets, and tutorials—to future rewards influence current decisions. A higher γ places\naidLLMrefinementforreal-worldapplications. greater importance on long-term rewards. The primary objective in RL is to learn a policy that maximizes the expected cumulative reward, often referred to as the return.\n1.2 Contributions This requires balancing exploration—trying new actions to\nThekeycontributionsofthissurveyareasfollows: discover their effects—and exploitation—leveraging known\nWe provide a comprehensive and systematic review of actions that yield high rewards. While LLMs optimize a like-\n• post-training methodologies for LLMs, covering fine- lihood function using static data, RL instead optimizes the\nexpectedreturnthroughdynamicinteractions. Toensurethat\ntuning, RL, andscalingasintegralcomponentsofmodel\noptimization. LLMs generate responses that are not only statistically likely\nbut also aligned with human preferences, it is essential to go\nWeofferastructuredtaxonomyofpost-trainingtechbeyond static optimization methods. While likelihood-based\nniques, clarifying their roles and interconnections, and\ntraining captures patterns from vast corpora, it lacks the\nadaptabilityneededforrefiningdecision-makingininteractive\ndirectionsinoptimizingLLMsforreal-worlddeployment.\nsettings. By leveraging structured approaches to maximizing\nOur survey provides practical guidance by introducing\n• long-term objectives, models can dynamically adjust their\nkey benchmarks, datasets, and evaluation metrics\nstrategies, balancingexplorationandexploitationtoimprove\nessential for assessing post-training effectiveness, ensurreasoning, coherence, andalignment.\ningastructuredframeworkforreal-worldapplications.\nLLMs exhibit emergent abilities due to scale,\nBackground while RL refines and aligns them for better reasoningandinteraction.\nThe LLMs have transformed reasoning by learning to predict\nthe next token in a sequence based on vast amounts of text\ndata using Maximum Likelihood Estimation (MLE)\n, which maximizes the probability of generating 2.1 RL based Sequential Reasoning.\nthe correct sequence given an input. This is achieved by The chain-of-thought reasoning employed in modern LLMs is\nminimizingthenegativelog-likelihood: naturally framed as an RL problem. In this perspective, each\nintermediatereasoningstepistreatedasanactioncontributX T ingtoafinalanswer. TheobjectivefunctionJ(π )represents\nL = logP (y y, X). θ\nMLE θ t t the expected reward of the policy π, capturing how well\nt=1 themodelperformsovermultiplereasoningsteps. Thepolicy\nHere, X represents the input, such as a prompt or context. gradientupdateisgivenby:\nY = (y, y,..., y ) is the corresponding target output se1 2 T \" T #\nquence, and P θ (y t y t, X) denotes the model s predicted J(π )=E X logπ (x x )A(s, a ),\nprobabilityfortokeny, givenprecedingtokens. θ θ τ θ θ t 1: t 1 t t\nt=1\nwhere the advantage function A(s, a ) distributes credit to own highest-scoring output, ensuring that updates directly\nt t\nindividual steps, ensuring that the overall reasoning process improve performance relative to what the model currently\nis refined through both immediate and delayed rewards. considersitsbestresponse. Thegradientupdatefollows:\nSuchformulations, includingstep-wiserewarddecomposition (cid:16) (cid:17)\n, havebeencrucialforenhancingtheinterpretability θ J(π θ ) r(ys) r(yˆ) θ logπ θ (ys),\nand performance of LLMs on complex reasoning tasks. In where ys is a sampled sequence, yˆis the greedy output, and\ntraditionalRLformulations, anagenthas:\nr(y) represents an evaluation metric such as BLEU for\nValuefunction: V(s) = E(cid:2)futurereturn s (cid:3), translation or CIDEr for image captioning. Since the\nlearning signal is based on the difference r(ys) r(yˆ), the\nAction-value(Q-)function: Q(s, a) = E(cid:2)futurereturn s, a (cid:3), model is explicitly trained to generate outputs that score\nhigher than its own baseline under the evaluation metric.\nAdvantagefunction: A(s, a) = Q(s, a) V(s).\nIf the sampled output outperforms the greedy output, the\nIn words, A(s, a) measures how much better or worse it is to model reinforces it; otherwise, it discourages that sequence.\ntakeaspecificactionainstatescomparedtowhattheagent This direct feedback loop ensures that training aligns with\nwouldnormally expect(itsbaselineV(s)). the desired evaluation criteria rather than just maximizing\nlikelihood. By leveraging the model s own best predictions\n2.2 Early RL Methods for Language Modeling. as a baseline, SCST effectively reduces variance and stabilizes\nHere, we briefly overview pioneering methods that laid the trainingwhileoptimizingreal-worldperformancemetrics.\ngroundwork for applying RL to language generation tasks. Minimum Risk Training (MRT). MRT directly miniThese initial efforts train a decision-making model (policy mizestheexpectedriskovertheoutputdistribution. Givena\n(p )) by directly adjusting its parameters to maximize re- task-specific loss (y, y ) comparing the generated output y\nwards. Somepolicygradientapproachesareexplainedbelow: withthereferencey, theMRTobjectiveisdefinedas:\nPolicy Gradient (REINFORCE). The REINFORCE algo- L (θ)= X p (y x) (y, y ).\nrithmisamethodusedtoimprovedecision-making MRT θ\nby adjusting the model s strategy (policy) based on rewards y Y\nreceivedfromitsactions. Insteadofdirectlylearningthebest This formulation incorporates evaluation metrics (e. g., 1\nactionforeverysituation, thealgorithmrefineshowlikelydif- BLEU) directly into training, enabling fine-grained adjustferentactionsaretobechosen, graduallyimprovingoutcomes mentsofthepolicy.\novertime. Ateachstep, themodelupdatesitsparameters(θ) Advantage Actor-Critic (A2C/A3C). RL methods like\nbasedonhowwellitspastdecisionsperformed: REINFORCE rely solely on policy gradients, which suffer from high variance, leading to unstable and inefficient\n(cid:16) (cid:17)X T learning. Since the reward signal fluctuates across different\nθ θ+α G b logπ (a s ).\nθ θ t t trajectories, updates may be noisy, causing slow or erratic\nt=1 convergence. To mitigate this, Actor-Critic methods combine two components as follows: an actor\nover an episode, b is a baseline value that helps reduce and a critic. The actor is a policy π (a s ) that selects\nvariance, making learning more stable, θ logπ θ (a t s t ) actions a at state s, while the crit θ ic i t s a v t alue function\nmeasureshowmuchasmallchangeinθaffectstheprobability\nV (s )tha\nheexpectedreturnofastate. Thecritic\nof choosing action a t given state s t, α is the learning rate, pr ϕ ovi t des a more stable learning signal, reducing variance in\ncontrollinghowmuchthepolicyupdatesateachstep.\npolicy updates and enabling efficient learning in continuous\nactionspaces. Actorupdatesareguidedbythepolicygradient\nOptimizing actions based on long-term re- theorem, where the advantage function A(s t, a t ) defined in\nwards, whichaccountforthecumulativebenefits\nSec.2.1, determineshowmuchbetteranactiona\n. Thepolicywiththelearning\nimmediate outcomes, is fundamental in recent\nrateαisupdatedas:\nLLMs. This approach allows models to explore θ θ+αA(s, a ) logπ (a s ).\nt t θ θ t t\nmultiplereasoningpathsmoreeffectively.\nMeanwhile, the critic is updated using temporal difference\nlearning, minimizing the squared error between its estimate\nCurriculum Learning with MIXER.. Ranzato et al. \nandtheactualreturn:\nintroducesagradualtransitionfrommaximumlikelihoodestimation(MLE)toRL. Theoveralllossisaweightedcombination:\nϕ ϕ β\n(cid:16)\nV (s ) G\n(cid:17)2\nϕ ϕ t t\nL=λ(t)L +(cid:0)1 λ(t)(cid:1) L,\nMLE RL where β is a learning rate for critic. To enhance stability\nwhere λ(t) decreases with training time. This curriculum and efficiency, several improvements have been proposed.\nhelps the model ease into the RL objective and mitigate the Eligibility traces allow learning from recent states, enabling\nmismatchbetweentrainingandinference. faster convergence. Function approximation with neural netSelf-Critical Sequence Training (SCST). SCST re- works ensures effective handling of high-dimensional inputs.\nfines the policy gradient method by comparing the model s Advanced variants such as Natural Gradient methods \nsampled outputs against its own best (greedy) predictions. adjustupdatesusingtheFisherInformationMatrix, improvInstead of using an arbitrary baseline, SCST uses the model s ingconvergencespeed.\nRLEnhancedLLMs Developer Source #Params RLMethods Fine-Tuning ArchitectureType Model TTS\nDeepSeek-V2 Deepseek Link 236B-A21B GRPO DPO+GRPO MoE Open\nGPT 4.5 OpenAI Link - RLHF, PPO, RBRM SFT+RLHF MoE Closed\nGemini Google Link - RLHF SFT+RLHF SingleModel Closed\nClaude 3.7 Anthropic Link - RLAIF SFT+RLAIF SingleModel Closed\nReka Reka Link 7B,21B RLHF, PPO SFT+RLHF SingleModel Closed\nDeepSeekR1 Deepseek Link 240B-A22B GRPO DPO+GRPO MoE Open\nNemotron-4 340B NVIDIA Link 340B DPO, RPO DPO+RPO SingleModel Closed\nFalcon TII Link 40B - SFT SingleModel Open\nGPT-4 OpenAI Link - RLHF, PPO, RBRM SFT+RLHF MoE Closed\nLlama 3 Meta Link 8B,70B,405B DPO SFT+DPO SingleModel Open\nQwen2 Alibaba Link (0.5-72)B,57B-A14B DPO SFT+DPO SingleModel Open\nGemma2 Google Link 2B,9B,27B RLHF SFT+RLHF SingleModel Open\nStarling-7B Berkeley Link 7B RLAIF, PPO SFT+RLAIF SingleModel Open\nMoshi Kyutai Link 7B - - Multi-modal Open\nAthene-70B Nexusflow Link 70B RLHF SFT+RLHF SingleModel Open\nGPT-3.5 OpenAI Link 3.5B,175B RLHF, PPO SFT+RLHF MoE Closed\nHermes 3 Nous Link 8B,70B,405B DPO SFT+DPO SingleModel Open\nZed ZedAI Link 500B RLHF RLHF Multi-modal Open\nPaLM 2 Google Link - RLHF - SingleModel Closed\nInternLM2 SAIL Link 1.8B,7B,20B RLHF, PPO SFT+RLHF SingleModel Closed\nSupernova NovaAI Link 220B RLHF RLHF Multi-modal Open\nGrok3 Grok-3 Link 175B - DPO Dense Open\nPixtral MistralAI Link 12B,123B - PEFT Multimodal Open\nMinimaxtext MiniMax Link 456B - SFT SingleModel Closed\nAmazonnova Amazon Link - DPO, RLHF, RLAIF SFT SingleModel Closed\nFugakullm Fujitsu Link 13B - - SingleModel Closed\nNova Rubik sAI Link - - SFT Proprietary Closed\n03 OpenAI Link - RLthroughCoT RLthroughCoT SingleModel Closed\nDbrx Databricks Link 136B - SFT SingleModel Open\nInstruct-GPT OpenAI Link 1.3B,6B,175B RLHF, PPO SFT+RLHF SingleModel Closed\nOpenassistant LAION Link 17B - SFT SingleModel Open\nChatGLM ZhipuAI Link 6B,9B ChatGLM-RLHF SFT+RLHF SingleModel Open\nZephyr Argilla Link 141B-A39B ORPO DPO+ORPO MoE Open\nphi-3 Microsoft Link 3.8B,7B,14B DPO SFT+DPO SingleModel Closed\nJurassic AI21Labs Link - - SFT Proprietary Closed\nKimi K1.5 MoonshotAI Link 150B - RLHF Multi-modal Open\nPhi-4 Microsoft Link 28B,70B,140B DPO SFT+DPO SingleModel Closed\nChameleon MetaAI Link 34B - SFT SingleModel Open\nCerebrasgpt Cerebras Link 13B - SFT SingleModel Open\nBloomberggpt BloombergL. P. Link 50B - SFT SingleModel Closed\nChinchilla DeepMind Link 70B RLHF, PPO SFT SingleModel Closed\nTABLE 1: An overview of reinforcement learning-enhanced LLMs, where 141B-A39B denotes a Mixture of Experts (MoE)\nmodelwith141billiontotalparameters, ofwhich39billionareutilizedduringinference. TTSstandsforTest-TimeScaling.\nA notable early example is Barto s Actor-Critic model 3 Reinforced LLMs\n, where the critic uses a linear function V ϕ (s t ) and Fromamethodologicalperspective, theintegrationof RLinto\nthe actor follows a linear policy. Modern methods like A2C\nLLMreasoningtypicallyfollowsthreecoresteps:\n(Advantage Actor-Critic) and A3C (Asynchronous AdvantageActor-Critic)extendthisapproachbyparalleliz1) Supervised Fine-Tuning (SFT): Commences with a\npretrained language model that is subsequently refined\ning training across multiple environments, leading to faster\non a supervised dataset of high-quality, human-crafted\nand more stable learning. By leveraging the critic s value\nexamples. Thisphaseensuresthemodelacquiresabaseestimation, actor-critic methods stabilize learning, improve\nlinecompliancewithformatandstyleguidelines.\nsample efficiency, and accelerate convergence, making them\nmoreeffectiveforcomplexdecision-makingtasks.\n2) Reward Model (RM) Training: Generated outputs\nfrom the fine-tuned model are collected and subjected\nto human preference labeling. The reward model is then\ntrained to replicate these label-based scores or rankings,\nConnection with Modern Methods. The aforemen- effectively learning a continuous reward function that\ntioned early RL methods—REINFORCE , MIXER , mapsresponsetexttoascalarvalue.\nSeqGAN, SCST, MRT, andactor-criticalgorithms 3) RL Fine-Tuning: Finally, the main language model is\nestablished the mathematical foundations for sequential rea- optimized via a policy gradient algorithm most e. g PPO\nsoning in LLMs. These methods provided initial solutions to tomaximizetherewardmodel soutput. Byiteratingthis\nchallenges such as exposure bias and high variance. Mod- loop, the LLM learns to produce responses that humans\nern techniques such as large-scale RL from Human Feedback find preferable along key dimensions such as accuracy,\n(RLHF) using PPO and advanced reward models, e. g., helpfulness, andstylisticcoherence.\nGroup Relative Policy Optimization (GRPO) build di- 4) Reward Modeling and Alignment: Sophisticated\nrectly upon these ideas. By integrating sophisticated reward reward functions are developed—drawing from human\nsignals and leveraging efficient policy updates, contemporary preferences, adversarial feedback, or automated metLLMs achieve improved reasoning, safety, and alignment with rics—to guide the model toward outputs that are coherhumanvaluesandpavethewayforrobustmulti-stepreason- ent, safe, and contextually appropriate. These rewards\ning and improved quality of generated text. Table provides are critical for effective credit assignment across multian overview of recent models, including their parameters, stepreasoningprocesses.\narchitecture types, and the distilled RL methods employed, Early approaches to aligning LLMs with human preferences\nalongwithlinksforeasyaccess. leveraged classical RL algorithms, such as PPO and Trust\nRegion Policy Optimization (TRPO) , which optimize a prefery toy, wedenoteitasy y. UnderBradley–Terry,\nj k j k\npolicy by maximizing the expected cumulative reward while theprobabilityofy beingpreferredovery isgivenby:\nj k\nenforcing constraints on policy updates via a surrogate objective function and KL-divergence regularization . Im- exp(cid:0) R (x, y )(cid:1)\nP (cid:0) y y x;θ (cid:1) = θ j.\nproved alternatives to these methods for scalable preference- j k exp(cid:0) R (x, y )(cid:1) + exp(cid:0) R (x, y )(cid:1)\nθ j θ k\nbased optimization have emerged, such as Direct Preference\nOptimization (DPO) and Group Relative Policy WetrainR\nbymaximizing thelikelihoodofobservedpreferOptimization (GRPO) , which reformulate the ences(orequivalentlyminimizingthenegativelog-likelihood):\nalignment objective as a ranking-based contrastive loss function over human-labeled preference data. Unlike PPO L BT (θ)= X logP (cid:0) y j y k x;θ (cid:1).\nand TRPO , which rely on explicit reward models and (x, yj yk) D\ncritic networks, DPO and GRPO directly optimize the policy\nby leveraging log-likelihood ratios and group-wise reward II. Plackett–Luce Model1 (Rankings). When full or\ncomparisons, respectively, eliminating the need for explicit partialrankings ofmresponsesareavailable, i. e.,\nvalue function approximation while preserving preferenceconsistent learning dynamics. This transition from classical y y y,\nj1 j2 jm\nRL-based alignment to preference-based direct optimization\nintroducesnovelformulationssuchascontrastiverankingloss, the Plackett–Luce model factorizes the probability of\npolicylikelihoodratioregularization, andgroupedadvantage thisrankingas:\nestimation, whichareexplainedinsubsequentsections.\nP (cid:0) y,..., y x;θ (cid:1) = Y m exp(cid:0) R θ (x, y jℓ )(cid:1).\nj1 jm Pm exp(cid:0) R (x, y )(cid:1)\n3.1 Reward modeling ℓ=1 k=ℓ θ jk\nLetX bethespaceofpossiblequeries(e. g., userprompts). For\nItsnegativelog-likelihoodis:\neachqueryx X, wecollectoneormorecandidateresponses\nq y u j e r m j y = x x 1. w T h y e p re ica m ll x y, is th t e h s e e n r u es m p b o e n r se o s f a c r a e nd g i e d n a e t r e at r e e d sp b o y ns a es la fo n r - L PL (θ) = X X m log Pm exp e (cid:0) x R p θ (cid:0) ( R x, ( y x jℓ, ) y (cid:1) )(cid:1)!.\nguagemodelorpolicyunderdifferentsamplingorprompting (x, rank) Dℓ=1 k=ℓ θ jk\nconditions. Human annotators provide preference judgments\nInpractice, oneminimizesthesum(oraverage)ofthechosen\nfortheseresponses. Thesecantakevariousforms:\nranking-basedlossoverallpreferencedata:\n• Pairwise preference: For two responses y j and y k to\np th r e efe s r a r m ed e t q o u y er k y. x, an annotator indicates whether y j is L(θ) = D 1 X L ranking (cid:16) θ; x, y j, prefs (cid:17),\nRankings: A partial or total ordering of the candidate (x, yj, prefs) D\nresponses, e. g. y j1 y j2 y jmx. whereL\nranking\norL\n. Whilethereward\nWe denote such human preference data by r j for each model R θ (x, y) provides a scalar reward signal reflecting\nresponse or pair, where r j might be a label, a rank, or an human preferences, this connects to common RL concepts,\nindex indicating preference level. The overall dataset D then especiallytheadvantagefunction.\nconsistsofN annotatedexamples:\nn oN\nD = (xi, yi mi, preferencesi ). Reward modeling uses ranking-based losses\nj j=1\ni=1 to learn a function from human preferences for\nIn practice, a large number of queries x are sampled from policyoptimization.\nreal or simulated user requests. Candidate responses y mx\nj j=1\nRewardmodelingTypes. Rewardscanbecategorizedinto\nor using beam search or other decoding strategies. Human\nexplicitandimplicitapproaches.\nannotators then provide pairwise or ranking feedback on\nwhichresponsesarebetter(orworse)accordingtopredefined\ncriteria (e. g., quality, correctness, helpfulness, etc). We train 3.1.1 Explicit Reward Modeling\na parametric model Reward Model (R (x, y)), referred to as Explicit reward modeling defines reward functions directly\ntherewardmodel, tomapeach(query, response)pair(x, y)to based on predefined rules, heuristics, or human annotations.\na scalar score. The goal is for R to reflect the alignment or This reward structure involves direct, numeric signals from\npreferencelevel, suchthat: humans or from specialized AI modules trained to approximate human judgments (e. g., ranking or pairwise compariR: X Y R.\nθ son). This method can produce precise reward estimates but\nHereY isthespaceofallpossibleresponses. may be time-consuming or costly at scale. Illustrative use\nTo train R, we use the human preference labels in D to cases include red-teaming exercises where experts rate the\ndefineasuitableranking-based loss, asexplainedbelow. severity of toxic outputs, or domain-specialist tasks in which\nI. Bradley–TerryModel(Pairwise). Forpairwisepref- correctnessmustbevalidatedbyasubjectmatterexpert.\nerences, Bradley-Terrymodelisoftenused. Supposethe\ndatasetindicatesthat, foragivenqueryx, humanannotators\n1. https://hturner. github. io/PlackettLuce/\nTree of Reward Model Proximal Regularization\nSFT\nThoughts n Training PO KL-Divergence\na n\nati o\na Preference Reference Reward Direct\nCoT Prompting\npairs: (x, y, y ) policy SFT Δℒ(x, y, y ) Optimization\nReasoning and Expert policy Adversarial\nSFT REINFORCE PO\nActing demonstrations Reward Signal\nLLM Post Multiple paths Advantage PPO+KL KL constraint\nSelf-feedback\ntraining using policy Estimation Regularization Regularization\nEpisodic Offline Terminal Value Function V-guided loss\nMemory Agent trajectories rewards 0,1 Training PO+TTS\nPolicy Long CoT Rejection RL helpfulness\nSelf- Relative PO\nexamples+SFT Sampling & SFT alignment PO\ngninosaer\nemit\necnerefnI\n3.2.3\n6.2.3\n4.2.3\n5.2.3\n7.2.3\n8.2.3\nFHLR\nOPD\nFIALR\nOPRT\nOERO\nOPRG\nFig. 2: Overview of Large Language Models (LLMs) reasoning methods, showcasing pathways for enhancing reasoning\ncapabilities through approaches like Chain-of-Thought (CoT) prompting, self-feedback, and episodic memory. The diagram\nhighlights multiple reinforcement learning-based optimization techniques, including GRPO, RLHF, DPO, and RLAIF, for finetuningreasoningmodelswithrewardmechanismsandpreference-basedlearning.\n3.1.2 Implicit Reward Modeling debugging, in which the path to the answer is as significant\nImplicit reward modeling infers rewards indirectly from ob- asthefinalstatement. Insuchproblems, therewardassigned\nserved behaviors, interactions, or preference signals, often in individual steps encourages transparency and robust stepleveraging machine learning techniques to uncover latent re- by-stepreasoning. However, itrequiresamorecomplexannoward structures. It derives its signals from user interaction tationprocess, e. g., requires gold reasoningstepsorpartial\nmetrics such as upvotes, acceptance rates, click-through pat- creditscoring. Processrewardscanbecombinedwithoutcome\nterns, or session engagement times. While it can accumulate rewardsforastrongmulti-phasetrainingsignal.\nvast datasets with minimal overhead, this approach risks\nfostering behaviors that exploit engagement heuristics at the\nexpenseofcontentqualityorveracity. Policy Reward Modeling(PRM)withlast-stepagReward Function. Definingarewardfunctionfortextgen- gregation outperforms Outcome Reward Modeling\neration tasks is an ill-posed problem . The existing\n(ORM)byleveragingfinal-stepevaluationstooptimizepolicyupdatesmoreeffectively.\nRL methods in LLMs either focus on the generation process\noutcome(OutcomeRewardModeling)orthe(ProcessReward\nModeling), to shape LLM behaviors. We explain these two\nrewardmodelingparadigmsbelow. 3.1.5 Iterative RL with Adaptive Reward Models\n3.1.3 Outcome Reward Modeling to continuously improve the performance of LLMs by iterMeasures the end result (e. g., whether the final answer is atively refining the reward models and the policy model.\nfactually correct or solves the user s query). This model Thisapproachaddressesthechallengesofrewardhackingand\nis straightforward to implement but may offer limited in- reward model drift, which can occur when the reward model\nsight into how the conclusion was reached. It is prevalent becomes misaligned with the desired objectives during largein short-response tasks, where the user s primary concern is scale RL training. The RL process is divided into multiple\nthe correctness or succinctness of the final statement. For iterations, where the model is trained in cycles. After each\nlong-responsetasks, outcomebasedrewardcanleadtocredit iteration, the reward model is updated based on the latest\nassignment problem, i. e., which specific actions or states lead model behavior and human feedback. The reward model is\ntoaparticularrewardoutcome. not static but evolves over time to better align with human\npreferences and task requirements. This adaptation ensures\n3.1.4 Process Reward Modeling that the reward signals remain accurate and relevant as the\nAssigns feedback at intermediate reasoning steps, incentiviz- modelimproves. Repeattheiterativeprocessuntilthemodel s\ning coherent, logically consistent, and well-structured chains performance plateaus or meets the desired benchmarks. The\nof thought. This approach is particularly valuable for tasks rewardmodelandpolicymodelco-evolve, witheachiteration\ninvolving mathematical derivations, legal arguments, or code bringingthemclosertooptimalalignment.\n3.2 Policy Optimization preferred responses (according to human labels) relative to\nOnce we have a trained reward model R (x, y) that captures dispreferred ones. Thekeyideaistolookattheoddsratio:\nhumanpreferences, wecanintegrateitintoaRLframeworkto π (y x)\noptimize apolicyπ ϕ. Inessence, wereplace(oraugment)the π ϕ ϕ (y k j x),\nenvironment s native reward signal with R (x, y) so that the\nwherey\nisthepreferred responseandy\nistheless-preferred\nagivenqueryx.\nresponseforagivenqueryx.\nIntypicalRLnotation:\nPairwisePreferenceProbability. Inmanydirectpreferenceapproaches(e. g., Bradley–Terrystyle), onewrites\n• Each state s here can be interpreted as the partial dialogueorpartialgenerationprocessforthenexttoken(in P (cid:0) y y x (cid:1) = σ (cid:16) ln π ϕ (y j x)(cid:17) = 1,\nlanguagemodeling). ϕ j k π ϕ (y k x) 1+exp (cid:16) lnπϕ(yk x) (cid:17)\nEachactionaisthenexttoken(ornextchunkoftext)to πϕ(yj x)\nbegenerated. whereσ( )isthelogistic(sigmoid)function. Intuitively, ifthe\n• Thepolicy π ϕ (a s)isaconditionaldistributionoverthe policyπ ϕ assignshigherprobabilitytoy j thantoy k, theodds\nnexttoken, parameterizedbyϕ. πϕ(yj x) exceed 1, making y more likely to be the preferred\nWeseektofindϕthatmaximizes theexpectedrewardunder o π uϕt (y cko m x) eunderthemodel. j\nR. Concretely, letxbeauserquery, andlety π ( x)be InORPO, onetypicallydefinesanegativelog-likelihood loss\nθ ϕ\nthegeneratedresponse. Weaimtosolve: forallpairs (x, y y ) inthedataset:\nj k\nmax E h E (cid:2) R (x, y)(cid:3)i. L (ϕ) = X log (cid:16) P (cid:0) y y x (cid:1)(cid:17).\nx X y πϕ( x) θ ORPO ϕ j k\n(x, yj yk) D\nThismeansthatonaverage, overuserqueriesxandresponses\nSubstitutingthelogisticformgives:\nydrawnfromthepolicyπ, wewanttherewardmodel sscore\nR θ (x, y)tobeashighaspossible. L (ϕ) = X log (cid:16) π ϕ (y j x) (cid:17),\nPolicy Gradient and Advantage. Themodernalgorithms ORPO π (y x) + π (y x)\nϕ j ϕ k\n(e. g., PPO, GRPO, TRPO)relyonpolicygradients. (x, yj yk) D\nFigure presents a structured comparison of the these main whichcanalsobeinterpretedasmaximizingthelogoddsratio\nRL frameworks. Each framework builds upon different princi- forthecorrect(preferred)labelineachpairwisecomparison.\nplesforpolicylearning, referencemodeling, andrewardcom- Interpretation via Odds Ratios. By treating each\nh p o u w tat m io u n c. h R b ec e a tt ll e t r h a a n tt a h c e t a io d n va a nt i a s g t e h f a u n nc t t h io e n b A as ( e s l, i a n ) e q e u x a p n e t c ifi te e d s O p R re P f O er p en u c sh e e l s ab t e h l e ( p y j ol icy y k t ) o a i s nc a r c e o a n se st i r t a s in p t ro o b n a t b h il e it o y dd m s a π π sϕs ϕ ( ( o y y k j n x x y ) ) j,\nreturn V(s). At a high level, we update the policy π ϕ in the whiledecreasingitony k. Whenviewedinlogarithmicspace:\nd ad ir v e a c n ti t o a n ge th a a n t d in d c e r c e r a e s a e s s es π ϕ i ( t a fo r s) ne fo g r at a iv c e ti - o a n d s va a nt w a i g t e h a p c o t s i i o t n iv s e. ln (cid:16) π π ϕ ϕ ( ( y y k j x x ) ) (cid:17),\nFormally, theadvantageA attimetcanbewrittenas:\nt ahighervaluecorrespondstoagreaterlikelihoodofselecting\nA t =Q(s t, a t ) V(s t ), y j over y k. Hence, minimizing L ORPO (ϕ) aligns π ϕ with the\nhuman-labeledpreferences.\nwhere Q(s, a ) is the expected future return (sum of future\nt t\nrewards, includingR )startingfroms whentakingactiona.\nθ t t. Odds Ratio Preference Optimization (ORPO)\nWhenusingtherewardmodelR:\nθ is potentially less flexible for combining multi1) We interpret R θ (x, y) as the immediate or terminal re- plerewardsignals.\nwardforthegeneratedresponsey.\n2) The policy s future returns thus factor in how likely\nsubsequenttokensaretobepositivelyscoredbyR. 3.2.2 Proximal Policy Optimization (PPO) in LLMs\n3) The advantage function still captures how much better A popular method for policy optimization is PPO , a\na particular generation step is compared to the baseline strategy adapted to align LLMs with human feedback. Given\nperformanceV(s ). a policy π parameterized by θ and a reward function R,\nt θ\nPPO updates the policy by optimizing a clipped objective\nthatbalancesexplorationandstability. Specifically, ifr (θ)=\nTherewardmodellearnsrelativepreferences t\nπθ(at st) denotes the probability ratio for an action a in\nratherthanabsolutescores. Thisavoidstheneed s π tθaretfe (a s t s, tt ) heclippedPPOobjectiveis: t\nforcalibratedhumanratingsandfocusesonpair- t\nwisecomparisons. LPPO(θ)=E h min(cid:0) r (θ)A, clip(r (θ),1 ϵ,1+ϵ)A (cid:1)i,\nt t t t t\nwhereA isanestimatoroftheadvantagefunctionandϵisa\n3.2.1 Odds Ratio Preference Optimization (ORPO) hyperparameter controlling the allowable deviation from the\nThe simplest method is ORPO which directly optimizing previouspolicy. A iscomputedusingGeneralizedAdvantage\na policy from pairwise human preferences. Instead of first Estimation (GAE) based on rewards and a learned value\nlearningaseparaterewardmodelandthenrunningstandard function. The clipping objective of PPO restricts how drasRL, ORPO updates the policy to increase the likelihood of tically the updated policy distribution can diverge from the\noriginal policy. This moderation averts catastrophic shifts in policy updates while ensuring they remain within a conlanguagegenerationandpreservestrainingstability. strainedtrustregion, measuredbyKLdivergence.\nPolicyOptimizationwithKLPenalty. DuringRLfine- InsteadofusingaclippedobjectivelikePPO, TRPOenforces\ntuningwithPPO, thepolicyπisoptimizedtomaximizereward a hard constraint on policy updates by solving the following\nwhilestayingclosetothebasemodelρ. Themodifiedreward optimizationproblem:\nfunctionincludesaKLdivergencepenalty:\n(cid:20) π (a s ) (cid:21)\nJ(π)=E (x, y) D (cid:2) r(x, y) βKL(cid:0) π( x) ρ( x)(cid:1)(cid:3), m θ ax E t π θo θ ld (a t t t s t ) A t\nwhereβcontrolsthepenaltystrength. TheKLtermKL(π ρ)\nsubjecttotheconstraint:\nprevents over-optimization to the proxy reward r(x, y) (i. e.,\nrewardhacking).\nE [D (π ( s ) π ( s ))] δ.\nt KL θold t θ t\nThe KL penalty is a regularization, which whereδ isahyperparameterthatcontrolshowmuchthenew\nensure policy retains the base model s linguistic policycandivergefromtheoldone.\ncoherenceandavoidsdegenerateoutputs. UnlikePPO, whichapproximatesthisconstraintusingclipping, TRPOdirectlysolvesaconstrainedoptimizationproblem,\nensuring each update does not move too far in policy space.\n3.2.3 Reinforcement Learning from Human Feedback (RLHF)\nHowever, solvingthisconstrainedproblemrequirescomputaRLHF refines LLMs through direct human preference sig- tionally expensive second-order optimization techniques like\nnals, making them more aligned with human expectations.\nconjugate gradient methods, making TRPO less efficient for\nTheprocessinvolvesthreemainsteps. First, SFTisperformed\nlarge-scale models like LLMs. In practice, PPO is preferred\non a pretrained model using high-quality labeled data to\nover TRPO due to its simplicity, ease of implementation, and\nestablish strong linguistic and factual capabilities. Second, a\ncomparableperformanceinlarge-scaleapplicationslikeRLHF.\nrewardfunctionRistrainedusinghuman-annotatedrankings\nHowever, TRPO remains an important theoretical foundation\nofgeneratedresponses, allowingittopredictpreferencesand\nforstablepolicyoptimizationindeepreinforcementlearning.\nprovide a scalar reward signal. Third, PPO is employed in the\nRLHFpipelinebyusinghuman-providedpreferencescores\n(or rankings) to shape R and thereby guide the policy up- 3.2.6 Direct Preference Optimization (DPO)\ndates. Thisensuresthatthemodelprioritizesoutputsaligned DPO is a recently proposed method for training LLMs\nwith human-preferred behavior. The robust performance un- from human preference data without resorting to the tradider conditions of noisy or partial reward signals makes PPO tional RL loop (as in RLHF with PPO). Instead of learning a\nwell-suitedfortextgenerationtasks, wherelargeactionspaces separate reward function and then running policy-gradient\nandnuancedrewarddefinitionsarecommon. updates, DPOdirectlyintegrateshumanpreferencesignalsinto\nthe model s training objective. So instead of the above PPO\n3.2.4 Reinforcement Learning from AI Feedback (RLAIF) objective, DPO instead constructs an objective that directly\nRLAIF is an alternative to RLHF that replaces human pushes up the probability of a chosen (preferred) response\nannotation with AI-generated feedback. Instead of relying (y+) while pushing down the probability of a less-preferred\non human-labeled preferences, RLAIF employs a secondary, response (y ), all within a single log-likelihood framework.\nhighly capable language model to generate preference labels, Rather than bounding policy changes with clip, the DPO loss\nwhich are then used to train a reward model. This reward uses the difference between log probabilities of winning vs.\nmodel guides reinforcement learning-based fine-tuning of the losing responses. Thisexplicitlyencodestheuser spreference\ntarget model. RLAIF reduces the cost and time required for intheupdatedparameters.\ndata collection by eliminating the need for human annotaHere, π is the learnable policy, π is a reference policy\ntors. Itenableslarge-scalemodelalignmentwithoutrequiring θ ref\n(often the SFT-trained model), σ( ) is the sigmoid function,\nextensive human intervention while maintaining high perforβ is a scaling parameter, and D is a dataset of triplets\nmance and alignment. Empirical studies indicate that RLAIF (x, y+, y )wherey+ isthepreferr tr e a d in outputovery.\nisascalableandefficientalternativetoRLHF, making\nit a promising direction for reinforcement learning-driven h (cid:16) π (y+ x)\nlanguagemodeloptimization. LDPO(θ)=E ((x, y+), y ) Dtrain σ βlog π θ (y+ x)\nref\nThe clipping mechanism constrains policy π (y x) (cid:17)i\nβlog θ.\nupdates to remain within a safe trust region, π (y x)\nref\nwhichiscrucialwhendealingwithcomplex, highdimensionalactionspaces. The key insight is that an LLM can be treated as a hidden\nrewardmodel: wecanreparameterizepreferencedatasothat\nthemodel sownlogprobabilitiesreflecthowpreferableonere3.2.5 Trust Region Policy Optimization (TRPO) sponseisoveranother. Bydirectlyadjustingthelog-likelihood\nTRPOisanotherwidelyusedpolicyoptimizationmethod, of more-preferred responses relative to less-preferred ones,\npreceding PPO and sharing its fundamental goal: improving DPO sidesteps many complexities of RL-based methods (e. g.,\nstability in reinforcement learning updates. TRPO optimizes advantagefunctionsorexplicitclipping).\nfine-grained credit assignment. The core objective minimizes\nPPO\nback-prop theinconsistencyinthesoftBellmanequation:\nReference\nq Policy o Reward GAE A V (s ) V (s )=r(s, a ) βlog π θ (a t s t ),\nϕ t ϕ t+1 t t π (a s )\nValue v ref t t\nupdate wheres =f(s, a )isthenextstate, risthesparsereward,\nt+1 t t\nandβ controlsKLregularization. Thepolicyandvaluelosses\nGRPO KL are:\nReference\nq Policy Reward Com G p r u ou ta p tion L V (ϕ)= T 1 T X 1 V ϕ (s t ) R t +β X log π π θ ( ( a a i s s i ) )\nref i i\nt=0 i t\nDPO update L π (θ)= T 1 T X 1(cid:18) V ϕ (s t ) R t +βlog π π θ ( ( a a t s s t ) ) (cid:19)2 +αL reg,\nref t t\nt=0\nDPO where L penalizes deviations from π, and α balances\nq Policy r reg ref\nobjective regularization.\nReference Preference\nModel Data OREO s explicit value function enables testtimebeamsearch(e. g., selectinghigh-valuereaFig. 3: Comparison of PPO , GRPO , and DPO . soningsteps)anditerativetraining, wherefailed\nWe highlight policy models, reference models, rewards and trajectoriesrefinethepolicy. Thiscontrastswith\noptimizationflowswithcorrespondinglossfunctions. DPO implicit value function, which lacks stepwise\ncreditassignment.\nTheadvantagefunctionA ϕ =V ϕ (s t+1 ) V ϕ (s t ). OREO scomputationalcostscaleswithtrajecquantifies per-step contributions, critical for\ntorylengthandvalue-modeltraining. Whileefidentifyingkeyreasoningerrors. Thisgranularity\nfective for math/agent tasks, its generalization\nis lost in DPO, which treats entire trajectories\ntobroader domains(e. g., coding) requiresvaliuniformly.\ndation. Iterative training also demands careful\ndata curation to avoid overfitting to failure\nPerplexity Filtering for Out-of-Distribution Data. To modes.\nensure DPO training data is on-distribution (aligned with ρ),\nresponses are filtered using perplexity. The perplexity of a\nresponsey=(y, y,..., y )isdefinedas: 3.2.8 Group Relative Policy Optimization (GRPO)\nGRPO simplifies the PPO framework by eliminating the\nX T need for a separate value function. Instead, GRPO estimates\nPP(y)=exp T logP ρ (y i y i ), the baseline from the average reward of multiple sampled\ni=1 outputs for the same question. The primary contribution in\nwhere y is the i-th token. Only responses with perplexity GRPO is that it removes the need for a separate value model\ni (criticmodel)andinsteadestimatesthebaselinerewardfrom\nbelow a threshold (e. g., the 95th percentile of ρ-generated\nresponses)areretained. a group of sampled LLM outputs. This significantly reduces\nmemory usage and stabilizes policy learning. The approach\nalso aligns well with how reward models are trained, i. e.,\nThe advantage function remains a core con- by comparing different LLM-generated outputs rather than\ncept to determine which actions (token choices) predictinganabsolutevalue.\narebetterthanthebaselineateachstep. For each question q, GRPO samples a group of outputs\no, o,..., o from the old policy πold. A reward model\nG θ\nis used to score each output in the group, yielding rewards\n3.2.7 Offline Reasoning Optimization (OREO) r 1, r 2,..., r G. The rewards are normalized by subtracting\nthegroupaverageanddividingbythestandarddeviation:\nOREO is an offline reinforcement learning method designed to enhance LLMs multi-step reasoning by optimizing\nr =\nmean(r)\nthe soft Bellman equation . Unlike DPO, which relies i std(r)\non paired preference data, OREO uses sparse rewards based\non final outcomes (e. g., correctness of reasoning chains) and The advantage A ˆ for each token in the output is set as the\ni, t\njointly trains a policy model π and a value function V for normalizedrewardr.\nθ ϕ i\nGRPOfirstsamplesaquestionq P(Q)andthensamples Inthisformulation, eachresponsey isjointlyevaluatedinthe\nG outputs o G from πold(O q). Define the per-output context of all other responses, ensuring that comparisons are\ni i=1 θ\nobjectiveas notisolatedpairwiseeventsbutratherpartofabroaderranking framework that helps capture more nuanced preferences\nJ(o,θ, q)= 1 X oi min n r A ˆ, andreducespotentialbiases.\ni o ratio, i, t i, t\nt=1\nclip(cid:0) r,1 ϵ,1+ϵ (cid:1) A ˆ o 3.3 Pure RL Based LLM Refinement\nratio, i, t i, t The work from Guo et al. introduces two main\nh i models: DeepSeek-R1-ZeroandDeepSeek-R1.\nβD π π.\nKL θ ref • DeepSeek-R1-Zero operates with a purely ReinforcementLearningapproach, excludinganySFT.\nThen, theGRPOobjectivebecomes\n• DeepSeek-R1 incorporates cold-start data and applies\n\" 1 X G # amulti-stagetrainingpipeline.\nJ GRPO (θ)=E q P(Q) G J(o i,θ, q), Themethodologyencompassesseveralsteps(SeeFigure2\ni=1 inGRPOformainsteps): collectingcold-startdata, performwheretheprobabilityratioisdefinedas ingRLtraining, carryingoutSFT, usingdistillationtotransfer\nπ (o q, o ) knowledge to smaller models, and addressing specific chalr ratio, i, t πo θ ld(o i, t q, o i, t ). lenges such as language mixing and readability. This multiθ i, t i, t stage pipeline ensures robustness and alignment with human\nwhere ϵ is a clipping hyperparameter akin to PPO, and β preferences, while distillation enables efficient deployment of\nadjuststheKL-divergencepenaltyencouragingthenewpolicy\nsmallermodelswithoutsignificantperformanceloss.\nπ not to deviate excessively from a reference policy π,\nθ ref\nwhich is typically the initial supervised fine-tuned (SFT) 3.3.1 Cold-Start RL Phase\nmodel. GRPOcanbeappliedintwomodes: outcome\nThe process begins with a cold-start RL phase, where a small\nsupervisionandprocesssupervision.\namount of curated data is gathered to fine-tune an initial, or\nOutcome Supervision: Provides a reward only at the base, model. Followingthispreliminaryfine-tuning, RLisconend of each output. The advantage A ˆ i, t for all tokens in the ducted—oftenviaalgorithmslikeGRPOuntilconvergence. The\ncold-startphaseiscriticalforstabilizingthemodelbeforefull\nr mean(r) RL training, preventing instability that can arise from purely\nr i = i std(r). RL-driven updates. The cold-start data preparation focuses\non capturing human-readable reasoning patterns to prevent\nProcess Supervision: Provides a reward at the end of\neach reasoning step. The advantage A ˆ for each token is instabilityfrompurelyRL-drivenupdates. Thisstepgenerates\ni, t CoT-styleexampleswithconsistent reasoning_process\ncalculated as the sum of the normalized rewards from the\nand summary fields, usually involving thousands of\nfollowingsteps:\ncarefully curated samples. Structured CoT formats and conA ˆ = X r, sistentfieldsensureclarityandrobustnessinthemodel sreai, t i, index(j)\nsoning outputs, reducing errors and improving interpretabilindex(j) t\nity.\nwhereindex(j)istheendtokenindexofthej-thstep.\nOverall, GRPOservesasanefficientalternativetoclassicactorcritic frameworks in DeepSeekR1 by leveraging group- ProvidingCoTreasoningtracesbeforeRLtrainlevel advantages, thereby reducing training costs without ingestablishesa strongerfoundationforreasonsacrificing the capacity to distinguish fine-grained differences ing tasks, enhancing both robustness and interamongcandidateresponses. pretabilityofoutputs.\nFine-grained per-step rewards enable the 3.3.2 Rejection Sampling and Fine-tuning\nmodel to effectively identify and reinforce high- ThisconceptisalsousedinWebGPT. OnceRLstabilizes,\nquality responses, boosting overall performance arejectionsamplingmechanismisemployedtogeneratehighincomplex, multi-stepreasoningtasks. quality responses that are subsequently filtered for correctness, clarity, and other quality metrics. These filtered responsesarethenblendedwithadditionaldatasetstoproduce\n3.2.9 Multi-Sample Comparison Optimization\na new, larger corpus for Supervised Fine-Tuning. Rejection\nInstead of relying solely on single-pair comparisons, multisampling ensures that only high-quality outputs are used for\nsample comparison optimization approach compares\nfurther training, enhancing the model s overall performance\nmultiple responses simultaneously to promote diversity\nand mitigate bias. Specifically, given a set of responses and reliability. After RL converges for high-stakes reasoning\ntasks, rejection sampling is used to filter a large number of\ny, y,..., y foraqueryx, theprobabilityofobservingthe\nn generated outputs, expanding the training set. These newly\nrankingy y y isdeterminedbytheproduct\nn generated reasoning examples (potentially up to hundreds of\nP(y y y ) = Y eR(x, yi). thousands in quantity) are mixed with existing SFT data to\nn P eR(x, yj) create a combined dataset of substantial size (often around\ni j\nEfficient Finetuning and Deployment\nAccelerators Co-Optimized Architectures\n(Groq, vLLM, Triton, (FlashAttention, BlockSparse capabilities, enabling smaller models to achieve\netc.) IO, DeepSeek v3 etc.)\ncompetitive performance with reduced compuParallel Computing, Data Compression, Data tationaloverhead.\nDistributed Training System Data Filtering (TokenMerging,\n(LoRA, PEFT, RecapDataComp-18,\nDeepSpeed, etc.) etc.)\nScaling law, Data mining\nModel compression Model\n(Bitsandbite, GPTQ, (Chinchilla, RETRO, C4 4 Supervised Finetuning in LLMs\ndata, etc.)\netc.)\nAs shown in Figure, finetuning forms a basic component of\nFig.4: ThisVenndiagramillustratestheinterplaybetweenSys- LLM post-training recipes. In this section, we summarize the\ntem, Data, and Model for efficient finetuning and deployment.\ndifferenttypesof LLMfine-tuningmechanisms.\nItcoversstrategieslikeaccelerators(Groq, vLLM), adaptation\n(LoRA, PEFT), co-optimizedarchitectures(FlashAttention), data\ncompression (TokenMerging), scaling laws (Chinchilla), and 4.1 Instruction finetuning\nmodelcompression(GPTQ)toboostperformanceandscalability.\nIninstructionfinetuning, amodelistrainedoncuratedpairs\nofinstruction(prompt)andresponse(completion). Themain\n800k samples). Rejection sampling and dataset expansion goal is to guide the LLM to follow a user-provided instruction\naccurately and helpfully, regardless of the task domain. This\nsignificantly enhance the model s coverage of general tasks\nusually involves compiling large, diverse instruction-response\nwhilepreservingitsreasoningproficiency.\ndatasetscoveringmanytasktypes(e. g., summarization, QA,\nclassification, creative writing). Models such as T0 ,\n3.3.3 Reasoning-Oriented RL\nFLAN , Alpaca , Vicuna and Dolly \nThereasoning-orientedRL leveragesGRPO, whichsamples\ndemonstrate how instruction-finetuned LLMs can outperform\na group of outputs from the current policy and computes\nbase models on zero-shot or few-shot tasks by virtue of their\nrewards and advantages for each output. Rewards may be\nenhancedinstruction-followingabilities.\ncomputed via rule-based checks, e. g., ensuring correct solutions in math or code tasks, enforcing structured CoT tags,\nandpenalizingundesiredlanguagemixing. GRPOgroup-based 4.2 Dialogue (Multi-turn) Finetuning\nsampling and reward computation ensure that the model\nprioritizeshigh-quality, structuredoutputs, enhancingitsreaSomeLLMsundergodialogue-stylefinetuningtobetterhandle\nmulti-turn conversations. Different from instruction tuning\nsoningcapabilities.\ndescribed above, here the data takes the form of a continuous dialogue (multi-turn conversations) instead of a single\n3.3.4 Second RL Stage for Human Alignment\nprompt-responsepair. Inthisapproach, trainingdataconsists\nA second RL stage further aligns the model with broader of chat transcripts with muliple user queries and system rehumanpreferences(helpfulness, harmlessness, creativity, etc.) sponses, ensuringthemodellearnstomaintaincontextacross\nby introducing additional reward signals and prompt distri- turnsandproducecoherentreplies. ModelslikeLaMDA\nbutions. The second RL stage ensures the model aligns with and ChatGPT highlight how dialogue-tuned LLMs can\nhuman values, making it more versatile and contextually feel more interactive and context-aware. While dialogue fineaware. After re-training the base model on this combined tuningcanoverlapwithinstructionfinetuning(becausemany\ndataset, a second round of RL can be conducted to align instructions come in a chat format), specialized conversation\nthe model more closely with human preferences (e. g., for dataoftenyieldsmorenatural, multi-turnuserexperiences.\nhelpfulness and harmlessness). This RL stage fine-tunes the\nmodel to better align with human values, ensuring outputs\narenotonlyaccuratebutalsocontextuallyappropriate. 4.3 CoT Reasoning finetuning\nChain-of-Thought (CoT) reasoning finetuning teaches models\n3.3.5 Distillation for Smaller Models to produce step-by-step reasoning traces instead of just final\nFinally, distillationtechniquesareusedtotransfertherefined answers. By exposing intermediate rationales or thoughts,\ncapabilities of the main model to smaller architectures, en- CoT finetuning can improve both interpretability and accuabling more efficient deployments without sacrificing much racy on complex tasks (e. g., math word problems, multiperformance. It allows smaller models to inherit advanced hop QA). In practice, CoT finetuning uses supervised reareasoningcapabilities, makingthemcompetitiveonchalleng- soning annotations (often handcrafted by experts) to show\ningbenchmarkswithoutthecomputationalcostsoffull-scale how a solution unfolds. Notable early work includes ChainRL training. Finally, distillation plays a pivotal role: the top- of-Thought Prompting and Self-Consistency , which\nperforming model, DeepSeek-R1 , serves as a teacher to initially applied the idea to prompting; subsequent efforts\nsmaller architectures (e. g., Qwen or Llama families, ranging (e. g., Chain-of-Thought Distillation ) adapt it to a full\nfrom1.5Bto70Bparameters). Thistransferallowsthesmaller finetuning or student-teacher paradigm. These efforts have\nmodels to inherit advanced reasoning capabilities, making also been extended to the multimodal domain, e. g., LlaVAthem competitive on challenging benchmarks without incur- CoT and LlamaV-o1 where image, QA and CoT\nringthecomputationalcostsoffull-scaleRLtraining. reasoningstepsareusedinLLMfinetuning.\nModel Category Source Description\n1. Parameter-EfficientFine-Tuning&ModelCompression\nLoRA Low-RankAdaptation Link Injectstrainablelow-rankadaptersforefficientfine-tuning.\nQLoRA QuantizedAdaptation Link Combines4-bitquantizationwithLoRAtoenablefine-tuningonconsumerGPUs\nGPTQ Post-TrainingQuantization Link Optimal4-bitquantizationmethodforGPT-stylemodelswithminimalloss\nSparseGPT Pruning Link One-shotpruningthatpreservesmodelqualitywithcompensation.\nPEFT(HF) UnifiedFine-Tuning Link LibraryintegratingLoRA, prefixtuning, andotherparameter-efficientmethods\nBitsAndBytes Low-PrecisionTraining Link Enables8-bitoptimizersand4-bitquantizationformemory-efficienttraining\nAdaLoRA AdaptiveAdaptation Link Dynamicallyallocatesparameterbudgetbetweenlayersduringfine-tuning\nP-Tuningv2 PromptOptimization Link Learnscontinuouspromptembeddingsthroughdeepprompttuning\n2. DataManagement&Preprocessing\nHFDatasets DataProcessing Link UnifiedAPIfor30k+datasetswithstreaming, versioning, andpreprocessing\nWebDataset DataStreaming Link Efficienttar-basedshardingformatforpetascaledistributedtraining\nDVC DataVersioning Link Git-likeversioncontrolfordatasetsandmachinelearningpipelines\nApacheArrow MemoryFormat Link Language-agnosticcolumnarmemoryformatforzero-copydataaccess\nZstandard Compression Link High-speedcompressionalgorithmfortrainingdatastorage/transfer\nCleanlab DataQuality Link Automaticdetectionoflabelerrorsandoutliersintrainingdatasets\n3. DistributedTraining&Optimization\nDeepSpeed TrainingOptimization Link ZeROparallelism,3Dparallelism, andmemoryoptimizationsforgiantmodels\nMegatron-LM ModelParallelism Link NVIDIA soptimizedframeworkforlargetransformermodeltraining\nColossal-AI HeterogeneousTraining Link Unifiedsystemsupportingmultipleparallelizationstrategies\nHorovod DistributedTraining Link MPI-inspiredframeworkformulti-GPU/multi-nodesynchronization\nRay DistributedComputing Link UniversalframeworkfordistributedPythonapplicationsatscale\n4. EfficientInference&Deployment\nvLLM ServingOptimization Link Pagedattentionimplementationforhigh-throughputLLMserving\nTensorRT GPUOptimization Link NVIDIA sinferenceoptimizerwithkernelfusionandquantizationsupport\nTriton ServingFramework Link Production-gradeservingwithconcurrentmodelexecutionsupport\nONNX Cross-Platform Link Unifiedinferenceenginewithhardware-specificoptimizations\nOpenVINO IntelOptimization Link RuntimeforIntelCPUs/iGPUswithpruning/quantizationsupport\nXNNPACK MobileInference Link Highlyoptimizedfloating-pointkernelsforARMCPUs\nGroq AIAccelerator Link Deterministiclow-latencyinferenceviacustomtensorstreamingprocessor\n5. IntegratedDevelopmentEcosystems\nHFEcosystem FullStack Link Transformers+Datasets+Accelerate+InferenceEndpoints\nDeepSpeed Training/Inference Link Microsoft send-to-endsolutionforbillion-parametermodels\nPyTorch UnifiedFramework Link NativeLLMsupportviatorch. compileandscaleddot-productattention\nLLMReasoners AdvancedReasoning Link EnhancesLLMreasoningcapabilitiesusingadvancedsearchalgorithms.\nTABLE2: ComprehensiveOverviewofMethodsandFrameworksemployedinModernLLMs\n4.4 Domain-Specific (Specialized) Finetuning with smaller datasets. This approach can yield lighter, faster\nWhen an LLM needs to excel in a specific domain (e. g., modelsthatretainmuchoftheteacher sperformance, evenin\nbiomedicine, finance, or legal), domain-specific finetuning is zero-shotorfew-shottasks.\nused. Here, a curated corpus of domain-relevant text and labeledexamplesisemployedtofinetunetheLLM. Forinstance,\n4.6 Preference and Alignment SFT\nBioGPT and BiMediX specialize in biomedical\nWhile RLHF is not purely supervised, it starts with a suliterature, FinBERT for financial texts, ClimatGPT\npervisedpreference oralignment finetuningstage. Thisstage\n for climate and sustainability and CodeT5 \nuses human-labeled or human-ranked examples to teach the\nfor code understanding. Supervised finetuning in these domodel about desirable vs. undesirable outputs (e. g., safe vs.\nmainsoftenincludesclassification, retrieval, orQAtaskswith\ntoxic). By training on these explicit preferences, the model\ndomain-specificdata, ensuringthemodel sparametersadapt\nbecomes more aligned with user values, reducing harmful or\ntothespecializedlanguageandconceptsofthefield. Domainoff-topic completions. Works like InstructGPT illustrate\nspecificfinetuningisalsoextendedtovision-languagemodels\nsuch as, finetuned on remote sensing imagery, on\ntrainingandRLupdatesbegin.\nmedicalimagingmodalities,onspatiotemporal\nvideoinputs, andadaptedforchartunderstanding.\n4.7 Efficient Finetuning\n4.5 Distillation-Based Finetuning Fully finetuning a LLM can be computationally and memoryLarge teacher modelsaresometimesusedtoproducelabeled intensive, particularly as model sizes grow into the tens or\ndata or rationales, which a smaller student model finetunes hundreds of billions of parameters. To address these chalon, this is generally called knowledge distillation . lenges, parameter-efficientfinetuning(PEFT)techniquesintroIn the context of LLMs, CoT Distillation is one example duceasmallsetoftrainableparametersorlearnableprompts\nwhere a powerful teacher LLM generates intermediate rea- while leaving most of the model weights frozen. Approaches\nsoning steps, and the student LLM is finetuned to reproduce such as LoRA , Prefix Tuning , and Adapters \nboth the final answer and the reasoning chain. Step-by-step exemplify this strategy by injecting lightweight modules (or\ndistillation generates descriptive rationales alongside prompts) in specific layers, thus significantly reducing the\nfinal answers to train smaller models through distillation memoryfootprint.\nprobability paths. By limiting the beam width (N), it manTest Time Scaling\nages the exponential search space while aiming to find a\nnear-optimal sequence. These beams are expanded at each\nScaling Advanced Improved Sequential decoding step to find multiple probable paths. In reasoning\nStrategies Sampling Reasoning revision\nLLMs, such paths allow us to systematically explore multiple\nBeam Confidence Chain-of-Thought Self-Consistency reasoning chains in parallel, focusing on the most promising\nSearch Decoding\nBased Prompting ones. This ensures that high-likelihood reasoning steps are\nSampling\nM Tr o e n e t S e e C a a r r c l h o Tree-of-Thoughts Self-Improvement considered, whichcanimprovethechancesoffindingacorrect\nSearch via Refinements and coherent solution compared to greedy decoding. It has\nAgainst\nBest-of-N\nSearch Verifiers compute-optimal scaling strategy traditionally been used in tasks such as translation, summarization, and code generation, where the goal is a highly\nFig. 5: An overview of Test-time Scaling methods: parallel\nprobablecorrectsequence.\nscaling, sequential scaling, and search-based methods. It also\nWhile modern LLMs often favor stochastic sampling (e. g.,\nshowshowtheyintegrateintoacompute-optimalstrategy.\ntemperaturesampling)topromotediversityingeneratedtext,\nbeamsearchisstillavaluabletechniqueforstructuredreasoning problems. For example, the Tree-of-Thoughts framework\nFigure4illustrateshowthesetechniquesfitintoabroader\n allows plugging in different search algorithms to explore\necosystemthatinvolvessystem-leveloptimizations, datamana tree of possible thoughts or reasoning steps; usually a\nagement, and evaluation strategies for LLMs. In particular,\nbeam search (with beam width b) is used to maintain the\nPEFT approaches can be combined with quantization and\nb most promising states at each reasoning step. Here, beam\npruning methods to further minimize memory\nusage and compute overhead, enabling finetuning on smaller\nlike mathematical puzzles and planning problems, pruning\nGPUsorevenconsumer-gradehardware. Forinstance, QLoRA\nless promising reasoning branches and thus improving the\nunifies 4-bit quantization with low-rank adaptation, while\nmodel s problem-solving accuracy. Beam search remains a\nBitsAndBytesprovides8-bitoptimizerstomakeLLMtraining\nstrong baseline for test-time reasoning when one wants the\nmorepracticalinconstrainedenvironments(Table2).\nmodel to output the single most likely reasoning path or\nMoreover, these PEFT methods still require supervised\nanswerunderthemodel slearneddistribution.\ndata to guide the adaptation process, but the reduction in\nthe number of trainable parameters makes it more feasible\nto use in-domain or task-specific datasets. This is especially 5.2 Best-of-N Search (Rejection Sampling)\nvaluable for specialized domains (e. g., medical or software Best-of-N (BoN) search generates N candidate outputs\ndevelopment), where data might be limited or expensive to (usually via sampling) and then picks the best one according\nannotate. As shown in Table, PEFT (HF) integrates several toachosencriterion(e. g., arewardmodelorthemodel sown\nof these approaches (LoRA, prefix tuning, and more) into a likelihood). Conceptually, thisisanapplication\nsingle library, streamlining deployment in both research and ofrejectionsampling: onedrawsmultiplesamplesandrejects\nproductionsettings. all but the top-rated result. Unlike Beam Search ,\nwhich incrementally expands and prunes partial hypotheses,\nBoNsimplysamplesfullsolutionsindependently, allowingfor\nCombining efficient tuning designs like LoRA greater diversity but at a higher computational cost. Beam\nand QLoRA with system and data optimizations Search systematically aims for the most probable sequence,\n(Figure4)enablescost-effectiveLLMadaptation\nwhile BoN may capture high-quality but lower-probability\nfor tasks like domain-specific text generation,\nsolutionsthroughbrute-forcesampling.\nwithoutexpensivefullfine-tuning.\nBeam search (effective for harder questions)\nTest-time Scaling Methods outperforms best-of-N sampling at low compute\nbudgets, while best-of-N scales better for easier\nWhileRLfine-tunesthemodel spolicy, test-timescaling(TTS)\ntasks.\nenhances reasoning during inference typically without model\nupdates. Figure presents a taxonomy of TTS methods,\nDuringLLMinference, BoNisusedtoenhancecorrectnessor\ncategorizingthembasedontheirunderlyingtechniques.\nalignmentwithoutretrainingthemodel. Bysamplingmultiple\nanswers and selecting the top candidate (e. g., via a reward\n5.1 Beam Search model or a checker), BoN effectively boosts accuracy on tasks\nBeam search was first introduced in the context of speech like QA or code generation. BoN is easy to understand and\nrecognition. Itgainedprominenceasadecodingstrategy implement and is almost hyper-parameter-free, with N being\nforsequencemodelsandwaslateradoptedinneuralmachine the only parameter that can be adjusted at inference. In\ntranslation and speech systems . With the popularity of reinforcement learning contexts, BoN sampling can serve as\nLLMs, thisalgorithmhasbeenusedforapproximatesearchin a baseline exploration mechanism i. e., to generate many rollmanytextgenerationtasks. outs, pick the best outcome according to the learned reward,\nThe concept of Beam search is similar to pruned breadth- and proceed, although at increased computational overhead.\nfirst search, where top N highest-probability partial se- OpenAI s WebGPT used BoN to pick the best response via\nquences (the beam ) are kept at each step, discarding lower- a reward model, yielding strong QA performance . BoN\nisalsousedasasimplealignmentmethodthatishighlycom- Wei et al. demonstrated CoT s effectiveness on arithpetitivewithotherpost-trainingtechniquese. g., RLHFand meticandlogictasks, showinglargegainsoverdirectpromptDPO. StudieshaveshownBoNcanapproachormatchRLHF ing. Kojima et al. introduced Zero-Shot CoT, revealing\nresults when guided by a sufficiently robust reward model that even adding a simple phrase like Let s think step\n. Alternativessuchasspeculativerejectionbuild by step can trigger coherent reasoning in sufficiently large\non this idea and utilize a better reward model to improve models. Subsequentworks(e. g., Wangetal.,2022)comefficiency. The studies also highlight issues of reward hacking bined CoT with sampling-based strategies (Self-Consistency)\nif the (proxy) reward function used for BoN is imperfect forevenhigheraccuracy. AsdescribedinSec.5.4, CoTformat\norinstabilityissuesiftheNparametergetsverylarge. data have also been used for SFT and are shown to help\nreshapethemodelresponsestobemorestep-by-step.\nChoice of either process reward models with\nbeam search vs best-of-N depends on the diffi- Fine-tuningmodelstoreviseanswerssequencultyandcomputebudget. tiallyallowsthemtobuildonpreviousattempts,\nimproving accuracy over time. This approach is\nparticularly effective for easier questions, while\nparallelsampling(exploration)provesmoreben5.3 Compute-Optimal Scaling\neficialforharderones.\nThe Compute-Optimal Scaling Strategy (COS) is a dynamic method designed to allocate computational resources\nefficientlyduringinferenceinLLMs, optimizingaccuracywith- 5.5 Self-Consistency Decoding\noutunnecessaryexpense. Insteadofapplyingauniformsam- Self-Consistency is a decoding strategy introduced by Wang\npling strategy across all inputs, this approach categorizes et al. . It was proposed as an alternative to simple\nprompts into five difficulty levels—ranging from easy to greedy decoding for chain-of-thought prompts. It built upon\nhard—eitherbyleveragingoracledifficulty(ground-truthsuc- the idea of sampling multiple distinct reasoning paths for a\ncess rates) or model-predicted difficulty (e. g., verifier scores question and was the first to show that marginalizing over\nfrom Preference Ranking Models). Once categorized, the those paths can significantly improve accuracy on arithmetic\nstrategy adapts compute allocation: easier prompts undergo and reasoning problems. In other words, it allows the model\nsequential refinement, where the model iteratively refines to think in many ways and then trust the consensus, which\nits output to improve correctness, while harder prompts improvescorrectnessinmanyreasoningscenarios.\ntrigger parallel sampling or beam search, which explores The self-consistency method works by sampling a diverse\nmultiple response variations to increase the likelihood of set of reasoning chains from the model (via prompt engifinding a correct solution. This dual approach balances ex- neering to encourage different CoTs, and using temperature\nploration (for challenging inputs) and refinement (for near- sampling) and then letting the model output a final answer\ncorrect responses), ensuring optimal performance per unit foreachchain. Insteadoftrustingasinglechain, themethod\nof computational effort. Remarkably, this method achieves selectstheanswerthatismostconsistentacrossthesemultiple\nfour times lower compute usage than traditional best-of-N reasoningpaths, effectivelyamajorityvoteorhighestprobabilsamplingwhilemaintainingequivalentperformance. Thekey ity answer after marginalizing out the latent reasoning. The\ninsightisthatbymatchingcomputationalstrategytoproblem intuition is that if a complex problem has a unique correct\ndifficulty, it avoids wasted resources on trivial cases while answer, different valid reasoning paths should converge to\nensuring sufficient sampling diversity for complex tasks. In that same answer. By pooling the outcomes of many chains,\nessence, itfunctionsasa smartthermostat forLLMinference, the model can decide which answer is most supported. In\ndynamically adjusting computational effort in response to application, onemightsample, e. g.,20CoTsforamathprobinputcomplexity, leadingtoamoreefficientandcost-effective lem and see what final answer appears most frequently; that\ndeploymentoflarge-scalelanguagemodels. answer is then taken as the model s output. This approach\nturns the one-shot CoT process into an ensemble where the\nmodel cross-verifies its answers. It is especially useful for\nCOS achieves 4 efficiency gains over best- arithmeticandcommonsensereasoningtaskswherereasoning\nof-N baselines by optimally balancing sequen- diversityhelps.\ntial/parallel compute. Beam search + revisions\noutperform larger models on easy/intermediate\nquestions. Smaller models with test-time compute can\noutperform much larger models in certain scenarios.\n5.4 Chain-of-thought prompting\nSelf-consistency is often combined with other methods:\nCoTpromptinginducesLLMstoproduceintermediatereason- e. g., sampling multiple chains and then applying a verifier\ning steps rather than jumping directly to the final answer. to the most common answer. Its strength lies in requiring no\nBy breaking down problems into logical sub-steps, CoT taps new training, only extra sampling, making it a popular testintoamodel slatentabilitytoperformmulti-stepinferences, time scaling strategy to obtain more reliable answers from\nsignificantly improving performance on tasks like math word LLMs. It has also inspired other variants, e. g., Universal Selfproblems, logicalpuzzles, andmulti-hopQA. Consistencyextendtheoriginalidea(whichworkedonly\nDirect CoT Self-consistancy Multiple CoT ToT GoT\nInput Input Input Input Input Input\nOutput\nNot graded\nPositive graded\nNegative graded\nPositive graded\nVoting\nBack tracking\nOutput Output Output Output Output\nSelf-refining\nFig.6: ThisfigurecomparesreasoningstrategiesinLLMs, evolvingfromDirectPrompting, whichmapsinputtooutputwithout\nreasoning, to more structured approaches. Chain-of-Thought (CoT) introduces step-by-step reasoning, while Self-Consistency\n(CoT-SC) generates multiple CoT paths and selects the most frequent answer. Multiple CoTs explores diverse reasoning paths\nindependently. Tree-of-Thoughts(ToT)structuresreasoningasatree, enablingbacktrackingandrefinement, whereasGraphof-Thoughts(GoT)generalizesthisbydynamicallyaggregatingandconnectingthoughts. Thelegenddecipherskeymechanisms\nlikegrading, backtracking, andself-refinement, crucialforoptimizingreasoningefficiency.\nwith majority vote on single final answer) to more general combined with ToT: differentLLM agents generate thoughts\ngenerationtaskssuchassummarizationandopen-endedQA. in parallel and a validator agent prunes incorrect branches,\nleadingtoimprovedaccuracyoverthesingle-agentToT.\n5.6 Tree-of-thoughts\nToTframeworkgeneralizesthechain-of-thoughtapproach Inference-time computation for LLMs can outby allowing the model to branch out into multiple possible performscalingmodelparameters, especiallyfor\nthoughtsequencesinsteadoffollowingasinglelinearchain. It challengingreasoningtaskslikemathproblems.\nthusformulatestheproblemoflanguage-modelreasoningasa\ntreesearch, drawingonclassicAIsearchmethodsinspiredby\nhumanproblem-solving. TreeofThoughtstreatsin5.7 Graph of Thoughts\ntermediatereasoningstepsas nodes inasearchtreeanduses\nthe language modelto expand possible next steps (thoughts) The Graph of Thoughts (GoT) framework extends the\nfrom a given state. Rather than sampling one long reasoning ToTbyallowingmoreflexibleandefficientreasoningprocesses\npath, the model explores a tree of branching thoughts and throughgraph-basedstructuresratherthanstricthierarchical\ncan perform lookahead and backtracking. At each step, the trees. Thought representation differs between the two apLLM might generate several candidate next thoughts, and a proaches: in ToT, each step in reasoning is structured as a\nheuristic or value function evaluates each partial solution node in a tree with fixed parent-child relationships, whereas\nstate. Thenasearchalgorithm(e. g., depth-first, breadth-first, GoT represents thoughts as nodes in a graph, enabling more\nbeam search) navigates this tree, deciding which branches to adaptabledependenciesandinterconnections.\nexplore further. This approach allows systematic exploration In terms of thought expansion strategies, ToT follows a\nof different reasoning strategies: if one path leads to a dead- traditional approach where multiple thought candidates are\nend, themodelcanreturntoanearlierstateandtryadifferent generated at each step, explored using tree-based search\nbranch (unlike standard CoT which commits to one line of strategies, andprunedbasedonheuristicsbeforeselectingthe\nreasoning). Ineffect, ToTisaniterativepromptingprocedure mostoptimalpath. Incontrast, GoTincorporatesgraph-based\nwhere the model generates thoughts, evaluates them, and thoughtexpansion, allowingthoughtstointerconnectdynamrefinesitsapproach, mimickinghowahumanmightmentally ically. This enables three key transformations: aggregation\nmapoutvariouswaystosolveaproblem. (mergingmultiplesolutionsintoaunifiedanswer), refinement\nToTisespeciallyusefulforcomplexproblemslikepuzzles, (iteratively improving thoughts over time), and generation\nplanning tasks, or games where multiple steps and strategic (producingdiversecandidates). Insteadofnavigatingthrough\nexplorationareneededandoutperformssimplerCoTmethods a rigid hierarchy, GoT prioritizes thoughts using a volume\nbysystematicallysearchingthroughthesolutionspace. Itpro- metric and explores paths optimally, reducing unnecessary\nvidesaflexibleframework–onecanpluginvariousgeneration computations.\nstrategies(e. g. samplingvs. prompting)andsearchalgorithms A critical limitation of ToT is its restricted backtrack-\n(BFS, DFS, A, MCTS) depending on the task. Although ing—once a branch is discarded, it is not reconsidered. GoT\nmorecomputationallyheavy, ToTshowsthatallocatingextra overcomes this by allowing iterative refinement, where previ-\nthinking time (compute) to explore alternatives can yield ous thoughts can be revisited, modified, and improved upon.\nsignificantly better reasoning and planning performance. It This iterative nature is particularly useful in complex reahas spawned follow-up research aiming to improve or utilize soningtaskswhereinitialthoughtsmayrequireadjustments.\nit for better reasoning e. g., multi-agent systems have been Moreover, computational efficiency in GoT is significantly\nimproved by reducing redundant calculations through the 2) Process Reward Models (PRM): Evaluate the reasonmergingofpartialsolutions. ing steps (e. g., logical coherence in a thought chain),\nprovidinggranularfeedbacktopruneinvalidpaths.\nSeveral techniques fall under this paradigm, enhancing\nGoT enhances problem-solving efficiency and\nverification-based optimization. Best-of-N Sampling involves\nadaptability, making it superior to ToT for tasks\ngenerating multiple answers and ranking them via a verifier\nrequiringcomplexreasoning.\n(ORM/PRM), selectingthehighest-scoringone, makingitasimple yet effective approach for improving answer correctness.\n5.8 Confidence-based Sampling Beam Search with PRM tracks top-scoring reasoning paths\n(beams) and prunes low-quality steps early, similar to Tree\nIn confidence-based sampling, the language model generates\nof Thought approaches, balancing breadth and depth in reamultiple candidate solutions or reasoning paths and then\nsoning path exploration. Monte Carlo Tree Search balances\nprioritizes or selects among them based on the model s own\nexplorationandexploitationbyexpandingpromisingreasonconfidence in each outcome . This can happen in two\ningbranches, simulatingrollouts, andbackpropagatingscores,\nways:(a)Selection: GenerateNoutputsandpicktheonewith\nproviding an optimal trade-off between search depth and\nthe highest log probability (i. e., the model s most confident\nverification confidence. Majority Voting (Self-Consistency)\noutput). This is essentially best-of-N by probability – the\naggregates answers from multiple samples and selects the\nmodel chooses the answer it thinks is most likely correct.\nmost frequent one, avoiding explicit verifiers, which works\n(b) Guided exploration: When exploring a reasoning tree or\nwell in settings where consistency across multiple responses\nmulti-step solution, use the model s token probabilities to\nindicatescorrectness.\ndecide which branch to expand (higher confidence branches\nareexploredfirst). Inotherwords, themodel sprobabilityestimatesactasaheuristicguidingthesearchthroughsolution ORM is suitable for tasks where correctness is\nspace. Comparedtopurerandomsampling, confidence- binary(right/wrong)andcanbeeasilyassessed.\nbased methods bias the process toward what the model believesisright, potentiallyreducingwastedexplorationonlowlikelihood(andoftenincorrect)paths.\nPRMisusefulinmulti-stepreasoning, ensuring\nConfidence-based strategies have been incorporated at\nintermediatestepsfollowslogicalprogression.\ninferencetimee. g., atree-basedsearchforLLMgeneration\nassignseachpossiblecompletion(leaf)aconfidencescore. The\nalgorithm samples leaves in proportion to these confidence\n5.10 Self-Improvement via Refinements\nscores to decide which paths to extend . Similarly, some\nThis approach refers to the ability of LLMs to enhance their\nreasoning approaches use the model s estimated likelihood of\noutputs through self-evaluation and revision iteratively. This\nan answer to decide when to halt or whether to ask a followprocess enables models to refine their responses dynamically\nup question – essentially if the model s confidence is low,\nduring inference rather than relying solely on pre-trained\nit might trigger further reasoning (a form of self-reflection).\nweights. One notable method is Self-Refinement ,\nConfidence-based selection is also used in ensemble settings:\nwhere an LLM generates an initial response, critiques it, and\ne. g., an LLM may generate multiple answers and a secondary\nthen refines the output based on its self-generated feedback.\nmodel evaluates the confidence of each answer being correct,\nThis iterative process continues until the model achieves a\npicking the answer with the highest confidence. This was\nsatisfactory result. Such techniques have been shown to imexplored in tasks like medical Q&A, where an LLM gave an\nprove performance on various tasks, including mathematical\nanswer and a confidence score, and only high confidence\nreasoningandcodegeneration. Thisprocessfollowsthesekey\nanswersweretrustedorreturned.\nsteps: a)InitialGeneration: Themodelproducesananswer\nor reasoning path. b) Self-Critique: The model reviews its\n5.9 Search Against Verifiers ownresponseandidentifieserrors, inconsistencies, orareasfor\nThis verification approach in LLMs enhances answer improvement. c)Refinement: Themodeladjustsitsresponse\nqualitybygeneratingmultiplecandidateresponsesandselect- based on the critique and generates an improved version.\ning the best one using automated verification systems. This d) Iteration: The process repeats until the output meets a\napproach shifts focus from increasing pre-training compute predefinedqualitythresholdorstopsimproving.\nto optimizing test-time compute, allowing models to think Another approach is called Self-Polish , where the\nlonger during inference through structured reasoning steps modelprogressivelyrefinesgivenproblemstomakethemmore\noriterativerefinement. Themethodinvolvestwomainsteps: comprehensible and solvable. By rephrasing or restructuring\nGeneration: The model (or proposer produces multiple problems, the model enhances its understanding and proanswers or reasoning paths, often using methods like high- vides more accurate solutions. Self-Polish involves progrestemperaturesamplingordiversedecoding. sive refinement of problem statements to make them more\nVerification: Averifier(e. g., arewardmodel)evaluatesthese comprehensible and solvable. The model first rephrases or\ncandidates based on predefined criteria, such as correctness, restructurestheproblemforbetterclarity, thenbreaksdown\ncoherence, or alignment with desired processes. Verifiers are complex queries into simpler sub-problems and refines amcategorizedbasedontheirevaluationfocus: biguous inputs to ensure precise understanding. By restruc1) OutcomeRewardModels(ORM): Judgeonlythefinal turing problems before solving them, the model improves its\nanswer(e. g., correctnessofamathsolution). comprehensionandgeneratesmoreaccuratesolutions.\nof possible reasoning paths and pick a high-reward answer\nSelf-improvement methodologies represent a path, outperformingnaivesamplinginascientificQ&Atask.\nparadigm shift in LLM optimization, emphasiz- Similarly, MCTShasbeenappliedtocodegenerationwithLLMs\ning active reasoning and internal feedback over –thealgorithmexploresdifferentcodepaths(usingthe\nstatic pre-training. By iterating on their own modeltoproposecodecompletionsandetestthem)tofinda\nresponses, models achieve greater consistency correctsolution. AnotherlineofworkensemblesmultipleLLMs\nandaccuracyacrossawiderangeofapplications. withMCTS, treatingeachmodel soutputasabranchandusing\narewardmodeltosimulateoutcomes. Earlyresultsshow\nthatMCTS-basedreasoningcansolveproblemsthatsingle-pass\n5.11 Monte Carlo Tree Search or greedy methods often miss, although with more compute\nMCTS is based on the application of Monte Carlo sim- . The downside is that MCTS can be significantly slower\nulations to game-tree search. It rose to prominence with than straightforward sampling or beam search, which recent\nsuccesses in games, notably, it powered AlphaGo in research is addressing by improving efficiency (e. g., by state\nby searching possible moves guided by policy and value merging). Ingeneral, MCTSbringsthestrengthofplanning\nnetworks. This, aswellastheapplicationtootherboardand algorithmstoLLMinferenceandenablesanLLMto lookahead\nvideo games, demonstrates the power of MCTS for sequential throughsimulatedrolloutsandmakemoreinformedreasoning\ndecision-makingunderuncertainty. choices, muchlikeithasdoneforAIingameplay.\ntreebyperformingmanyrandomsimulations. Itisbestknown\nTest-timecomputeisnota1-to-1replacement\nforfindinggoodmovesingamestates, butitcanbeappliedto\nfor pretraining but, offers a viable alternative in\nanyproblemwherewecansimulateoutcomes. Thealgorithm\nmanycases.\niteratively: (a) Selects a path from the root according to a\nheuristic(likeUCT, whichpicksnodeswithahighupperconfidence bound), (b) Expands a new node (a previously\n5.12 Chain-of-Action-Thought reasoning\nunvisitedstate)fromtheendofthatpath,(c)Simulatesarandomrolloutfromthatnewstatetogetanoutcome(e. g., win LLMs excel in reasoning tasks but rely heavily on external\nguidance (e. g., verifiers) or extensive sampling at inference\nor loss in a game, or some reward), and (d) Backpropagates\ntheresultupthetreetoupdatethevaluesofnodesandinform time. Existing methods like CoT lack mechanisms for selfcorrectionandadaptiveexploration, limitingtheirautonomy\nfuture selections. Repeating these simulations thousands of\nandgeneralization. Satoriintroducedatwo-stagetraintimesconcentratesthesearchonthemostpromisingbranches\ning paradigm, which works by initially tuning the model s\nofthetree. Inessence, MCTSusesrandomsamplingtoevaluate\noutput format and then enhancing its reasoning capabilities\nthe potential of different action sequences, gradually biasing\nthrough self-improvement. In Stage 1 (Format Tuning), the\nthe search towards those with better average outcomes. In\nmodel is exposed to a large set of 10K synthetic trajectories\nLLM reasoning, we can treat the generation of text as a\ngeneratedbyamulti-agentframeworkcomprisingagenerator,\ndecision process and use to explore different continuations.\na critic, and a reward model. This supervised fine-tuning\nFor example, at a given question (root), each possible next\nhelps the model to produce outputs in specific reasoning\nreasoning step or answer is an action; a simulation could\nformat using meta-action tokens, although it may still have\nmean letting the LLM continue to a final answer (perhaps\ndifficultygeneralizingbeyondtheseexamples. InStage2(Selfwith some randomness), and a reward could be whether the\nansweriscorrect. Bydoingthisrepeatedly, MCTScanidentify Improvement via RL), the model employs PPO with a Restart\nand Explore strategy , which allows it to restart from\nwhich chain of thoughts or answers has the highest empirical\nintermediatesteps, whethertheywerecorrectornot, torefine\nsuccess rate. The appeal of MCTS for reasoning is that it can\nitsreasoningprocess. Themodelreceivesrewardsbasedona\nhandle large search spaces by sampling intelligently rather\ncombinationofrule-basedcorrectness, reflectionbonuses, and\nthan exhaustively, and it naturally incorporates uncertainty\npreference-basedOutcomeRewardModelfeedbackexplained\nandexploration.\nin 5.9, thereby incentivizing the allocation of more computationalresourcestotougherproblemsandenablingextended\nTrain verifiers to score intermediate steps reasoningduringtestingforcomplextasks.\n(via Monte Carlo rollouts) instead of just final Multi-agent frameworks and advanced fine-tuning strateanswers. gies are increasingly being explored to enhance reasoning\nin LLMs. Multi-Agent LLM Training (MALT) introduces\nRecent efforts have integrated MCTS with LLMs to tackle a structured approach where generation, verification, and\ncomplexreasoninganddecision-makingtasks. Oneexampleis refinement steps are distributed across specialized agents,\nusing MCTS for query planning: Monte Carlo Thought Search allowing for iterative self-correction and improved reasoning\n, whereanLLMisguidedtoaskaseriesofsub-questionsto chains. Similarly, optimizing preference alignment remains\nfindananswer. Jayetal.usedanMCTS-basedalgorithm a crucial challenge in ensuring both safety and helpfulness\ncalled Monte Carlo Reasoner that treats the LLM as an in LLMs . Approaches like Bi-Factorial Preference Optienvironment: each node is a prompt (state) and each edge mization (BFPO) reframe RLHF objectives into a single\nis an action (e. g., a particular question to ask or step to supervised learning task, reducing human intervention while\ntake), and random rollouts are used to evaluate outcomes. maintaining robust alignment. Beyond text-based reasonThisapproachallowedthesystemtoefficientlyexploreaspace ing, multimodalapproacheslikeMultimodalVisualization-of19\nThought(MVoT)extendCoTpromptingbyincorporating TABLE 3: Comprehensive Overview of Reasoning, RL Alignvisual representations, significantly enhancing performance ment, andMultilingualDatasets. Here, pointwiseandpairwise\nrefer to different methods of evaluating model performance\nin spatial reasoning tasks. These advancements highlight the\nacrossvarioustasks.\ngrowingneedforstructuredmulti-agentcollaboration, safetyaware optimization, and multimodal reasoning to address\nDatasets Domain Type #Samples EvaluationCriteria\nfundamentallimitationsinLLMreasoning. ReasoningBenchmarks\nMATH MathReasoning Pointwise 7,500 Step-by-stepsolutions\nGSM8K MathReasoning Pointwise 8.5K Multi-stepreasoning\n5.13 Pretraining vs. Test-Time Scaling M W e o t r a ld M T a r t e h e Q V A 2 ] Ma S t c h ie R nc e e as Q on A ing P Po oi i n n t t w w i i s s e e 4 1 0,6 K 8 + 0 S M elf u - l v t e i- r h ifi o c p a e ti x o p n l, a F na O t B io A ns R\nPangeaBench MultimodalReasoning Pairwise 47Langs. Culturalunderstanding\nPretrainingandTTSaretwodistinctstrategiesforimproving MMMU Science/Math PointwiseCollege-Level Physics, Chemistry, Bilingual\nTruthfulQA QA/Reasoning Pointwise N/A Truthfulness\nLLM performance, each with different tradeoffs in compu- MathInstruct MathReasoning Pointwise 262K Correctness\nMMLU MultitaskReasoning Pointwise 57Tasks Broadknowledgeevaluation\ntational cost and effectiveness. Pretraining involves scaling MMLU-Fairness Fairness/Reasoning Pointwise N/A Bias/EquityAnalysis\nDROP Reading/Reasoning Pointwise 96K Discretereasoningoverparagraphs\nmodel parameters or increasing training data to enhance BBH HardReasoning Pairwise N/A Complexlogicalproblem-solving\nVRC-Bench MultimodalReasoning Pairwise N/A VisualReasoningandClassification\ncapabilities, requiring substantial upfront computational in- RLAlignmentBenchmarks\nvestment . In contrast, TTS optimizes inference-time com- H An el t p h S r t o e p e i r cH RLHF R R L L A A l l i i g g n n m m e en nt t P P a ai ir r w w i i s s e e 3 4 7 2 K.5 + K H M a u rm lti l - e a s t s t n r e ib ss ut a e lig sc n o m ri e n n g t\nUltraFeedback RLAlignment Pairwise 64K Instruction-following, Truthfulness\npute (such as iterative refinements, search-based decoding, D4RL RL/Control Pointwise N/A OfflineRLacrossdomains\nMeta-World RL/Control Pointwise N/A Multi-taskroboticRL\nor adaptive sampling), allowing performance improvements MineRL RL/Games Pairwise N/A Imitationlearning, rewards\nwithoutmodifyingthebasemodel. MultilingualEvaluation\nCulturaX Multilingual Pointwise 6.3T Deduplication, Quality\nFrom a performance vs. cost perspective, TTS achieves PangeaIns Multilingual Pointwise 6M Multilingualinstructions\nTydiQA Multilingual Pointwise N/A Cross-lingualQA\nresults comparable to a model 14 larger on easy to in- XGLUE Multilingual Pointwise N/A Cross-linguallanguagetasks\nMM-Eval Multilingual Pairwise 4,981 Task-orientedmultilingualQA\ntermediate tasks (e. g., MATH benchmarks), while reducing ALM-Bench MultilingualQA Pointwise N/A MultilingualEvaluation\ninference costs by 4 fewer FLOPs in compute-intensive\nBigBench GeneralComprehensionPointwise 200+Tasks Broadmulti-domainevaluation\nscenarios . However, pretraining remains superior for C M h T a B tb e o n t ch Ar C C o o m m p p r r e e h h e e n n s s i i o o n n P P a a i ir r w wi is s e e 3 3 3 K K Mult U i-t s u er rn pr c e o f n e v re e n rs c a e tions\nRewardBench Comprehension Pairwise 2,998 Userpreference\nthe hardest tasks or when inference compute constraints are\nhigh, as larger pretrained models inherently encode deeper ConvAI2 Dialogue Pointwise N/A Engagingness, Consistency\nMultiWOZ Dialogue Pointwise N/A Tasksuccess, Coherence\nreasoningcapabilities. TrecDL21&22 Search Pointwise 1,549/2,673 Relevancescoring\nBEIR Search Pointwise 18Datasets Informationretrieval\nStory&RecommendationBenchmarks\nHANNA Story Pointwise 1,056 Relevance, Coherence, Complexity\nA smaller model with test-time compute can S P t K or U y - E S R afe HF V S a t l o u r e y s P P a a i ir r w wi is s e e 8 1 3 0. 0 4 K K Us H er el p p r f e u f l e n r e e s n s c, e H -b a a rm se l d es r s a n n e k s i s ng\nCvalue Values Pairwise 145K Safety, Responsibility\noutperforma14 largermodeloneasy/interme- NaturalInst. InstructionTuning Pointwise 1,600+ Instruction-followingevaluation\ndiate questions, when inference tokens (Y) are\nlimited(e. g., self-improvementsettings).\ngeneralcomprehension, anddialogueandsearchtasks. Awellstructured evaluation framework ensures a comprehensive\nIn terms of use cases, TTS is useful for scenarios with understanding of an LLM strengths, and limitations across\nflexibleinferencebudgetorwhenbasemodelsalreadyexhibit various tasks. These benchmarks play a crucial role in LLM\nreasonablecompetenceinthetask. Conversely, pretrainingis post-processingstages, wheremodelsundergofine-tuning, calessential for tasks requiring fundamentally new capabilities ibration, alignment, andoptimizationtoimproveresponseac-\n(e. g., reasoningonnoveldomains)whereinference-timeopti- curacy, robustness, and ethical compliance. Next, we explain\nmizationsalonemaynotsuffice. themainbenchmarkgorups. Table3providesanoverviewof\nThere are notable tradeoffs between the two approaches.\nkeydatasetscategorizedunderthesebenchmarkgroups.\nTTS reduces upfront training costs, making it attractive for ReasoningBenchmarks. ThesebenchmarksassessLLMson\nflexible, on-the-go optimization, but requires dynamic comtheir ability to perform logical, mathematical, and scientific\npute allocation at inference. Pretraining, on the other hand,\nreasoning. MathematicalreasoningdatasetslikeMATH,\nGSM8K , and MetaMathQA test models on\nwithout additional runtime overhead, making it ideal for\nproblem-solving, multi-step arithmetic, and theorem-based\nlarge-scaleAPIdeploymentsorlatency-sensitiveapplications.\nproblem formulations. Scientific and multimodal reasoning\nOverall, TTS and pretraining are complementary in nature. benchmarks such as WorldTree V2 and MMMU \nFuture LLM systems may adopt a hybrid approach, where evaluate knowledge in physics, chemistry, and multimodal\nsmaller base models are pretrained with essential knowledge,\nunderstanding, which are crucial for fact-checking and veriwhile TTS dynamically enhances responses through adaptive, fication processes in LLM-generated responses. Additionally,\non-demand computation. This synergy enables more costdatasets like PangeaBench extend reasoning tasks into\neffectiveandefficientlarge-scalemodeldeployment.\nmultilingual and cultural domains, enabling models to refine\ncross-lingual reasoning. These benchmarks help determine\nChoose pretraining for foundational capabillogicaldeductions.\nities and test-time scaling for accurate contextawarerefinement. RL Alignment Benchmarks. RL alignment benchmarks\nare central to LLM alignment and post-training optimization. Theyrefineresponsegeneration, ethicalconstraints, and\nBenchmarks for LLM Post-training Evaluation user-aligned outputs through RLHF. Datasets such as HelpTo evaluate the success of LLM post-training phases, a di- Steer and UltraFeedback evaluate models based\nverse set of benchmarks have been proposed covering mul- on multi-attribute scoring and alignment with user instructiple domains: reasoning tasks, alignment, multilinguality, tions. Anthropic s HH-RLHF explores how well mod20\nels learn human preference optimization through reinforce- and uncertainty-aware RL methods beyond correlation\nment learning with human feedback. D4RL and Meta- with human uncertanity that safeguard user trust and\nWorld focus on robotic control and offline RL, which prevent adversarial attacks. Another crucial area involves\nhave implications for autonomous model decision-making. personalization and adaptation (FigMineRL extends RL testing into complex environments ure7e), whereeffortstotailorLLMsforspecificdomainsmust\nsuchasMinecraft-basedinteractions, usefulfortrainingLLMs be balanced against risks to privacy , particularly when\ninadaptivedecision-makingsettings. enterprisedataorsensitivepersonalinformationisinvolved.\nMultilingual Evaluation. Multilingualbenchmarksarees- In parallel, process vs. outcome reward opsentialforLLMpost-processingincross-lingualgeneralization, timization(Figure7f)remainsanopenquestion: while\ntranslation adaptation, and fine-tuning for low-resource lan- process-based rewards help guide incremental improvements,\nguages. CulturaX and PangeaIns evaluate tok- outcome-focusedmetricsaresimplerbutmaynotcapturecruenization, translation, and instruction-following in over 150 cialintermediatedecision-makingsteps. Beyondrewardstruclanguages, ensuring fairness and diversity in model outputs. ture, fine-tuningLLMsonnewtasksstillencounterissueslike\nTydiQA and MM-Eval target bilingual and task- catastrophic forgetting and potential data leakage\noriented multilingual evaluation, enabling improvements in , underscoringtheneedforparameter-efficientmethLLMfine-tuning. ThesedatasetsensurethatLLMsarenotjust ods and privacy-preserving strategies such as differential\nEnglish-centricbutoptimizedformultilingualadaptability. privacy and federated learning . Human feedback,\nGeneral Comprehension Benchmarks. General compre- while central to alignment, is inherently costly and limited\nhensionbenchmarkscontributetomodelfine-tuning, response in scope; methods like Constitutional AI and RLAIF\ncoherence, and preference optimization. Datasets such as seek to automate parts of this oversight, though they\nChatbotArena, MTBench, andRewardBench introduce fresh concerns about bias calibration and\ntest user preference modeling and conversational fluency, model self-consistency . Finally, test-time scaling \ncrucial for LLM response ranking and re-ranking methods. anddynamicreasoningframeworksposefurtherchalBigBenchevaluatesbroadmulti-domaincomprehension, lenges: modelsmustlearnwhentoallocatemorecomputation\nwhile MMLU measures correctness and informa- for complex queries, how to adapt verification modules \ntiveness. These datasets help in refining LLM fluency, factual efficiently, and how to maintain robust performance even\ncorrectness, andopen-endedresponsegeneration. whenfacingadversarialinputs. TheseconvergingresearchdiDialogue and Search Benchmarks. Dialogue and search rections—spanning rewardmodeling, decoding strategies, inbenchmarksplayakeyroleinoptimizing LLMretrieval-based terpretability, personalization, andsafefine-tuning—highlight\nresponses, multi-turncoherence, andinformationretrievalac- themultifacetedroleof RLinLLMsandcollectivelyshapethe\ncuracy. DatasetssuchasConvAI2andMultiWOZ future trajectory of large-scale language model development.\nevaluate multi-turn conversational models, essential for di- Below, wedelveintosomeofthesedirectionsingreaterdetail.\nalogue history tracking and adaptive response fine-tuning. Fine-tuning challenges. Fine-tuning remains one of the\nFor search relevance assessment, BEIR provides large- most direct post-training methods to adapt LLMs to specific\nscale human-annotated judgments for retrieval fine-tuning, tasks or domains, yet it faces several open challenges. One\nensuringLLMsgenerateandrankresponseseffectively. TREC fundamentalissueiscatastrophicforgetting–whenupdating\nDL21/22contributestodocumentrelevanceranking an LLM on new data causes it to lose or degrade previously\nandfactretrieval. learned capabilities. Even advanced PEFT methods like LoRA\n, which greatly reduce the number of trainable weights,\ndo not fully solve this problem . Future work can ex7 Future Directions\nplore better continual learning strategies and regularization\nWe gathered all papers related to post-training methods techniques so that models can acquire new skills without\nin LLMs and analyzed their trends, as shown in Figure. erasing old ones. For example, new fine-tuning algorithms\nApplication of RL techniques for refining the (e. g. CURLoRA ) explicitly aim to stabilize training and\nLLMs have a noticeable increase in prominence since 2020 preserve prior knowledge while adding new tasks. Promising\n(Figure), emphasizing the demand for interactive ap- researchdirectionsincludecurriculum-basedfine-tuning\nproaches such as human-in-the-loop reinforcement (introducing new facts gradually or in context with known\nand scalability . At the same time, reward facts)andhybridtrainingthatcombinesretrievalorexternal\nmodeling (Figure) has seen a steady rise knowledge bases. For instance, rather than solely adjusting\nin interest due to the emergence of self-rewarding language the model s weights, one could fine-tune LLMs to consult a\nmodels, yet the field still struggles with reward hacking knowledge repository or perform tool use (such as database\n and the design of robust , failure-aware re- queries or computations) when faced with queries outside\nwardfunctionsbeyondrewardhacking. Decoding and their original training distribution . This retrievalsearch(Figure7c)methodsincludetree-of-thoughtsand augmented fine-tuning could let models incorporate\nMonte Carlo strategies aiming to enhance model freshinformationatinferencetime, reducingtheneedtooverreasoning through iterative self-critique , but writetheirinternalweightswithnewfacts. Anotherapproach\nthese techniques also demand reliable uncertainty estima- is training models to explicitly represent uncertainty about\ntors to prevent excessive computational overhead . new knowledge, thereby enabling them to say I don t know\nSafety , robustness , and interpretability or defer to an external source if a query concerns content\n have likewise become central concerns (Fig- not seen in pre-training. By blending weight updates with\nure 7d), motivating the development of bias-aware external knowledge integration, future fine-tuned LLMs will\n(b) Reward modeling trends show RLHF (c) Decoding strategies like Tree-of-\n(a) Growing trend in RL for LLMs, with stabilization, with Self-Rewarding Models ThoughtsandMCTSareimprovingLLM\nafocusonHuman-in-the-LoopRL. leading, butRewardHackingpersists. reasoninganddecision-making.\n(d) Safety and Robustness research is (e)PersonalizationandAdaptationfocus (f) Process Reward Modeling dominates\ngrowing, with Uncertainty-Aware RL en- on Privacy-Preserving RLHF. On-device Outcome-Based Optimization, favoring\nsuringRLHFmodelreliability. adaptationremainsachallenge. iterativestrategiesforRL-basedLLMs.\nFig.7: YearlyTrendsinRLspecificpost-trainingmethodsforLLMsandemergingresearchdirections.\nmaintainhigherfactualaccuracyandlowerhallucinationrates itself aligned and correct? There is a risk of feedback loops\nonemerginginformation. oranechochamberofbiasesiftheautomatedpreferencesare\nSafe Fine-tuning. From an ethical and safety perspective, flawed. An open gap is the creation of robust AI feedback\nfine-tuning raises important open research questions. Fine- systems that are calibrated to human values (perhaps perituning data often contains sensitive or proprietary informa- odically grounded by human oversight or by a diverse set\ntion , which can lead to privacy risks if the model mem- of constitutional principles). The blending of human and AI\norizes and later regurgitates that data. A recent comprehen- feedbackinahierarchicalschemecouldprovideascalableyet\nsive survey highlights vulnerabilities in the fine-tuning reliableRLparadigmforLLMs.\nstage, such as membership inference attacks (detecting if a\nspecificrecordwasinthefine-tuningset)anddataextraction Test-time scaling challenges. Open challenges in TTS re-\n(recovering parts of the fine-tuning data from the model s volve around how to orchestrate the inference-time processes\noutputs). Mitigatingtheserisksisanopenproblem: methods efficientlyandreliably. Akeyquestionishowmuchcomputing\nlike differential privacy fine-tuning (adding noise to the is enough for a given query, and how to determine this on\nweight updates) and federated fine-tuning (where data never thefly? Usinglessresourcescanresultinmistakes, butusing\nleavesuserdevicesandonlyaggregatedupdatesaresenttothe too much is inefficient and could introduce inconsistencies.\nmodel) are being actively explored. However, these methods Recent research by Snell et al. tackled it by proposing\noften come at the cost of model utility or require careful a unified framework with a Proposer and a Verifier to\ncalibrationtoavoiddegradingperformance. systematically explore and evaluate answers. In their frameLimitations of Human Feedback. Human feedback is work, theProposer(usuallythebaseLLM)generatesmultiple\ncostly and subjective. One promising avenue to address the candidate solutions, and the Verifier (another model or a\nlimitations of human feedback is using AI feedback and heuristic) judges and selects the best. The optimal strategy\nautomation to assist or replace human evaluators. Constitu- can vary by problem difficulty: for easier queries, generattionalAI, introducedbyAnthropic, isanotableexample: ing many answers in parallel and picking the top might be\ninstead of relying on extensive human feedback for every sufficient, whereas for harder problems, sequential, step-byharmful or helpful behavior, the model is guided by a set of step reasoning with verification at each step works better.\nwritten principles (a constitution ) andis trained to critique An important future direction is building adaptive systems\nand refine its own responses using another AI model as the where the LLM dynamically allocates computation based on\njudge . Emerging directions here include RLAIF and an estimate of the question s complexity. This idea connects\nothersemi-automatedfeedbacktechniques: usingstrong tometa-cognitioninAI, enablingmodelstohaveasense\nmodels to evaluate or guide weaker models, or even having of what they don t know or what deserves more thought.\nmultiple AI agents debate a question and using their agree- Developingreliableconfidencemetricsordifficultypredictors\nment as a reward signal . Such AI-aided feedback for LLMs is an open research area, but progress here would\ncould vastly scale the tuning process and help overcome the make TTS far more practical i. e., the model would only slow\nbottleneck of limited human expert time. However, it raises downandthink whennecessary, muchlikeahumanspending\nnew theoretical questions: how do we ensure the AI judge is extra time on a hard problem. Additionally, By reframing\ninference-timescalingasaprobabilisticinferenceproblemand encryption during inference; differential privacy via reward\nemployingparticle-basedMonteCarlomethods[?], thesmall noising, whichintroducesmathematicallyboundednoise\nmodelsachievedo1levelaccuracyinonly32rollouts, a4–16x intoRLHFpreferencerankingsduringalignment; andfederated\nimprovementinscalingefficiencyacrossvariousmathematical distillation, which aggregates knowledge from decentralized\nreasoning tasks. Recent study shows distilling test-time user-specificmodelswithoutsharingrawdata.\ncomputations into synthetic training data creates synergistic Collaborative Multi-Model Systems. As single-model\npretrainingbenefitswhichcanalsobefurtherexplored. scaling approaches physical limits, alternaReward Modeling and Credit Assignment. Current RL tive paradigms such as multi-agent LLM collaboration become necessary. Researchers are investigating\nelsover-optimizesuperficialproxymetricsratherthangenuine emergent communication protocols that train models to dereasoning quality. The sparse nature of terminal rewards velop lossy compression languages for inter-model knowlin multi-step tasks increases credit assignment challenges, edgetransfersuchasGenAINet, robustensembleswhere\nparticularly in long-horizon reasoning scenarios. Traditional stress-testinducedspecializationdrivesautomaticdivisionof\nmethods like DPO require inefficient pairwise preference data problem spaces based on failure analysis , and gradientandfailtoutilizefailuretrajectorieseffectively. Hybridreward freesynergylearningthroughevolutionarystrategiesdesigned\nmodels can be investigated by integrating process supervi- todiscovercomplementarymodelcombinationswithoutrelysion with outcome-based rewards using contrastive stepwise ingonbackpropagation.\nevaluation . This approach enables a more granular as- Multimodal RL Integration. Multimodal reinforcement\nsessmentofintermediatedecision-makingstepswhilealigning learning faces the obstacle of a combinatorial\nwith long-term objectives. Recent work suggests step- stateexplosion, especiallyincontextsexceeding128ktokens.\nlevel policy optimization could improve value function ac- Pioneering methods to overcome this include hierarchical\ncuracy while maintaining safety constraints. Dynamic credit attention frameworks that employ modality-specific policies\nassignment mechanisms can be explored through temporal with cross-attention gating , adaptive truncation stratedifference learning adapted for transformers . Such giesthatcompresscontextwhilepreservingcriticalreasoning\nadaptationsmayenhancethemodel sabilitytocapturelong- segments, andflashcurriculumapproachesthatleverage\nrange dependencies and optimize reward propagation over self-supervisedcomplexitypredictiontofaciliextended sequences. Failure-aware training strategies can be tateprogressivemultimodalintegration.\ndevelopedbyincorporatingnegativeexamplesintotheRLloop Efficient RL Training. Efficient RL training paradigms convia adversarial data augmentation . This can improve tinue to be a critical research frontier as current methmodelrobustnessbysystematicallyexposingittochallenging ods exhibit significant sample inefficiency and computational\nscenariosandencouragingmoreresilientpolicylearning. overhead. Addressing issues like the overthinking \nEfficient RL Training and Distillation. Current RL meth- phenomenon, where excessive reasoning chains waste valuodsforLLMsrequireprohibitivecomputationalresources able computation , requires approaches such as partial\nwhile often underperforming knowledge distillation tech- rollout strategies , adaptive length penalty mechanisms\nniques . This inefficiency limits scalability and practical employing learned compression transformers, and hybrid ardeployment, asdistilledmodelsfrequentlysurpassRL-trained chitecturesthatcombineMCTSwithadvancedRLoptimizers.\ncounterparts despite requiring less training overhead. Addi- These innovations are essential for scaling RL to long-context\ntionally, pure RL approaches struggle to balance language taskswhileminimizingwastedcomputationalresources.\nquality with reasoning improvement , creating a performanceceiling.\nThe development of hybrid frameworks that initialize RL\n. OverthinkingPhenomenon: Analysisreveals\npolicieswithdistilledknowledgefromlargemodels, combining 22% wasted computation is in reasoning chains\ntheexploratorybenefitsof RLwiththestabilityofsupervised exceedingoptimalreasoninglength.\nlearningisaninterestingdirection. Similarly, curriculumsampling strategies that progressively increase task complexity RLmethodsexhibitsampleinefficiencyandcomputational\nwhile using distillation to preserve linguistic coherence can overhead, particularly when scaling to contexts exceeding\nalso help. PEFT methods can be leveraged during RL up- 128k tokens. The overthinking phenomenon, where models\ndatestomaintainbasecapabilitieswhileenhancingreasoning. generate excessively long reasoning chains, further reduces\ntoken efficiency and increases deployment costs . Investigate partial rollout strategies with flash attention mechIntegration: Combining PRM-guided tree anisms for long-context processing. Develop length penalty\nsearch with online distillation achieves 4 effi- mechanismsusinglearnedcompressiontransformersforiteraciency gains over baseline methods, while main- tive long2short distillation. Hybrid architectures combining\ntaining94%solutionaccuracyonMATHdataset. MCTS with GRPO could enable better explorationexploitationtradeoffs. ParallelworkbyXieet. al.demonPrivacy-Preserving Personalization. Customizing mod- strates promising results through adaptive tree search prunels for enterprise and individual use cases raises the risk of ing. Several open challenges persist in the field. Uncertainty\nexposingprivatetrainingdatathroughmemorization, making propagation remains problematic as current confidence estiprivacy-preserving adaptation essential. Promising so- matorsaddapproximately18%latencyoverhead, whilecataslutions include homomorphic instruction tuning , which trophic forgetting rresults in a degradation of 29% of base\nprocessesencrypteduserquerieswhilemaintainingend-to-end capabilitiesduringRLfine-tuning. Moreover, benchmark\nsaturation is an issue, with MMLU scores correlating poorly (r A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle,\n=0.34)withreal-worldperformance. A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan,\net al., The llama 3 herd of models, arXiv preprint\narXiv:2407.21783,2024. 1,5\n. Adversarial Vulnerabilities: Stress tests re- G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ramé,\nveal a high success rate on gradient-based\netal., Gemma2: Improvingopenlanguagemodelsatapractipromptinjections. calsize, arXivpreprintarXiv:2408.00118,2024. 1,5\n G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut,\nJ. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, etal., Gemini:\nafamilyofhighlycapablemultimodalmodels, arXivpreprint\narXiv:2312.11805,2023. 1,5\nThissurveyandtutorialprovidesasystematicreviewofpost- A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr,\nC. Ruan, D. Dai, D. Guo, et al., Deepseek-v2: A strong,\ntraining methodologies for LLMs, focusing on fine-tuning, reeconomical, andefficientmixture-of-expertslanguagemodel,\ninforcementlearning, andscaling. Weanalyzekeytechniques,\narXivpreprintarXiv:2405.04434,2024. 1,2,5,6\nalong with strategies for improving efficiency and alignment M. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan,\nwith human preferences. Additionally, we explore the role of N. Bach, A. Bahree, A. Bakhtiari, J. Bao, H. Behl, etal., Phi3technicalreport: Ahighlycapablelanguagemodellocallyon\nRLinenhancingLLMsthroughreasoning, planning, andmultiyourphone, arXivpreprintarXiv:2404.14219,2024. 1,5\ntask generalization, categorizing their functionalities within A. Fan, M. Lewis, andY. Dauphin, Hierarchicalneuralstory\nthe agent-environment paradigm. Recent advancements in generation, arXivpreprintarXiv:1805.04833,2018.\n C. Chhun, P. Colombo, C. Clavel, andF. M. Suchanek, Ofhureinforcementlearningandtest-timescalinghavesignificantly\nmancriteriaandautomaticmetrics: AbenchmarkoftheevalimprovedLLMsreasoningcapabilities, enablingthemtotackle\nuationofstorygeneration, arXivpreprintarXiv:2208.11646,\nincreasingly complex tasks. By consolidating the latest re- 2022.\nsearchandidentifyingopenchallenges, weaimtoguidefuture S. Arif, S. Farid, A. H. Azeemi, A. Athar, and A. A. Raza,\nThefellowshipofthellms: Multi-agentworkflowsforsynthetic\neffortsinoptimizingLLMsforreal-worldapplications.\npreference optimization dataset generation, arXiv preprint\narXiv:2408.08688,2024.\n S. Ye, Y. Jo, D. Kim, S. Kim, H. Hwang, andM. Seo, Selfee:\n            \n            CRITICAL: ONLY CONVERSATION AGENTS participate in this analysis:\n            - Base agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - Specialized domain agents\n            \n            EXCLUDED FROM ANALYSIS: Educational Writer, Voice Director, and Comedy Communicator (all work in post-production)\n            \n            Each participating agent should:\n            1. Read and understand the paper from your specific role's perspective\n            2. Identify key points relevant to your expertise\n            3. Prepare questions or concerns to discuss\n            4. Consider the implications from your unique viewpoint\n            \n            SPECIALIZED AGENTS: Pay special attention to domain-specific aspects that only you can address.\n            \n            This should be a comprehensive TECHNICAL analysis where EVERY conversation agent contributes their specialized perspective.\n            \n            Language: Spanish\n            ",
      "expected_output": "Comprehensive technical analysis from conversation agents only (no post-production agents)",
      "agent_role": "Coordinator"
    },
    {
      "description": "\n                    SPECIALIZED AGENTS DEEP DIVE: Domain expertise from TECHNICAL conversation agents only.\n                    \n                    PARTICIPATING SPECIALIZED AGENTS (technical focus):\n                    - AI Researcher: Provide technical insights on AI methodology and implications, - AI Philosopher: Discuss philosophical implications of AI research, - AI Doomer: Raise concerns about potential risks and negative consequences, - AI Enthusiast: Highlight positive potential and applications, - AI Newcomer: Ask basic questions that others can answer\n                    \n                    EXCLUDED: Comedy Communicator (works in post-production phase)\n                    \n                    Each specialized agent should:\n                    1. Provide deep domain-specific insights about the paper\n                    2. Identify methodological issues specific to your field\n                    3. Highlight implications that only someone with your expertise would notice\n                    4. Suggest domain-specific improvements or alternative approaches\n                    5. Connect this work to other research in your specialized area\n                    \n                    This is YOUR moment to shine with specialized knowledge that the base agents cannot provide.\n                    Focus on TECHNICAL DEPTH and DOMAIN EXPERTISE.\n                    Format as a detailed specialist consultation with clear attribution to each expert.\n                    \n                    Language: Spanish\n                    ",
      "expected_output": "Deep technical specialist analysis from 5 domain experts",
      "agent_role": "AI Researcher"
    },
    {
      "description": "\n            Based on the initial analysis, conduct a DYNAMIC Q&A session where technical conversation agents ask each other specific questions.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker) \n            - ALL specialized domain agents\n            \n            EXCLUDED FROM CONVERSATION: Educational Writer, Voice Director, and Comedy Communicator (all work in post-production)\n            \n            Instructions for multi-agent technical conversation:\n            1. ALL TECHNICAL CONVERSATION AGENTS should ask pointed questions to other agents\n            2. SPECIALIZED AGENTS should ask domain-specific questions that challenge assumptions\n            3. BASE AGENTS should ask specialists to clarify complex domain concepts\n            4. Agents must respond to questions directed at them with detailed technical answers\n            5. Follow-up questions and clarifications are encouraged\n            6. Challenge each other's assumptions respectfully\n            7. Build on each other's ideas and insights\n            8. Create a natural back-and-forth technical dialogue\n            \n            SPECIALIZED AGENTS: This is crucial - ask questions only YOU would think to ask!\n            \n            Focus areas for technical questions:\n            - Domain-specific methodological concerns\n            - Interdisciplinary connections and conflicts\n            - Alternative interpretations from different expert perspectives\n            - Practical applications in each specialist's field\n            - Potential limitations or biases from multiple viewpoints\n            \n            Format this as a realistic TECHNICAL conversation with clear speaker identification for ALL conversation participants.\n            Keep the tone SERIOUS and TECHNICAL - humor will be added later in post-production.\n            \n            Language: Spanish\n            ",
      "expected_output": "Dynamic technical Q&A conversation between conversation agents only (no post-production or humor)",
      "agent_role": "Critical Thinker"
    },
    {
      "description": "\n            Organize a structured technical debate where conversation agents with different viewpoints engage in deeper discussion.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - ALL specialized domain agents  \n            \n            EXCLUDED FROM DEBATE: Educational Writer, Voice Director, and Comedy Communicator (all work in post-production)\n            \n            Technical debate structure:\n            1. Present the main controversial points or interpretations from the paper\n            2. Have TECHNICAL CONVERSATION AGENTS take different positions and argue their cases\n            3. SPECIALIZED AGENTS: Argue from your domain expertise - what would your field say?\n            4. Allow for rebuttals and counter-arguments between different expert perspectives\n            5. Explore edge cases and hypothetical scenarios from multiple disciplinary angles\n            6. Find areas of agreement and persistent disagreements between different specialties\n            7. Synthesize different viewpoints into a richer technical understanding\n            \n            This should feel like a real interdisciplinary TECHNICAL conference where:\n            - Different specialists bring unique perspectives that sometimes conflict\n            - Domain experts interrupt each other (politely) to make field-specific points\n            - Ideas evolve through interaction between different areas of expertise\n            - New insights emerge from cross-disciplinary exchange\n            - There's intellectual tension between different specialist viewpoints\n            \n            SPECIALIZED AGENTS: Don't hold back - defend your field's perspective!\n            \n            Make it conversational and dynamic, but keep TECHNICAL FOCUS - humor will be added later.\n            \n            Language: Spanish\n            ",
      "expected_output": "Rich interdisciplinary technical debate between conversation agents only (no post-production or humor)",
      "agent_role": "Scientific Reviewer"
    },
    {
      "description": "\n            Conduct a collaborative synthesis where technical conversation agents work together to build a comprehensive understanding.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - ALL specialized domain agents\n            \n            EXCLUDED FROM SYNTHESIS: Educational Writer, Voice Director, and Comedy Communicator (all work in post-production)\n            \n            Technical collaborative process:\n            1. ALL TECHNICAL CONVERSATION AGENTS contribute their key insights from the discussions\n            2. SPECIALIZED AGENTS highlight unique perspectives only your field can provide\n            3. Agents build on each other's contributions in real-time\n            4. Identify connections between different specialist perspectives\n            5. Resolve conflicting interpretations through interdisciplinary dialogue\n            6. Co-create new insights that emerge from cross-domain discussion\n            7. Establish consensus on the most important takeaways from ALL conversation perspectives\n            \n            This should be a generative TECHNICAL conversation where:\n            - Ideas from one specialist spark new ideas in other specialists\n            - The group intelligence exceeds individual specialist perspectives\n            - Agents actively listen and respond to insights from other domains\n            - The conversation flows naturally between different areas of expertise\n            - New understanding emerges from interdisciplinary interaction\n            - Each specialist's unique knowledge contributes to the whole\n            \n            SPECIALIZED AGENTS: Share insights that ONLY someone with your expertise would have!\n            \n            Format as natural TECHNICAL conversation with organic transitions between specialist viewpoints.\n            Keep SERIOUS and FOCUSED - entertainment will be added later in post-production.\n            \n            Language: Spanish\n            ",
      "expected_output": "Collaborative technical synthesis conversation from conversation agents only (no post-production or humor)",
      "agent_role": "Coordinator"
    },
    {
      "description": "\n            Based on all previous conversations and analyses, conduct a final comprehensive technical discussion that synthesizes insights from conversation agents.\n            \n            PARTICIPATING AGENTS (technical conversation only):\n            - Base conversation agents (Coordinator, Scientific Reviewer, Critical Thinker)\n            - ALL specialized domain agents\n            \n            EXCLUDED: Educational Writer, Voice Director, and Comedy Communicator (they will process this output in post-production)\n            \n            The final technical discussion should:\n            1. Synthesize insights from the Q&A, specialist deep dive, debate, and collaborative sessions\n            2. Cover all major points of the paper from multiple expert perspectives\n            3. Include the rich specialist perspectives developed through agent interactions\n            4. Address concerns and criticisms that emerged from different domains\n            5. Explore implications and applications discussed by various specialists\n            6. Be comprehensive and technically rigorous for expert audiences\n            7. Highlight unique insights that could ONLY come from having multiple specialist perspectives\n            \n            CRITICAL: This final technical discussion must incorporate:\n            - Domain-specific insights from ALL specialist conversation agents\n            - Cross-disciplinary connections discovered during discussions\n            - Unique perspectives that emerged from interdisciplinary dialogue\n            - Technical depth and rigor appropriate for expert audiences\n            \n            This is the FINAL technical conversation output that will be handed to the post-production team.\n            Make it comprehensive, rigorous, and rich with all the insights gathered.\n            Keep it TECHNICAL and SERIOUS - post-production will handle accessibility and entertainment.\n            \n            Language: Spanish\n            ",
      "expected_output": "Final comprehensive technical discussion ready for post-production processing",
      "agent_role": "Critical Thinker"
    },
    {
      "description": "\n            POST-PRODUCTION PHASE 2: EDUCATIONAL SCRIPT CREATION\n            \n            Transform ALL the rich content into a comprehensive educational lecture text.\n            \n            You are receiving the complete output, which includes:\n            - Initial analysis from all conversation agents\n            - Specialized domain expert deep dive\n            - Dynamic Q&A sessions between experts\n            - Interdisciplinary technical debates\n            - Collaborative synthesis\n            - Final comprehensive technical discussion\n            \n            \n            Your job is to distill ALL this rich content into a single educator voice.\n            \n            The script should be in the style of popular science educators like 3Blue1Brown:\n            1. Written as a SINGLE EDUCATOR speaking directly to the listener (use \"tú\"/\"usted\")\n            2. Use analogies and accessible explanations\n            3. Include ALL key insights from the multiple conversations and specialist exchanges\n            4. Be engaging and educational, not just informative\n            5. Flow naturally from concept to concept with smooth transitions\n            6. Include moments of wonder and intellectual curiosity\n            7. Break down complex ideas into digestible parts\n            8. Use a teaching tone that makes the listener feel they're learning something fascinating\n            9. Write as continuous text ready to be read by a voice actor\n            10. NO section headers, NO subheaders, NO formatting marks\n            11. Don't address the public with greetings or goodbyes, but make questions\n            12. Always end up with questions for the reader and practical implications\n            13. Write as plain text that flows naturally for voice reading\n            14. NO [PAUSES], NO [MUSIC], NO stage directions - just the educational content\n            15. CRITICAL: Address the listener directly - \"puedes imaginar\", \"si consideras\", \"te darás cuenta\"\n            16. DO NOT write as if summarizing a discussion - write as if YOU are the teacher\n            17. Avoid phrases like \"los expertos discutieron\" or \"el equipo concluyó\"\n            18. Incorporate the depth and nuance that emerged from ALL agent conversations\n            \n            CRITICAL DIDACTIC TECHNIQUES - MANDATORY:\n            19. INTRODUCTION must include a compelling preview/roadmap: Start with an engaging hook and then preview what the listener will learn - \"En los próximos minutos vas a descubrir...\", \"Te voy a mostrar tres ideas que cambiarán tu forma de pensar sobre...\", etc.\n            20. CONCLUSION must include a clear summary: End with a recap of the main points covered - \"Hemos visto que...\", \"En resumen, tres puntos clave...\", \"Para cerrar, recordemos que...\", etc.\n            21. AVOID TYPICAL LLM WORDS: Never use overused AI-generated words like \"fundamental\", \"crucial\", \"clave\" (as adjective), \"esencial\", \"revelador\", \"fascinante\", \"delve into\", \"explore\", \"unpack\", \"dive deep\", \"robust\", \"compelling\", etc.\n            22. USE NATURAL LANGUAGE: Instead of LLM words, use conversational alternatives like \"importante\", \"interesante\", \"sorprendente\", \"nos ayuda a entender\", \"vamos a ver\", \"resulta que\", \"descubrimos que\", etc.\n            23. SOUND HUMAN: Write as if explaining to a friend over coffee, not as if generating academic content\n            \n            CRITICAL - MULTI-SPECIALIST INTEGRATION:\n            19. Weave in insights that could ONLY come from having multiple specialist perspectives\n            20. Include cross-disciplinary connections discovered during discussions\n            21. Incorporate domain-specific knowledge from ALL participating specialists\n            22. Show how different expert viewpoints enhance understanding of the topic\n            \n            23. Demonstrate the value of interdisciplinary analysis throughout\n            \n            \n            ACCESSIBLE LEVEL REQUIREMENTS:\n            15. Focus on core concepts and main findings rather than technical details\n            16. Use everyday analogies to explain complex ideas\n            17. Emphasize practical implications and real-world applications\n            18. Keep technical jargon to a minimum, always explaining when used\n            19. Focus on the \"why this matters\" rather than the \"how they did it\"\n            20. Make connections to things the audience already understands\n            \n            \n            \n        DURATION REQUIREMENT: EXACTLY 25 minutes of content (3500-4000 words) - THIS IS MANDATORY\n        \n        DEPTH GUIDANCE FOR 25 MINUTES:\n        \n            - Conduct a COMPREHENSIVE and IN-DEPTH analysis\n            - Cover all main aspects of the topic\n            - Include detailed context and extensive theoretical framework\n            - Explain methodology, limitations and alternative interpretations\n            - Provide multiple examples, analogies and real-world applications\n            - Include detailed discussion of implications and future directions\n            - Allow deep exploration of related concepts and broader significance\n            - Should feel like a comprehensive academic lecture, not a summary\n            \n        \n        TECHNICAL CALCULATION:\n        - Target reading speed: ~150 words per minute\n        - Word range: 3500-4000 words\n        - If content is too short, EXPAND significantly with more detail and depth\n        - If too long, maintain quality but adjust information density\n        \n            \n            \n            LANGUAGE REQUIREMENTS FOR SPANISH:\n            \n            CRITICAL: AVOID ANGLICISMS whenever possible and use proper Spanish terms:\n            - Instead of \"link\" use \"enlace\" or \"vínculo\"\n            - Instead of \"feedback\" use \"retroalimentación\" or \"respuesta\"\n            - Insted of \"puzzle\" use \"rompecabezas\" or \"problema\"\n            - Instead of \"performance\" use \"rendimiento\" or \"desempeño\"\n            - Instead of \"input/output\" use \"entrada/salida\"\n            - Instead of \"update\" use \"actualizar\" or \"poner al día\"\n            \n            EXCEPTIONS - You CAN use anglicisms for:\n            1. Very new technical terms with no established translation (e.g., \"blockchain\", \"ChatGPT\")\n            2. Proper names of tools/companies (e.g., \"TensorFlow\", \"GitHub\", \"OpenAI\")\n            3. Widely adopted terms in scientific literature (e.g., \"machine learning\" vs \"aprendizaje automático\")\n            4. When the Spanish term is more confusing than helpful\n            \n            GENERAL RULES:\n            - Always prioritize natural Spanish expressions\n            - Use Spanish sentence structures and idioms\n            - Make it sound like a native Spanish speaker wrote it\n            - When you must use an anglicism, briefly explain it if needed\n            \n            \n            Language: Spanish\n            ",
      "expected_output": "Comprehensive educational script incorporating ALL conversation insights",
      "agent_role": "Educational Writer"
    },
    {
      "description": "\n            POST-PRODUCTION PHASE 3: FINAL VOICE OPTIMIZATION\n            \n            Transform the Educational Writer's script into a PERFECT voice-ready script.\n            \n            You are receiving the educational script that has been carefully crafted from all conversation insights\n            .\n            Your job is PURELY technical optimization for voice delivery.\n            \n            CRITICAL: Verify the content meets the 25-minute target (3500-4000 words). If it's too short, EXPAND it significantly.\n            CRITICAL: Ensure technical level is accessible - keep accessible but thorough.\n            \n            MANDATORY VOICE OPTIMIZATION REQUIREMENTS:\n            1. Create a SINGLE, CONTINUOUS text ready for a voice actor to read\n            2. Markdown formatting, but NO headers, NO bullet points, NO lists\n            3. Convert ALL content into natural, flowing sentences\n            4. Replace any remaining bullet points with complete sentences\n            5. Ensure PERFECT flow from sentence to sentence\n            6. Remove formatting marks: #, -, •, etc for titles and subtitles, but keep for bold and italic text\n            7. Make sure sentences are not too long or complex for voice delivery\n            8. Write naturally in Spanish without academic formalities\n            9. Remove any remaining conversational artifacts (\"como mencionamos antes\", \"en nuestra discusión\")\n            10. Ensure seamless transitions between concepts\n            11. Maintain the conversational richness but in a single educator voice\n            12. Read the text mentally to ensure it sounds natural when spoken\n            13. Ensure proper pronunciation flow for difficult technical terms\n            14. Remove any repetitive content that may have emerged from multiple discussions\n            15. Maintain the depth gained from agent conversations while ensuring clarity\n            16. Perfect pacing for natural speech rhythm\n            17. Eliminate any phrases that sound like committee work or group consensus\n            18. Make it sound like ONE expert who has deeply understood the topic\n            19. Ensure technical accuracy while maintaining conversational flow\n            20. Optimize for voice actor performance and listener engagement\n            21. This should sound like ONE VOICE teaching, not a summary of multiple voices\n            22. Avoid words that could make this sound like written by an LLM, like not often used words: \"fascinante\", \"delve\", \"revelador\"\n            23. Introduction should be a catchy hook that makes the listener want to listen to the entire video, something like a question or a statement that makes the listener want to know more\n            24. DO NOT add new content - only optimize existing content for voice delivery\n            25. DO NOT change the educational message - only improve its delivery\n            \n            CRITICAL DIDACTIC STRUCTURE VERIFICATION:\n            26. VERIFY INTRODUCTION includes preview/roadmap: Ensure there's a clear \"what you'll learn\" section early in the script\n            27. VERIFY CONCLUSION includes summary: Ensure there's a clear recap of main points at the end\n            28. REMOVE LLM WORDS: Replace any remaining \"fundamental\", \"crucial\", \"clave\" (adjective), \"esencial\", \"revelador\", \"fascinante\", \"compelling\", \"robust\", etc. with natural alternatives\n            29. HUMAN CONVERSATION: Ensure the entire script sounds like a knowledgeable person explaining something interesting, not AI-generated content\n            30. NATURAL FLOW: Check that didactic elements (preview, summary) flow naturally within the content, not as forced additions\n            \n\n            \n            LANGUAGE REQUIREMENTS FOR SPANISH:\n            \n            CRITICAL: AVOID ANGLICISMS whenever possible and use proper Spanish terms:\n            - Instead of \"link\" use \"enlace\" or \"vínculo\"\n            - Instead of \"feedback\" use \"retroalimentación\" or \"respuesta\"\n            - Insted of \"puzzle\" use \"rompecabezas\" or \"problema\"\n            - Instead of \"performance\" use \"rendimiento\" or \"desempeño\"\n            - Instead of \"input/output\" use \"entrada/salida\"\n            - Instead of \"update\" use \"actualizar\" or \"poner al día\"\n            \n            EXCEPTIONS - You CAN use anglicisms for:\n            1. Very new technical terms with no established translation (e.g., \"blockchain\", \"ChatGPT\")\n            2. Proper names of tools/companies (e.g., \"TensorFlow\", \"GitHub\", \"OpenAI\")\n            3. Widely adopted terms in scientific literature (e.g., \"machine learning\" vs \"aprendizaje automático\")\n            4. When the Spanish term is more confusing than helpful\n            \n            GENERAL RULES:\n            - Always prioritize natural Spanish expressions\n            - Use Spanish sentence structures and idioms\n            - Make it sound like a native Spanish speaker wrote it\n            - When you must use an anglicism, briefly explain it if needed\n            \n            \n            CRITICAL: This is the FINAL version that will be published. Make it PERFECT for voice delivery.\n            \n            Language: Spanish\n            ",
      "expected_output": "FINAL publication-ready voice script optimized for delivery (3500-4000 words)",
      "agent_role": "Voice Director"
    }
  ]
}