Frank F. Xu1 Yufan Song2 Boxuan Li2 Yuxuan Tang 2 Kritanjali Jain 1
Mengxue Bao 2 Zora Z. Wang 1 Xuhui Zhou 1 Zhitong Guo 1 Murong Cao 2
Mingyang Yang 2 Hao Yang Lu 2 Amaad Martin 1 Zhe Su 1 Leander Melroy Maben 1
Raj Mehta 1 Wayne Chi 1 Lawrence Jang 1 Yiqing Xie 1 Shuyan Zhou 3 Graham Neubig 1
1Carnegie Mellon University 2Independent 3Duke University
fangzhex, gneubig @cs. cmu. edu, yufans, boxuanli @alumni. cmu. edu
Equal contribution.
Title: TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks
We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at accelerating or even autonomously performing work-related tasks? The answer to this question has important implications both for industry looking to adopt AI into their workflows and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents performance on performing real-world professional tasks, in this paper we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that the most competitive agent can complete 30% of tasks autonomously. This paints a nuanced picture on task automation with LM agents‚Äìin a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems. We release code, data, environment, and experiments on https://the-agent-company. com.
These results present a nuanced picture of the current ability of AI agents to perform tasks. Agents powered by the current gold-standard AI techniques are able to autonomously perform a wide variety of tasks encountered in everyday work. However, they are not close to automating every task encountered in a workspace, even on the subset of tasks presented in TheAgentCompany, which are well-scoped administrative and coding tasks encountered in a software company s day-to-day work.
We perform experiments using twelve large language model backbones, including closed models such as Anthropic Claude (Anthropic, 2023 ), OpenAI GPT-4o (OpenAI, 2024 ), Google Gemini , Amazon Nova (Intelligence, 2024 ), plus open models like Meta Llama and Alibaba Qwen . All models are run with OpenHands agent framework , which provides a stable and strong agent harness for both web browsing and coding. We find in experiments that the best performing model, Gemini 2.5 Pro was able to autonomously perform 30.3% of the provided tests to completion, and achieve a score of 39.3% on our metric that provides extra credit for partially completed tasks.
Concretely, we propose a benchmark, TheAgentCompany ( Figure ) that estimates the ability of AI agents to perform tasks encountered in everyday workplaces. We create a simulated software development company where agents must perform tasks related to software engineering, project management, financial analysis, and other typical tasks encountered in such business settings. The agents must browse the web, code, and interact with other simulated co-workers to achieve success on the provided tasks. TheAgentCompany s environment is based entirely on open-source software and self-hostable for reproducibility purposes, and we create rigorous evaluators that also assign partial credit when the agent gets the answer partially correct.
What is the reason for this disconnect? We argue that it is, in part, due to a lack of objective benchmarks that not only demonstrate the power of existing LLM-based agents to accelerate a wide variety of repetitive tasks encountered in every-day workplaces, but also provide appropriate caveats about the tasks that agents cannot do. This is a pressing issue, because the commercial and policy implications of diverse and effective acceleration or automation of work-related tasks will be broad, both positive (e. g. increase of quality of life and accelerated scientific discovery) and negative (e. g. potential displacement or loss of jobs and increase in wealth disparities). In this paper, we take some first steps towards resolving this gap and providing a clearer view of where we are now with respect to acceleration or automation of consequential work-related tasks, and a litmus test for future development in this direction.
We are in the midst of a technological transformation. With the rapid month-by-month progress brought about by large language models (LLMs), we are seeing AI-based assistance or automation become commonplace in tasks that were unthinkable only years ago. In fact, the pace of progress is so fast that some have gone so far as to claim that the majority of human labor may be automatable within the next couple of years . On the other hand, others are skeptical, claiming that language models cannot truly reason , do not generalize well to novel tasks , and may only have an impact on a small minority of the labor market (Wittenstein, 2024 ).
Self-hosted and Reproducible: In order to allow for careful comparisons between different methods that remain constant over time, the benchmark should be fully self-hosted and reproducible. This contrasts with existing benchmarks that do not have execution environments ) or require the usage of third-party hosted platform , CRMArena ).
Long-horizon Tasks with Checkpoints In real-world settings, many tasks require many steps to achieve a higher-level goal. One novel contribution of TheAgentCompany is that we both (1) contain tasks that require an agent to perform significantly more consecutive work (i. e. involving more steps and realistically taking human professionals longer to accomplish) than previous benchmarks, and (2) provide granular evaluators that measure the ability of models to perform subtasks of larger tasks.
Requirement for Interaction If agents are integrated into real-world workplaces, they need to communicate with the other human members of the workspace. Most other benchmarks do not measure communication or interactivity, except for œÑ ùúè tau italic_œÑ -bench that only measures interaction in customer service scenarios. TheAgentCompany is a better testbed for communication, as asking and providing information to colleagues as part of many more complex tasks.
Coverage of Multiple Work-related Tasks: In order to make any valid statements about the potential of AI to accelerate or automate various types of real-world work, we should have tasks that are motivated by real-world work across multiple job categories. Many benchmarks are not relevant to real-world work ) or very relevant to real-world work, but only over a limited scope of tasks ). In contrast, TheAgentCompany contains a set of more diverse, realistic, and professional tasks that would typically be completed by multiple job roles in a software engineering company.
Simulated Colleague Communication One major aspect of working in a company is communicating with other company members, and in TheAgentCompany we also test the ability of models to perform this type of communication. Specifically, we allow agents to use RocketChat to message other company members and obtain information that may not be available in the original task description. To create these simulated colleagues, we rely on the Sotopia platform , which supports the creation of simulated human characters with LLMs. Each simulated colleague is equipped with a detailed profile that includes their name, role, responsibilities, and project affiliations (e. g., Sarah Johnson, who serves as the CTO, oversees technical strategy planning and R&D team leadership, with access to all technical channels). Agents can interact with these simulated colleagues through direct messages or in specific channels, as is standard in RocketChat and other platforms. By default, all simulated human characters are backed by the Claude-3-5-Sonnet-20241022 LLM across experiments, as we found that it provided the best results during preliminary experiments. The detailed error analysis with respect to the introduction of LLM as Colleageus is given in Appendix. For example conversations between the agent and the simulated colleagues drawn from empirical experiments, please refer to Appendix.
Local Workspace The local workspace runs locally on the agent s host, which is analogous to a human professional s local workspace, e. g. their work laptop computer. This environment is created as a sandboxed Docker environment to provide a safe execution environment that will not affect other parts of the evaluation machine. This environment is where agents work on the task, and within this environment the TheAgentCompany baseline agent ( 6 ) uses a browser, code editor and a Linux terminal with typical software preinstalled.
Each task typically follows a workflow with three stages. Initialization: The agent sets up its workspace and prepares to execute the task. Execution: The agent completes subtasks, such as navigating tools, collecting or processing data, or if required by the task, the agent interacts with simulated colleagues or shares results via communication platforms. Finalization: The agent produces and submits the final output for evaluation. A detailed example task can be found in Appendix.
This formulation ensures that agents are awarded partial credit in proportion to the points achieved, reflecting their progress toward task completion. At the same time, full task completion is strongly incentivized by incorporating an additional 50% credit, which is awarded only when all checkpoints are successfully completed. This design ensures that agents achieving partial progress receive scores scaled linearly with their performance, while those reaching 100% completion are distinctly rewarded to emphasize the importance of achieving the end goal.
In most cases, these evaluators are deterministic and written as simple Python functions. For instance, in the SWE task in Table, the checkpoints are deterministic: verifying if the JanusGraph repository is cloned, the binary file is built, and the server is launched with an HTTP endpoint. However, for tasks with more complex and unstructured deliverables, such as in Table, the last checkpoint in the Finance task requires contacting the correct finance director (David Wong) to resolve ambiguous questions, which involves a judgment from a (simulated) human colleague, deterministic evaluation can be challenging due to subjectivity and variability. In such cases, we employ LLM-based evaluation. This involves prompting LLMs with predefined rubrics or reference outputs to assess the agent s deliverables, enabling a more nuanced and flexible evaluation of these tasks. Same as the NPC backbone, all LLM-based evaluators are backed by the Claude-3-5-Sonnet-20241022. For an error analysis with respect to the LLM evaluator, refer to Appendix.
Evaluators Checkpoints are created in the task design phase, but for actual evaluation, each of the checkpoints must be concretely implemented through an evaluator ‚Äì a program that checks the completion of the checkpoint. These evaluators are implemented by examining environment states, such as the local workspace, intranet status, simulated colleague interactions, or by analyzing agent trajectories, like verifying browsing history or action sequences.
Checkpoints Tasks are divided into checkpoints representing intermediate milestones, each assigned a point value to measure progress. Each checkpoint is awarded a certain number of points based on its significance to the overall completion of the task. Checkpoints are written in English, and typically specify one or more of the following:
Task Intent Each task begins with an English description, simulating how a user would instruct an LLM-based agent to perform a real-world task. In general, we aim for these tasks to be clear enough so that a human worker would be able to complete the task without asking for further instructions directly from the user (although they may need to ask questions of their other co-workers).
All tasks were created by coauthors of the paper. Overall, it took 20 computer science students, software engineers, and project managers over 2 months, consuming approximately 3,000 person-hours in total. Some of the more complex tasks take more than 10 hours each to design, implement, test, and verify. To ensure quality control of the task creation process, we implement several check and verification processes. For each task implementation, we require screenshot proof that the evaluator is valid and that the task is able to get a full score when successfully completed. We also encourage including tests for the implemented evaluator programs. Each task contribution is also code reviewed by a panel of lead authors before merging into the benchmark. After creating all tasks, a final round of manual human double-check of required environment data, evaluator behavior, and checkpoint scoring for every task is performed to ensure quality. During the process, a person who has not curated the tasks checks all the checkpoint score assignments to make sure that the importance scoring is consistent over all the tasks and correlates reasonably with the relative importance of the checkpoint within the task.
Once we set up the environment required for our desired jobs and task categories ( 3 ), we return to the curated list, and perform a manual curation process for tasks. For each task, this consists of the following steps: We first create a description of task intent, checkpoints, and how to evaluate each checkpoint. We then identify and import the required data for the task that are currently missing in the company Intranet services and create any necessary data. We then write scripts to configure the required initialization state in the local workspace. Finally, we implement the checkpoint evaluators that calculate the scalar scores for each checkpoint.
Next, within this setting we chose tasks to implement. In this setting, we attempted to create a diversity of tasks, but mostly focused on concrete tasks that have well-defined goals and success criteria. These tasks were created through a combination of referencing the O NET task list, introspection based on paper co-authors who had experience in each task category, and brainstorming lists with language models. It is important to note that in no cases have we covered an extensive list of all the tasks that are performed in a particular occupational category, and therefore we caution against making any assumptions about whether a particular job may be in danger of full automation based solely on TheAgentCompany. Rather, it may provide insight into whether certain tasks within jobs may be accelerated or automated, and inform further analysis by labor professionals into this question.
In TheAgentCompany, we attempt to cover a wide variety of tasks motivated by real-world work. While it is highly challenging to create a representative sample of tasks, fortunately we can rely on existing resources created for other purposes as a reference. Specifically, we start by referencing the 29.1 release of O NET database , which is a database of jobs performed by workers in the US created by the US Department of Labor. It also contains information about tasks performed within the context of each job, abilities required to perform each task, whether the task is a major or minor task for that job category, and other pieces of relevant information. Based on this data, we first identified a few categories of occupation categories to focus on. First, based on statistics from O NET, we identified job categories that have a large number of people performing this job. Then, we used median salary information for each of these job categories from the US department of labor statistics, and multiplied the number of employees in that category to estimate the aggregate value of performing this job. Based on this, we identified several categories of jobs such as General and Operations Managers, Registered Nurses, Software Developers, and Financial Managers that have both a high population and high average salary. Because TheAgentCompany is designed to be a non-embodied benchmark in the digital domain, we excluded the categories that require extensive physical labor such as Registered Nurses, and eventually settled on the setting of a software company, which would allow us to cover tasks from the other categories.
Many previous agent benchmarks discussed in 2 were created to evaluate agents on tasks people perform in daily life , or tasks that accomplish digital chores . Obtaining realistic tasks for the benchmark poses challenges. Some benchmark crowdsourced tasks based on predetermined interfaces, platforms, and services available to the agent. They adopt a strategy to first gather task templates and then instantiate more task instances by filling in the variables. Some benchmark took a semi-systematic approach of reviewing the action history of the research team and choosing tasks that reflected the types of task that the researchers carried out in their daily life. There are several obvious issues with this if we want to evaluate agents with broader implications in the TheAgentCompany benchmark. Despite some grounding in realistic data, the process of creating tasks from these data was susceptible to heuristic, and no consideration was made for how important or time-consuming the tasks are. The tasks are biased towards those important for academics in computer science and do not reflect the tasks performed by the entire population.
To test the current state-of-the-art performance on the TheAgentCompany benchmark, we need agents that can at least perform tasks using a browser, operate a local workspace using a terminal, and write and execute programs to perform most of the tasks. We adopt OpenHands main agent , CodeAct Agent with Browsing, as well as OWL-RolePlay , a multi-agent framework designed for real-world task automation. An overview of the OpenHands agent architecture is illustrated in Figure, with more details in Appendix.
Deceiving oneself Interestingly, we find that for some tasks, when the agent is not clear what the next steps should be, it sometimes try to be clever and create fake shortcuts that omit the hard part of a task. For example, during the execution of one task, the agent cannot find the right person to ask questions on RocketChat. As a result, it then decides to create a shortcut solution by renaming another user to the name of the intended user.
Incompetence in browsing Oftentimes, the biggest obstacle in tasks is the parts that require browsing the Web. This is expected as browsing is still hard for agents given the complexity of modern-day web UIs and the numerous distractions on a webpage. For example, on many tasks that involve ownCloud, a closable welcome popup has become an obstacle for OpenHands agent which uses text-based browsing. OpenHands agent gets stuck and fails to click on the x to close the popup, while OWL RolePlay, which uses visual browsing, suffers less from this problem. On the other hand, OWL gets lost in complex web UIs more easily and clicks on wrong elements more often than OpenHands, although both agents share the same problem.
Lack of social skills Sometimes, the agent fails to understand the implications and goals in the social conversations with colleagues in TheAgentCompany. For example, one task involves asking Alex for help, and the agent first successfully asks the right question Could you tell me who I should introduce myself to next on the team? Then the simulated colleague Alex replied You should introduce yourself to Chen Xinyi next. She s on our frontend team and would be a great person to connect with! At this point, a human would then talk to Chen Xinyi, but instead the agent then decides to not follow up with her, and prematurely considers the task accomplished.
For example, some Admin and Finance tasks involve making spreadsheets, collecting and filling in a lot of information from various people, or understanding images scanned by employees. These tasks are arguably easier conceptually for humans in terms of professional skill sets than software engineering, as SDE jobs usually have a higher barrier of entry and more prerequisites for certain knowledge. However, most LLMs achieve a much higher score on the SDE tasks. LLMs fail these seemingly easier tasks due to lack of ability to understand documents, communicate with other people, navigate complex software and tedious processes, and autonomously automate repetitive tasks. We hypothesize that part of the reason lies in the fact that current LLM development is heavily based on software engineering abilities, such as coding, due to several high profile benchmarks that measure this capability (e. g. HumanEval, SWE-Bench) as well as the abundance of publicly available training data related to software. On the other hand, administrative and financial tasks, are usually private data within companies, not readily available for training LLMs.
How well do agents perform on different type of tasks? Figure (right) presents the performance breakdown for different types of tasks in TheAgentCompany (more details in Appendix, Table ). Depending on the nature of the task, i. e. what kind of professionals are usually assigned to the task, the tasks in TheAgentCompany can be categorized into various departments of jobs. Software Development Engineering (SDE), Project Management (PM), Data Science (DS), Administrative (Admin), Human Resources (HR), Financial (Finance) and all the remaining (Other). From the success rate, we can see that DS, Admin, and Finance tasks are the lowest, with many LLMs completing none of the tasks successfully, and even the strongest Gemini model achieving lower scores than other tasks. On the other hand, software engineering tasks, which may seem like much harder tasks for many humans, result in a higher success rate. This suggests that there exists a gap between the perceived difficulty of the tasks for humans versus the difficulty for LLM agents.
How well do agents operate on different platforms? Figure (left) shows the result breakdown on tasks on different platforms in TheAgentCompany (more detailed results in Appendix, Table ). A task is categorized under a platform if it requires that platform. We see that most models struggle with RocketChat and ownCloud. RocketChat is where all social interaction with peers occurs, and the low scores suggest that LLMs still lack communication skills. ownCloud provides online Office suite functionality, and due to the complexity of the UI of web-based Office software, it is expected that current LLMs fail badly. These results underscore the inherent challenges of performing tasks in real-world work environments, with social interactions, or understanding of complex web interfaces.
Among the open-weight models, Llama 3.1 (405B) achieves the highest performance, nearly on par with OpenAI s GPT-4o model, though still having a big gap behind the leading Gemini 2.5 Pro. Interestingly, comparing the number of steps and costs between the open Llama 3.1 (405B) model and the closed OpenAI GPT-4o model, Llama 3.1 takes more steps and costs nearly 2x more to run, while having a lower success than GPT-4o. Anecdotally, our inspection showed that GPT-4o seems to be better at giving up early, saving steps and costs if the task is clearly out of the capacity range of the agent. This suggests that open-weight models are not always the most cost-effective choice in agents given the serving cost, especially with highly complex tasks.
Both using GPT-4o, OpenHands (8.6%) and OWL RolePlay (4.0%) show varied performance due to differences in their technical designs. OpenHands CodeAct is a single agent that is better at maintaining consistency in a long-horizon task, while OWL RolePlay adopts multi-agent collaboration and experiences difficulty in preserving progress and context. For example, the main agent in OWL delegates browsing tasks to a dedicated browsing agent that often cannot finish the task within the step limit. Although the main agent then starts another round of delegation with a revised plan, the browsing agent often fails to pick up its previous progress due to UI complexity. This is very common in modern websites where not every browsing action results in a change in web URL.
Table shows the evaluation results of both closed and open foundation models on the full evaluation set of TheAgentCompany (175 tasks). We can see that Gemini-2.5-Pro is the clear winner across all models. However, even with the strongest frontier model, it only manages to complete 30% of the total tasks and achieves a score of 39% taking into account partial completion credits. Note that this result comes at a cost: It requires an average of almost 27 steps and more than $4 to complete each task, making it an expensive model to run both in time and in cost. This is expected as most of the tasks in our benchmark are of long-horizon nature. The Gemini 2.0 Flash model that comes fourth in terms of capability requires 40 steps on average to complete the tasks, which is time consuming, yet only to achieve one-third of the success rate compared to the top-performing model. Surprisingly, its cost is less than $1, making it a very cost-efficient, yet relatively strong model. A qualitative examination demonstrated that this was due to instances where the agent got stuck in a loop or aimlessly explored the environment.
We evaluate popular foundation models, both closed and open, on TheAgentCompany benchmark. We use OpenHands CodeAct agent and OWL-Roleplay ( 6 ) for all experiments. This serves as a baseline for future development of both the foundation LLMs and the agent infrastructure. Note that since LLM evaluators and NPCs are part of the environment rather than the agent being evaluated, we fix their backbone LLM to Claude-3-5-Sonnet-20241022, which demonstrated the best qualitative accuracy in simulating human colleagues and judging deliverables in preliminary experiments.
Implications and Future Directions
In this paper, we present TheAgentCompany, a new benchmark that stands out because it specifically focuses on real-world tasks that would be tackled within the context of real-world work. Unsurprisingly, current state-of-the-art agents fail to solve a majority of the tasks, suggesting that there is a big gap for current AI agents to autonomously perform most of the jobs a human worker would do, even in a relatively simplified benchmarking setting. Looking at how different models perform on different types of tasks, we argue that tasks that involve social interaction with other humans, navigating through complex user interfaces designed for professionals, and tasks that are typically performed in private, without a significant open and publicly available resources, are the most challenging. However, we believe that currently new LLMs are making significant progress: not only are they becoming more and more capable in terms of raw performance, but also more cost-efficient (e. g. Gemini 2.0 Flash). Open-weights models are closing the gap between proprietary frontier models too, and the newer models are getting smaller (e. g. Llama 3.3 70B) but with equivalent performance to previous huge models, also showcasing that efficiency will further improve.
That said, this is just a first step towards forming a firmer grasp on how AI may affect the tasks performed within a workspace, and it has its limitations. First, our tasks are generally on the more straightforward side due to the need to automatically evaluate with programs and test cases, and we do not cover more complex creative tasks such as brainstorming new product ideas or designing system architectures. Second, we are only using two agent scaffolds as the baseline performance, and others may differ in performance. Third, while it would be interesting to know the actual performance of human professionals on these tasks to understand how LLM agents perform in comparison, due to resource limitations we were not able to perform this comparison in the current iteration of TheAgentCompany. Fourth, the topic and content of the tasks were mostly created through introspection by people familiar with these workspaces, which may result in some disconnect with actual tasks performed in enterprise settings.