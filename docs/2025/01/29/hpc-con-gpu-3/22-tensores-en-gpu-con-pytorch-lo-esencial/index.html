<!doctype html>
<html
  lang="es"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="dark"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  
    <meta http-equiv="content-language" content="es">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <meta name="theme-color">

  
  
    <title>HPC con GPU (3/22): Tensores en GPU con PyTorch — lo esencial &middot; The Dojo MX Blog</title>
    <meta name="title" content="HPC con GPU (3/22): Tensores en GPU con PyTorch — lo esencial &middot; The Dojo MX Blog">
  

  
  
    <meta name="description" content="Aprende a mover datos entre CPU y GPU, entiende dtypes y pinned memory, y mide cuándo la transferencia domina sobre el cómputo.">
  
  
    <meta name="keywords" content="-,PyTorch,">
  
  
  
  <link rel="canonical" href="http://localhost:1313/2025/01/29/hpc-con-gpu-3/22-tensores-en-gpu-con-pytorch-lo-esencial/">
  

  
  
  

  
  <meta property="og:url" content="http://localhost:1313/2025/01/29/hpc-con-gpu-3/22-tensores-en-gpu-con-pytorch-lo-esencial/">
  <meta property="og:site_name" content="The Dojo MX Blog">
  <meta property="og:title" content="HPC con GPU (3/22): Tensores en GPU con PyTorch — lo esencial">
  <meta property="og:description" content="Aprende a mover datos entre CPU y GPU, entiende dtypes y pinned memory, y mide cuándo la transferencia domina sobre el cómputo.">
  <meta property="og:locale" content="es">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-01-29T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-01-29T00:00:00+00:00">
    <meta property="article:tag" content="-">
    <meta property="article:tag" content="PyTorch">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="HPC con GPU (3/22): Tensores en GPU con PyTorch — lo esencial">
  <meta name="twitter:description" content="Aprende a mover datos entre CPU y GPU, entiende dtypes y pinned memory, y mide cuándo la transferencia domina sobre el cómputo.">

  
  
  
  
    
      
    
  
    
      
    
  
    
      
    
  
  
    
  

  
  
  
  
  
  

  

  
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="/css/main.bundle.min.f3f0a0f5f0a98349075e89ac6efd50d4005df562f33f20a0130bd2ad7167d99c0c20140704a8bb9618407ec5a01059c4c6ae400dd65f404a692fa4ba95f82eaa.css"
    integrity="sha512-8/Cg9fCpg0kHXomsbv1Q1ABd9WLzPyCgEwvSrXFn2ZwMIBQHBKi7lhhAfsWgEFnExq5ADdZfQEppL6S6lfguqg==">

  
  
  <script
    type="text/javascript"
    src="/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js"
    integrity="sha512-b0EXSzoFtoCCD&#43;CMrb&#43;l&#43;3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script>
  
  
  
  
  
  
    
    <script src="/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js" integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7&#43;kfJ6kKCJxQGC&#43;8wm&#43;Bz9JucDjDTGNew=="></script>
  

  
  
  
    
  
  
    
  
  
    
  
  
  
  
  
  
    
    <script
      defer
      type="text/javascript"
      id="script-bundle"
      src="/js/main.bundle.min.9cc802d09f28c6af56ceee7bc6e320a39251fdae98243f2a9942f221ac57a9f49c51609699a91794a7b2580ee1deaa8e4d794a68ffa94aa317c66e893ce51e02.js"
      integrity="sha512-nMgC0J8oxq9Wzu57xuMgo5JR/a6YJD8qmULyIaxXqfScUWCWmakXlKeyWA7h3qqOTXlKaP&#43;pSqMXxm6JPOUeAg=="
      data-copy="Copiar"
      data-copied="Copiado"></script>
  

  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>


























  

  

  

  

  





  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
  

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Posts",
    "name": "HPC con GPU (3\/22): Tensores en GPU con PyTorch — lo esencial",
    "headline": "HPC con GPU (3\/22): Tensores en GPU con PyTorch — lo esencial",
    "description": "Aprende a mover datos entre CPU y GPU, entiende dtypes y pinned memory, y mide cuándo la transferencia domina sobre el cómputo.",
    "abstract": "\u003ch2 class=\u0022relative group\u0022\u003eObjetivo\n    \u003cdiv id=\u0022objetivo\u0022 class=\u0022anchor\u0022\u003e\u003c\/div\u003e\n    \n    \u003cspan\n        class=\u0022absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none\u0022\u003e\n        \u003ca class=\u0022text-primary-300 dark:text-neutral-700 !no-underline\u0022 href=\u0022#objetivo\u0022 aria-label=\u0022Ancla\u0022\u003e#\u003c\/a\u003e\n    \u003c\/span\u003e\n    \n\u003c\/h2\u003e\n\u003cp\u003eDominar el \u003cstrong\u003emovimiento de tensores entre CPU y GPU\u003c\/strong\u003e en PyTorch, entender el impacto de dtypes (FP32, FP16, INT8), usar pinned memory para transferencias rápidas, y medir cuándo el overhead de transferencia anula las ganancias de cómputo en GPU.\u003c\/p\u003e",
    "inLanguage": "es",
    "url" : "http:\/\/localhost:1313\/2025\/01\/29\/hpc-con-gpu-3\/22-tensores-en-gpu-con-pytorch-lo-esencial\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    "copyrightYear": "2025",
    "dateCreated": "2025-01-29T00:00:00\u002b00:00",
    "datePublished": "2025-01-29T00:00:00\u002b00:00",
    
    "dateModified": "2025-01-29T00:00:00\u002b00:00",
    
    "keywords": ["-","PyTorch"],
    
    "mainEntityOfPage": "true",
    "wordCount": "1603"
  }]
  </script>



  
  

  
  

  
  

  
  

  
  
</head>


















  
  
  <body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content">
        <span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
        Ir al contenido
      </a>
    </div>
    
    
      <div class="min-h-[148px]"></div>
<div class="fixed inset-x-0 z-100">
  <div
    id="menu-blur"
    class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div>
  <div class="relative m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32">
    













<div
  class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0">
  
  
    
    

  <div class="flex flex-1 items-center justify-between">
    <nav class="flex space-x-3">
      
        <a href="/" class="text-base font-medium">
          The Dojo MX Blog
        </a>
      
    </nav>
    
  <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">
    
      
        
  <a
  href="/posts/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="Posts"
  title="Posts">
  
  
    <p class="text-base font-medium">
      Posts
    </p>
  
</a>



      
        
  <a
  href="/tags/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="Tags"
  title="Tags">
  
  
    <p class="text-base font-medium">
      Tags
    </p>
  
</a>



      
        
  <a
  href="/about/"
  
  class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
  aria-label="Acerca de"
  title="Acerca de este Blog">
  
  
    <p class="text-base font-medium">
      Acerca de
    </p>
  
</a>



      
    

    

    

    
      <button
        id="search-button"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Buscar (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <div class=" flex items-center">
        <button
          id="appearance-switcher"
          aria-label="Dark mode switcher"
          type="button"
          class="text-base hover:text-primary-600 dark:hover:text-primary-400">
          <div class="flex items-center justify-center dark:hidden">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
          </div>
          <div class="items-center justify-center hidden dark:flex">
            <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
          </div>
        </button>
      </div>
    
  </nav>

    
  <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">
    <span></span>

    

    

    
      <button
        id="search-button-mobile"
        aria-label="Search"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400"
        title="Buscar (/)">
        <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
      </button>
    

    
      <button
        id="appearance-switcher-mobile"
        aria-label="Dark mode switcher"
        type="button"
        class="text-base hover:text-primary-600 dark:hover:text-primary-400 me-1">
        <div class="flex items-center justify-center dark:hidden">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>
</span>
        </div>
        <div class="items-center justify-center hidden dark:flex">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>
</span>
        </div>
      </button>
    
  </div>

  </div>
  
  <div class="-my-2 md:hidden">
    <div id="menu-button" class="block">
      
        <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
          <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>
</span>
        </div>
        <div
          id="menu-wrapper"
          class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]">
          <ul
            class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl">
            <li id="menu-close-button">
              <span
                class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">
                <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
              </span>
            </li>

            
              
  <li class="mt-1">
  <a
    href="/posts/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="Posts"
    title="Posts">
    
    
      <p class="text-bg font-bg">
        Posts
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/tags/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="Tags"
    title="Tags">
    
    
      <p class="text-bg font-bg">
        Tags
      </p>
    
  </a>
</li>



            
              
  <li class="mt-1">
  <a
    href="/about/"
    
    class="flex items-center hover:text-primary-600 dark:hover:text-primary-400"
    aria-label="Acerca de"
    title="Acerca de este Blog">
    
    
      <p class="text-bg font-bg">
        Acerca de
      </p>
    
  </a>
</li>



            

          </ul>
          
        </div>
      
    </div>
  </div>

</div>




  <script>
    (function () {
      var $mainmenu = $(".main-menu");
      var path = window.location.pathname;
      $mainmenu.find('a[href="' + path + '"]').each(function (i, e) {
        $(e).children("p").addClass("active");
      });
    })();
  </script>


  </div>
</div>


<script
  type="text/javascript"
  src="/js/background-blur.min.00a57c73ea12f2cab2980c3c3d649e89f6d82f190f74bbe2b67f2f5e39ab7d032ece47086400ca05396758aace13299da49aca43ea643d2625e62c506267a169.js"
  integrity="sha512-AKV8c&#43;oS8sqymAw8PWSeifbYLxkPdLvitn8vXjmrfQMuzkcIZADKBTlnWKrOEymdpJrKQ&#43;pkPSYl5ixQYmehaQ=="
  data-blur-id="menu-blur"></script>

    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  
  <article>
    
    
      
      
      
      
        







  
  
    
  
    
  
    
  
    
  



  
  




  





      
    

    
    <header id="single_header" class="mt-5 max-w-prose">
      
        <ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden">
  
  
    
  
    
  
  <li class="hidden">
    <a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href="/"
      >The Dojo MX Blog</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="inline">
    <a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href="/posts/"
      >Posts</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="hidden">
    <a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href="/2025/01/29/hpc-con-gpu-3/22-tensores-en-gpu-con-pytorch-lo-esencial/"
      >HPC con GPU (3/22): Tensores en GPU con PyTorch — lo esencial</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

</ol>


      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        HPC con GPU (3/22): Tensores en GPU con PyTorch — lo esencial
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2025-01-29T00:00:00&#43;00:00">29 enero 2025</time><span class="px-2 text-primary-500">&middot;</span><span>1603 palabras</span><span class="px-2 text-primary-500">&middot;</span><span title="Tiempo de lectura">8 mins</span>
    

    
    
      <span class="ps-2"><span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Borrador
  </span>
</span>
</span>
    
  </div>

  

  
  
    <div class="flex flex-row flex-wrap items-center">
      
        
      
        
          
        
      
        
          
            
              <a class="relative mt-[0.5rem] me-2" href="/tags/-/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    -
  </span>
</span>

              </a>
            
              <a class="relative mt-[0.5rem] me-2" href="/tags/pytorch/">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    PyTorch
  </span>
</span>

              </a>
            
          
        
      
    </div>
  

  
  



      </div>
      
        
  
  
  
  
  
  

  

  
    
    
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

  

  

  
    <div class="mb-5"></div>
  

      
    </header>

    
    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
      
      
      
      
        <div class="order-first lg:ml-auto px-0 lg:order-last lg:ps-8 lg:max-w-2xs">
          <div class="toc ps-5 print:hidden lg:sticky lg:top-[140px]">
            
              <details
  open
  id="TOCView"
  class="toc-right mt-0 overflow-y-auto overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg -ms-5 ps-5 pe-2 hidden lg:block">
  <summary
    class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 -ms-5 ps-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Tabla de contenido
  </summary>
  <div
    class="min-w-[220px] py-2 border-dotted border-s-1 -ms-5 ps-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#objetivo">Objetivo</a></li>
    <li><a href="#anatomía-de-un-tensor-en-pytorch">Anatomía de un tensor en PyTorch</a>
      <ul>
        <li><a href="#cpu-vs-gpu-dónde-viven-los-datos">CPU vs GPU: ¿dónde viven los datos?</a></li>
      </ul>
    </li>
    <li><a href="#dtypes-precisión-vs-memoria-vs-velocidad">dtypes: precisión vs memoria vs velocidad</a>
      <ul>
        <li><a href="#tipos-comunes">Tipos comunes</a></li>
        <li><a href="#crear-tensores-con-dtype-específico">Crear tensores con dtype específico</a></li>
        <li><a href="#impacto-en-memoria">Impacto en memoria</a></li>
      </ul>
    </li>
    <li><a href="#transferencia-cpu--gpu-midiendo-overhead">Transferencia CPU ↔ GPU: midiendo overhead</a>
      <ul>
        <li><a href="#benchmark-básico">Benchmark básico</a></li>
      </ul>
    </li>
    <li><a href="#pinned-memory-acelerando-transferencias">Pinned memory: acelerando transferencias</a>
      <ul>
        <li><a href="#qué-es-pinned-memory">¿Qué es pinned memory?</a></li>
        <li><a href="#uso-en-pytorch">Uso en PyTorch</a></li>
      </ul>
    </li>
    <li><a href="#transferencia-vs-cómputo-cuándo-vale-la-pena-gpu">Transferencia vs cómputo: ¿cuándo vale la pena GPU?</a>
      <ul>
        <li><a href="#benchmark-cpu-vs-gpu-con-transferencia-incluida">Benchmark CPU vs GPU con transferencia incluida</a></li>
      </ul>
    </li>
    <li><a href="#estrategias-para-minimizar-transferencias">Estrategias para minimizar transferencias</a>
      <ul>
        <li><a href="#1-mantener-datos-en-gpu-tanto-como-sea-posible">1. Mantener datos en GPU tanto como sea posible</a></li>
        <li><a href="#2-batch-processing">2. Batch processing</a></li>
        <li><a href="#3-dataloader-con-pin_memorytrue">3. DataLoader con <code>pin_memory=True</code></a></li>
      </ul>
    </li>
    <li><a href="#práctica-guiada-micro-benchmark-elementwise">Práctica guiada: micro-benchmark elementwise</a></li>
    <li><a href="#pitfalls-comunes">Pitfalls comunes</a>
      <ul>
        <li><a href="#1-olvidar-torchcudasynchronize">1. Olvidar <code>torch.cuda.synchronize()</code></a></li>
        <li><a href="#2-comparar-cpu-single-threaded-vs-gpu">2. Comparar CPU single-threaded vs GPU</a></li>
        <li><a href="#3-no-calentar-gpu-antes-de-benchmark">3. No calentar GPU antes de benchmark</a></li>
      </ul>
    </li>
    <li><a href="#entrega-comparativa-cpu-vs-gpu-en-tu-hardware">Entrega: comparativa CPU vs GPU en tu hardware</a></li>
    <li><a href="#qué-estudiar-para-escribir-este-artículo">Qué estudiar para escribir este artículo</a>
      <ul>
        <li><a href="#fundamentos-necesarios">Fundamentos necesarios</a></li>
        <li><a href="#lecturas-recomendadas">Lecturas recomendadas</a></li>
        <li><a href="#práctica-previa">Práctica previa</a></li>
        <li><a href="#experimentos-previos-necesarios">Experimentos previos necesarios</a></li>
        <li><a href="#conceptos-avanzados-opcional-para-mencionar">Conceptos avanzados (opcional para mencionar)</a></li>
        <li><a href="#tiempo-estimado-de-estudio">Tiempo estimado de estudio</a></li>
        <li><a href="#recursos-de-validación">Recursos de validación</a></li>
      </ul>
    </li>
    <li><a href="#siguiente">Siguiente</a></li>
  </ul>
</nav>
  </div>
</details>
<details class="toc-inside mt-0 overflow-hidden rounded-lg -ms-5 ps-5 lg:hidden">
  <summary
    class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 -ms-5 ps-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Tabla de contenido
  </summary>
  <div
    class="py-2 border-dotted border-neutral-300 border-s-1 -ms-5 ps-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#objetivo">Objetivo</a></li>
    <li><a href="#anatomía-de-un-tensor-en-pytorch">Anatomía de un tensor en PyTorch</a>
      <ul>
        <li><a href="#cpu-vs-gpu-dónde-viven-los-datos">CPU vs GPU: ¿dónde viven los datos?</a></li>
      </ul>
    </li>
    <li><a href="#dtypes-precisión-vs-memoria-vs-velocidad">dtypes: precisión vs memoria vs velocidad</a>
      <ul>
        <li><a href="#tipos-comunes">Tipos comunes</a></li>
        <li><a href="#crear-tensores-con-dtype-específico">Crear tensores con dtype específico</a></li>
        <li><a href="#impacto-en-memoria">Impacto en memoria</a></li>
      </ul>
    </li>
    <li><a href="#transferencia-cpu--gpu-midiendo-overhead">Transferencia CPU ↔ GPU: midiendo overhead</a>
      <ul>
        <li><a href="#benchmark-básico">Benchmark básico</a></li>
      </ul>
    </li>
    <li><a href="#pinned-memory-acelerando-transferencias">Pinned memory: acelerando transferencias</a>
      <ul>
        <li><a href="#qué-es-pinned-memory">¿Qué es pinned memory?</a></li>
        <li><a href="#uso-en-pytorch">Uso en PyTorch</a></li>
      </ul>
    </li>
    <li><a href="#transferencia-vs-cómputo-cuándo-vale-la-pena-gpu">Transferencia vs cómputo: ¿cuándo vale la pena GPU?</a>
      <ul>
        <li><a href="#benchmark-cpu-vs-gpu-con-transferencia-incluida">Benchmark CPU vs GPU con transferencia incluida</a></li>
      </ul>
    </li>
    <li><a href="#estrategias-para-minimizar-transferencias">Estrategias para minimizar transferencias</a>
      <ul>
        <li><a href="#1-mantener-datos-en-gpu-tanto-como-sea-posible">1. Mantener datos en GPU tanto como sea posible</a></li>
        <li><a href="#2-batch-processing">2. Batch processing</a></li>
        <li><a href="#3-dataloader-con-pin_memorytrue">3. DataLoader con <code>pin_memory=True</code></a></li>
      </ul>
    </li>
    <li><a href="#práctica-guiada-micro-benchmark-elementwise">Práctica guiada: micro-benchmark elementwise</a></li>
    <li><a href="#pitfalls-comunes">Pitfalls comunes</a>
      <ul>
        <li><a href="#1-olvidar-torchcudasynchronize">1. Olvidar <code>torch.cuda.synchronize()</code></a></li>
        <li><a href="#2-comparar-cpu-single-threaded-vs-gpu">2. Comparar CPU single-threaded vs GPU</a></li>
        <li><a href="#3-no-calentar-gpu-antes-de-benchmark">3. No calentar GPU antes de benchmark</a></li>
      </ul>
    </li>
    <li><a href="#entrega-comparativa-cpu-vs-gpu-en-tu-hardware">Entrega: comparativa CPU vs GPU en tu hardware</a></li>
    <li><a href="#qué-estudiar-para-escribir-este-artículo">Qué estudiar para escribir este artículo</a>
      <ul>
        <li><a href="#fundamentos-necesarios">Fundamentos necesarios</a></li>
        <li><a href="#lecturas-recomendadas">Lecturas recomendadas</a></li>
        <li><a href="#práctica-previa">Práctica previa</a></li>
        <li><a href="#experimentos-previos-necesarios">Experimentos previos necesarios</a></li>
        <li><a href="#conceptos-avanzados-opcional-para-mencionar">Conceptos avanzados (opcional para mencionar)</a></li>
        <li><a href="#tiempo-estimado-de-estudio">Tiempo estimado de estudio</a></li>
        <li><a href="#recursos-de-validación">Recursos de validación</a></li>
      </ul>
    </li>
    <li><a href="#siguiente">Siguiente</a></li>
  </ul>
</nav>
  </div>
</details>


<script>
  (function () {
    'use strict'

    const SCROLL_OFFSET_RATIO = 0.33
    const TOC_SELECTOR = '#TableOfContents'
    const ANCHOR_SELECTOR = '.anchor'
    const TOC_LINK_SELECTOR = 'a[href^="#"]'
    const NESTED_LIST_SELECTOR = 'li ul'
    const ACTIVE_CLASS = 'active'
    let isJumpingToAnchor = false

    function getActiveAnchorId(anchors, offsetRatio) {
      const threshold = window.scrollY + window.innerHeight * offsetRatio
      const tocLinks = [...document.querySelectorAll('#TableOfContents a[href^="#"]')]
      const tocIds = new Set(tocLinks.map(link => link.getAttribute('href').substring(1)))

      if (isJumpingToAnchor) {
        for (let i = 0; i < anchors.length; i++) {
          const anchor = anchors[i]
          if (!tocIds.has(anchor.id)) continue
          const top = anchor.getBoundingClientRect().top + window.scrollY
          if (Math.abs(window.scrollY - top) < 100) {
            return anchor.id
          }
        }
      }

      for (let i = anchors.length - 1; i >= 0; i--) {
        const top = anchors[i].getBoundingClientRect().top + window.scrollY
        if (top <= threshold && tocIds.has(anchors[i].id)) {
          return anchors[i].id
        }
      }
      return anchors.find(anchor => tocIds.has(anchor.id))?.id || ''
    }

    function updateTOC({ toc, anchors, links, scrollOffset, collapseInactive }) {
      const activeId = getActiveAnchorId(anchors, scrollOffset)
      if (!activeId) return

      links.forEach(link => {
        const isActive = link.getAttribute('href') === `#${activeId}`
        link.classList.toggle(ACTIVE_CLASS, isActive)

        if (collapseInactive) {
          const ul = link.closest('li')?.querySelector('ul')
          if (ul) ul.style.display = isActive ? '' : 'none'
        }
      })

      if (collapseInactive) {
        const activeLink = toc.querySelector(`a[href="#${CSS.escape(activeId)}"]`)
        let el = activeLink
        while (el && el !== toc) {
          if (el.tagName === 'UL') el.style.display = ''
          if (el.tagName === 'LI') el.querySelector('ul')?.style.setProperty('display', '')
          el = el.parentElement
        }
      }
    }

    function initTOC() {
      const toc = document.querySelector(TOC_SELECTOR)
      if (!toc) return

      const collapseInactive = false
      const anchors = [...document.querySelectorAll(ANCHOR_SELECTOR)]
      const links = [...toc.querySelectorAll(TOC_LINK_SELECTOR)]

      if (collapseInactive) {
        toc.querySelectorAll(NESTED_LIST_SELECTOR).forEach(ul => ul.style.display = 'none')
      }

      links.forEach(link => {
        link.addEventListener('click', () => {
          isJumpingToAnchor = true
        })
      })

      const config = {
        toc,
        anchors,
        links,
        scrollOffset: SCROLL_OFFSET_RATIO,
        collapseInactive
      }

      window.addEventListener('scroll', () => updateTOC(config), { passive: true })
      window.addEventListener('hashchange', () => updateTOC(config), { passive: true })

      updateTOC(config)
    }

    document.readyState === 'loading'
      ? document.addEventListener('DOMContentLoaded', initTOC)
      : initTOC()
  })()
</script>


            
          </div>
        </div>
      


      <div class="min-w-0 min-h-0 max-w-fit">
        

        <div class="article-content max-w-prose mb-20">
          
<h2 class="relative group">Objetivo
    <div id="objetivo" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#objetivo" aria-label="Ancla">#</a>
    </span>
    
</h2>
<p>Dominar el <strong>movimiento de tensores entre CPU y GPU</strong> en PyTorch, entender el impacto de dtypes (FP32, FP16, INT8), usar pinned memory para transferencias rápidas, y medir cuándo el overhead de transferencia anula las ganancias de cómputo en GPU.</p>
<hr>

<h2 class="relative group">Anatomía de un tensor en PyTorch
    <div id="anatomía-de-un-tensor-en-pytorch" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#anatom%c3%ada-de-un-tensor-en-pytorch" aria-label="Ancla">#</a>
    </span>
    
</h2>

<h3 class="relative group">CPU vs GPU: ¿dónde viven los datos?
    <div id="cpu-vs-gpu-dónde-viven-los-datos" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#cpu-vs-gpu-d%c3%b3nde-viven-los-datos" aria-label="Ancla">#</a>
    </span>
    
</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Tensor en CPU (por defecto)</span>
</span></span><span style="display:flex;"><span>x_cpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>print(x_cpu<span style="color:#f92672">.</span>device)  <span style="color:#75715e"># cpu</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Tensor en GPU</span>
</span></span><span style="display:flex;"><span>x_gpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">1000</span>, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>print(x_gpu<span style="color:#f92672">.</span>device)  <span style="color:#75715e"># cuda:0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Mover CPU → GPU</span>
</span></span><span style="display:flex;"><span>y_gpu <span style="color:#f92672">=</span> x_cpu<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Mover GPU → CPU</span>
</span></span><span style="display:flex;"><span>z_cpu <span style="color:#f92672">=</span> x_gpu<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cpu&#39;</span>)
</span></span></code></pre></div><p><strong>Regla de oro</strong>: PyTorch <strong>NO</strong> mueve datos automáticamente. Si mezclas tensores de CPU y GPU en una operación, obtienes error:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x_cpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>x_gpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">10</span>, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ❌ RuntimeError: Expected all tensors to be on the same device</span>
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> x_cpu <span style="color:#f92672">+</span> x_gpu
</span></span></code></pre></div><hr>

<h2 class="relative group">dtypes: precisión vs memoria vs velocidad
    <div id="dtypes-precisión-vs-memoria-vs-velocidad" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#dtypes-precisi%c3%b3n-vs-memoria-vs-velocidad" aria-label="Ancla">#</a>
    </span>
    
</h2>

<h3 class="relative group">Tipos comunes
    <div id="tipos-comunes" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#tipos-comunes" aria-label="Ancla">#</a>
    </span>
    
</h3>
<table>
  <thead>
      <tr>
          <th>dtype</th>
          <th>Bits</th>
          <th>Rango</th>
          <th>Uso típico</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>torch.float32</code> (FP32)</td>
          <td>32</td>
          <td>±3.4e38, ~7 dígitos</td>
          <td>Default, máxima precisión</td>
      </tr>
      <tr>
          <td><code>torch.float16</code> (FP16)</td>
          <td>16</td>
          <td>±6.5e4, ~3 dígitos</td>
          <td>Mixed precision, Tensor Cores</td>
      </tr>
      <tr>
          <td><code>torch.bfloat16</code> (BF16)</td>
          <td>16</td>
          <td>±3.4e38, ~3 dígitos</td>
          <td>Training estable, mejor rango que FP16</td>
      </tr>
      <tr>
          <td><code>torch.int8</code></td>
          <td>8</td>
          <td>-128 a 127</td>
          <td>Cuantización, inferencia</td>
      </tr>
  </tbody>
</table>

<h3 class="relative group">Crear tensores con dtype específico
    <div id="crear-tensores-con-dtype-específico" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#crear-tensores-con-dtype-espec%c3%adfico" aria-label="Ancla">#</a>
    </span>
    
</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># FP32 (default)</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># FP16</span>
</span></span><span style="display:flex;"><span>x_fp16 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">1000</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float16, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Convertir dtype</span>
</span></span><span style="display:flex;"><span>x_gpu <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>to(device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float16)
</span></span></code></pre></div>
<h3 class="relative group">Impacto en memoria
    <div id="impacto-en-memoria" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#impacto-en-memoria" aria-label="Ancla">#</a>
    </span>
    
</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">print_memory</span>(tensor, name):
</span></span><span style="display:flex;"><span>    bytes_used <span style="color:#f92672">=</span> tensor<span style="color:#f92672">.</span>element_size() <span style="color:#f92672">*</span> tensor<span style="color:#f92672">.</span>nelement()
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>bytes_used <span style="color:#f92672">/</span> <span style="color:#ae81ff">1e6</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> MB&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x_fp32 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">1000</span>, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>x_fp16 <span style="color:#f92672">=</span> x_fp32<span style="color:#f92672">.</span>half()  <span style="color:#75715e"># conversión a FP16</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print_memory(x_fp32, <span style="color:#e6db74">&#34;FP32&#34;</span>)  <span style="color:#75715e"># 4.00 MB</span>
</span></span><span style="display:flex;"><span>print_memory(x_fp16, <span style="color:#e6db74">&#34;FP16&#34;</span>)  <span style="color:#75715e"># 2.00 MB</span>
</span></span></code></pre></div><p><strong>Lección</strong>: FP16 usa <strong>mitad de memoria</strong> que FP32, crucial para modelos grandes.</p>
<hr>

<h2 class="relative group">Transferencia CPU ↔ GPU: midiendo overhead
    <div id="transferencia-cpu--gpu-midiendo-overhead" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#transferencia-cpu--gpu-midiendo-overhead" aria-label="Ancla">#</a>
    </span>
    
</h2>

<h3 class="relative group">Benchmark básico
    <div id="benchmark-básico" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#benchmark-b%c3%a1sico" aria-label="Ancla">#</a>
    </span>
    
</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">benchmark_transfer</span>(size, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Crear tensor en CPU</span>
</span></span><span style="display:flex;"><span>    x_cpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(size, size, dtype<span style="color:#f92672">=</span>dtype)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Medir CPU → GPU</span>
</span></span><span style="display:flex;"><span>    start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>    x_gpu <span style="color:#f92672">=</span> x_cpu<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>synchronize()  <span style="color:#75715e"># ⚠️ IMPORTANTE: esperar a que termine</span>
</span></span><span style="display:flex;"><span>    transfer_time <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter() <span style="color:#f92672">-</span> start
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Calcular throughput</span>
</span></span><span style="display:flex;"><span>    bytes_transferred <span style="color:#f92672">=</span> x_cpu<span style="color:#f92672">.</span>element_size() <span style="color:#f92672">*</span> x_cpu<span style="color:#f92672">.</span>nelement()
</span></span><span style="display:flex;"><span>    throughput_gb_s <span style="color:#f92672">=</span> (bytes_transferred <span style="color:#f92672">/</span> <span style="color:#ae81ff">1e9</span>) <span style="color:#f92672">/</span> transfer_time
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Size: </span><span style="color:#e6db74">{</span>size<span style="color:#e6db74">}</span><span style="color:#e6db74">x</span><span style="color:#e6db74">{</span>size<span style="color:#e6db74">}</span><span style="color:#e6db74">, dtype: </span><span style="color:#e6db74">{</span>dtype<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Transfer time: </span><span style="color:#e6db74">{</span>transfer_time<span style="color:#f92672">*</span><span style="color:#ae81ff">1000</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ms&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Throughput: </span><span style="color:#e6db74">{</span>throughput_gb_s<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> GB/s&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Data size: </span><span style="color:#e6db74">{</span>bytes_transferred <span style="color:#f92672">/</span> <span style="color:#ae81ff">1e6</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> MB</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x_gpu
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Probar diferentes tamaños</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> size <span style="color:#f92672">in</span> [<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">5000</span>]:
</span></span><span style="display:flex;"><span>    benchmark_transfer(size, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
</span></span></code></pre></div><p><strong>Salida típica (PCIe 3.0 x16)</strong>:</p>
<pre tabindex="0"><code>Size: 100x100, dtype: torch.float32
Transfer time: 0.15 ms
Throughput: 2.67 GB/s
Data size: 0.04 MB

Size: 1000x1000, dtype: torch.float32
Transfer time: 0.51 ms
Throughput: 7.84 GB/s
Data size: 4.00 MB

Size: 5000x5000, dtype: torch.float32
Transfer time: 11.23 ms
Throughput: 8.91 GB/s
Data size: 100.00 MB
</code></pre><p><strong>Observaciones</strong>:</p>
<ul>
<li>Throughput <strong>aumenta</strong> con tamaños grandes (latencia de lanzamiento amortizada)</li>
<li>PCIe 3.0 x16 teórico: ~16 GB/s; real: ~8-12 GB/s</li>
<li>Transferencias pequeñas: <strong>latencia domina</strong> sobre bandwidth</li>
</ul>
<hr>

<h2 class="relative group">Pinned memory: acelerando transferencias
    <div id="pinned-memory-acelerando-transferencias" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#pinned-memory-acelerando-transferencias" aria-label="Ancla">#</a>
    </span>
    
</h2>

<h3 class="relative group">¿Qué es pinned memory?
    <div id="qué-es-pinned-memory" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#qu%c3%a9-es-pinned-memory" aria-label="Ancla">#</a>
    </span>
    
</h3>
<p>Por defecto, memoria CPU es <strong>pageable</strong> (puede moverse a swap). La GPU no puede acceder directamente a memoria pageable, requiere copia intermedia.</p>
<p><strong>Pinned (page-locked) memory</strong>: memoria CPU fija en RAM física, la GPU puede acceder vía DMA (Direct Memory Access).</p>

<h3 class="relative group">Uso en PyTorch
    <div id="uso-en-pytorch" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#uso-en-pytorch" aria-label="Ancla">#</a>
    </span>
    
</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compare_pinned_vs_normal</span>(size<span style="color:#f92672">=</span><span style="color:#ae81ff">5000</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Normal (pageable)</span>
</span></span><span style="display:flex;"><span>    x_normal <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(size, size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Pinned</span>
</span></span><span style="display:flex;"><span>    x_pinned <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(size, size)<span style="color:#f92672">.</span>pin_memory()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Benchmark normal</span>
</span></span><span style="display:flex;"><span>    start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>    x_gpu_normal <span style="color:#f92672">=</span> x_normal<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>synchronize()
</span></span><span style="display:flex;"><span>    time_normal <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter() <span style="color:#f92672">-</span> start
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Benchmark pinned</span>
</span></span><span style="display:flex;"><span>    start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>    x_gpu_pinned <span style="color:#f92672">=</span> x_pinned<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>, non_blocking<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>synchronize()
</span></span><span style="display:flex;"><span>    time_pinned <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter() <span style="color:#f92672">-</span> start
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Normal: </span><span style="color:#e6db74">{</span>time_normal<span style="color:#f92672">*</span><span style="color:#ae81ff">1000</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ms&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Pinned: </span><span style="color:#e6db74">{</span>time_pinned<span style="color:#f92672">*</span><span style="color:#ae81ff">1000</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ms&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Speedup: </span><span style="color:#e6db74">{</span>time_normal<span style="color:#f92672">/</span>time_pinned<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">x&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>compare_pinned_vs_normal()
</span></span></code></pre></div><p><strong>Salida esperada</strong>:</p>
<pre tabindex="0"><code>Normal: 11.45 ms
Pinned: 7.32 ms
Speedup: 1.56x
</code></pre><p><strong>Trade-off</strong>: pinned memory es <strong>recurso limitado</strong> (típicamente &lt;10% de RAM total). Úsala solo para buffers de transferencia frecuente.</p>
<hr>

<h2 class="relative group">Transferencia vs cómputo: ¿cuándo vale la pena GPU?
    <div id="transferencia-vs-cómputo-cuándo-vale-la-pena-gpu" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#transferencia-vs-c%c3%b3mputo-cu%c3%a1ndo-vale-la-pena-gpu" aria-label="Ancla">#</a>
    </span>
    
</h2>

<h3 class="relative group">Benchmark CPU vs GPU con transferencia incluida
    <div id="benchmark-cpu-vs-gpu-con-transferencia-incluida" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#benchmark-cpu-vs-gpu-con-transferencia-incluida" aria-label="Ancla">#</a>
    </span>
    
</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">benchmark_cpu_vs_gpu</span>(size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, include_transfer<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(size, size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># CPU</span>
</span></span><span style="display:flex;"><span>    start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>    y_cpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(x, x)
</span></span><span style="display:flex;"><span>    time_cpu <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter() <span style="color:#f92672">-</span> start
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># GPU (con transferencia)</span>
</span></span><span style="display:flex;"><span>    start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> include_transfer:
</span></span><span style="display:flex;"><span>        x_gpu <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        x_gpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(size, size, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    y_gpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(x_gpu, x_gpu)
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>synchronize()
</span></span><span style="display:flex;"><span>    time_gpu <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter() <span style="color:#f92672">-</span> start
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> include_transfer:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Transferir resultado de vuelta</span>
</span></span><span style="display:flex;"><span>        y_cpu_back <span style="color:#f92672">=</span> y_gpu<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cpu&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Size: </span><span style="color:#e6db74">{</span>size<span style="color:#e6db74">}</span><span style="color:#e6db74">x</span><span style="color:#e6db74">{</span>size<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;CPU time: </span><span style="color:#e6db74">{</span>time_cpu<span style="color:#f92672">*</span><span style="color:#ae81ff">1000</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ms&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;GPU time (con transfer): </span><span style="color:#e6db74">{</span>time_gpu<span style="color:#f92672">*</span><span style="color:#ae81ff">1000</span><span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> ms&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Speedup: </span><span style="color:#e6db74">{</span>time_cpu<span style="color:#f92672">/</span>time_gpu<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">x&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># FLOPS</span>
</span></span><span style="display:flex;"><span>    flops <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> size<span style="color:#f92672">**</span><span style="color:#ae81ff">3</span>  <span style="color:#75715e"># matmul: 2n³ ops</span>
</span></span><span style="display:flex;"><span>    gflops_cpu <span style="color:#f92672">=</span> flops <span style="color:#f92672">/</span> time_cpu <span style="color:#f92672">/</span> <span style="color:#ae81ff">1e9</span>
</span></span><span style="display:flex;"><span>    gflops_gpu <span style="color:#f92672">=</span> flops <span style="color:#f92672">/</span> time_gpu <span style="color:#f92672">/</span> <span style="color:#ae81ff">1e9</span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;CPU: </span><span style="color:#e6db74">{</span>gflops_cpu<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> GFLOPS&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;GPU: </span><span style="color:#e6db74">{</span>gflops_gpu<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> GFLOPS</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Diferentes tamaños</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> size <span style="color:#f92672">in</span> [<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">500</span>, <span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">5000</span>]:
</span></span><span style="display:flex;"><span>    benchmark_cpu_vs_gpu(size, include_transfer<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p><strong>Salida típica (RTX 3080)</strong>:</p>
<pre tabindex="0"><code>Size: 100x100
CPU time: 0.08 ms
GPU time (con transfer): 0.42 ms
Speedup: 0.19x  # ⚠️ CPU gana!

Size: 500x500
CPU time: 2.15 ms
GPU time (con transfer): 1.87 ms
Speedup: 1.15x

Size: 1000x1000
CPU time: 12.34 ms
GPU time (con transfer): 2.56 ms
Speedup: 4.82x

Size: 5000x5000
CPU time: 1834.12 ms
GPU time (con transfer): 78.45 ms
Speedup: 23.38x
</code></pre><p><strong>Lección clave</strong>: para matrices <strong>pequeñas (&lt;500×500)</strong>, overhead de transferencia anula beneficio de GPU.</p>
<hr>

<h2 class="relative group">Estrategias para minimizar transferencias
    <div id="estrategias-para-minimizar-transferencias" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#estrategias-para-minimizar-transferencias" aria-label="Ancla">#</a>
    </span>
    
</h2>

<h3 class="relative group">1. Mantener datos en GPU tanto como sea posible
    <div id="1-mantener-datos-en-gpu-tanto-como-sea-posible" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#1-mantener-datos-en-gpu-tanto-como-sea-posible" aria-label="Ancla">#</a>
    </span>
    
</h3>
<p>❌ <strong>Malo</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">100</span>):
</span></span><span style="display:flex;"><span>    x_cpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>    x_gpu <span style="color:#f92672">=</span> x_cpu<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>    y_gpu <span style="color:#f92672">=</span> some_operation(x_gpu)
</span></span><span style="display:flex;"><span>    y_cpu <span style="color:#f92672">=</span> y_gpu<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cpu&#39;</span>)  <span style="color:#75715e"># Transferencia innecesaria</span>
</span></span><span style="display:flex;"><span>    process(y_cpu)
</span></span></code></pre></div><p>✅ <strong>Bueno</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">100</span>):
</span></span><span style="display:flex;"><span>    x_gpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">1000</span>, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>    y_gpu <span style="color:#f92672">=</span> some_operation(x_gpu)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Solo transferir al final si necesitas en CPU</span>
</span></span><span style="display:flex;"><span>results_cpu <span style="color:#f92672">=</span> [y<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cpu&#39;</span>) <span style="color:#66d9ef">for</span> y <span style="color:#f92672">in</span> all_results_gpu]
</span></span></code></pre></div>
<h3 class="relative group">2. Batch processing
    <div id="2-batch-processing" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#2-batch-processing" aria-label="Ancla">#</a>
    </span>
    
</h3>
<p>❌ <strong>Malo</strong> (transferencia por elemento):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> img <span style="color:#f92672">in</span> images:  <span style="color:#75715e"># 1000 imágenes</span>
</span></span><span style="display:flex;"><span>    img_gpu <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>    result <span style="color:#f92672">=</span> model(img_gpu)
</span></span></code></pre></div><p>✅ <strong>Bueno</strong> (batch):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>batch <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack(images)<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)  <span style="color:#75715e"># Una transferencia</span>
</span></span><span style="display:flex;"><span>results <span style="color:#f92672">=</span> model(batch)
</span></span></code></pre></div>
<h3 class="relative group">3. DataLoader con <code>pin_memory=True</code>
    <div id="3-dataloader-con-pin_memorytrue" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#3-dataloader-con-pin_memorytrue" aria-label="Ancla">#</a>
    </span>
    
</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Para training loops</span>
</span></span><span style="display:flex;"><span>dataloader <span style="color:#f92672">=</span> DataLoader(
</span></span><span style="display:flex;"><span>    dataset,
</span></span><span style="display:flex;"><span>    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>,
</span></span><span style="display:flex;"><span>    pin_memory<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,  <span style="color:#75715e"># Acelera transferencias</span>
</span></span><span style="display:flex;"><span>    num_workers<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> dataloader:
</span></span><span style="display:flex;"><span>    inputs <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>, non_blocking<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># ...</span>
</span></span></code></pre></div><hr>

<h2 class="relative group">Práctica guiada: micro-benchmark elementwise
    <div id="práctica-guiada-micro-benchmark-elementwise" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#pr%c3%a1ctica-guiada-micro-benchmark-elementwise" aria-label="Ancla">#</a>
    </span>
    
</h2>
<p><strong>Objetivo</strong>: medir cuándo GPU supera a CPU en operaciones simples.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">elementwise_benchmark</span>(size, operation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;add&#39;</span>):
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(size)
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># CPU</span>
</span></span><span style="display:flex;"><span>    start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> operation <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;add&#39;</span>:
</span></span><span style="display:flex;"><span>        z_cpu <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> y
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> operation <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;mul&#39;</span>:
</span></span><span style="display:flex;"><span>        z_cpu <span style="color:#f92672">=</span> x <span style="color:#f92672">*</span> y
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> operation <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;exp&#39;</span>:
</span></span><span style="display:flex;"><span>        z_cpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(x)
</span></span><span style="display:flex;"><span>    time_cpu <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter() <span style="color:#f92672">-</span> start
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># GPU (con transferencia)</span>
</span></span><span style="display:flex;"><span>    start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>    x_gpu <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>    y_gpu <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> operation <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;add&#39;</span>:
</span></span><span style="display:flex;"><span>        z_gpu <span style="color:#f92672">=</span> x_gpu <span style="color:#f92672">+</span> y_gpu
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> operation <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;mul&#39;</span>:
</span></span><span style="display:flex;"><span>        z_gpu <span style="color:#f92672">=</span> x_gpu <span style="color:#f92672">*</span> y_gpu
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> operation <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;exp&#39;</span>:
</span></span><span style="display:flex;"><span>        z_gpu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(x_gpu)
</span></span><span style="display:flex;"><span>    torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>synchronize()
</span></span><span style="display:flex;"><span>    time_gpu <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter() <span style="color:#f92672">-</span> start
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>operation<span style="color:#f92672">.</span>upper()<span style="color:#e6db74">}</span><span style="color:#e6db74">, size=</span><span style="color:#e6db74">{</span>size<span style="color:#e6db74">:</span><span style="color:#e6db74">&gt;10</span><span style="color:#e6db74">}</span><span style="color:#e6db74">: CPU </span><span style="color:#e6db74">{</span>time_cpu<span style="color:#f92672">*</span><span style="color:#ae81ff">1e6</span><span style="color:#e6db74">:</span><span style="color:#e6db74">&gt;8.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> μs | GPU </span><span style="color:#e6db74">{</span>time_gpu<span style="color:#f92672">*</span><span style="color:#ae81ff">1e6</span><span style="color:#e6db74">:</span><span style="color:#e6db74">&gt;8.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> μs | Speedup: </span><span style="color:#e6db74">{</span>time_cpu<span style="color:#f92672">/</span>time_gpu<span style="color:#e6db74">:</span><span style="color:#e6db74">&gt;5.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">x&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Probar</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> op <span style="color:#f92672">in</span> [<span style="color:#e6db74">&#39;add&#39;</span>, <span style="color:#e6db74">&#39;mul&#39;</span>, <span style="color:#e6db74">&#39;exp&#39;</span>]:
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">=== </span><span style="color:#e6db74">{</span>op<span style="color:#f92672">.</span>upper()<span style="color:#e6db74">}</span><span style="color:#e6db74"> ===&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> size <span style="color:#f92672">in</span> [<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">1000</span>, <span style="color:#ae81ff">10000</span>, <span style="color:#ae81ff">100000</span>, <span style="color:#ae81ff">1000000</span>]:
</span></span><span style="display:flex;"><span>        elementwise_benchmark(size, op)
</span></span></code></pre></div><p><strong>Ejercicio</strong>: ejecuta este script y responde:</p>
<ol>
<li>¿A partir de qué tamaño GPU supera a CPU en cada operación?</li>
<li>¿Qué operación se beneficia más de GPU?</li>
<li>¿Cambiaría si <strong>no</strong> incluyes transferencias (datos ya en GPU)?</li>
</ol>
<hr>

<h2 class="relative group">Pitfalls comunes
    <div id="pitfalls-comunes" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#pitfalls-comunes" aria-label="Ancla">#</a>
    </span>
    
</h2>

<h3 class="relative group">1. Olvidar <code>torch.cuda.synchronize()</code>
    <div id="1-olvidar-torchcudasynchronize" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#1-olvidar-torchcudasynchronize" aria-label="Ancla">#</a>
    </span>
    
</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># ❌ INCORRECTO: tiempo no incluye ejecución real</span>
</span></span><span style="display:flex;"><span>start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(x_gpu, x_gpu)
</span></span><span style="display:flex;"><span>end <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()  <span style="color:#75715e"># Kernel puede no haber terminado</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ✅ CORRECTO</span>
</span></span><span style="display:flex;"><span>start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(x_gpu, x_gpu)
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>synchronize()  <span style="color:#75715e"># Espera a que termine</span>
</span></span><span style="display:flex;"><span>end <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span></code></pre></div>
<h3 class="relative group">2. Comparar CPU single-threaded vs GPU
    <div id="2-comparar-cpu-single-threaded-vs-gpu" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#2-comparar-cpu-single-threaded-vs-gpu" aria-label="Ancla">#</a>
    </span>
    
</h3>
<p>PyTorch usa MKL/OpenBLAS multi-threaded en CPU. Para benchmark justo:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Limitar threads CPU</span>
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>set_num_threads(<span style="color:#ae81ff">1</span>)
</span></span></code></pre></div>
<h3 class="relative group">3. No calentar GPU antes de benchmark
    <div id="3-no-calentar-gpu-antes-de-benchmark" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#3-no-calentar-gpu-antes-de-benchmark" aria-label="Ancla">#</a>
    </span>
    
</h3>
<p>Primera ejecución incluye tiempo de inicialización:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Warmup</span>
</span></span><span style="display:flex;"><span>_ <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(x_gpu, x_gpu)
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>synchronize()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Ahora sí benchmark</span>
</span></span><span style="display:flex;"><span>start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>perf_counter()
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ...</span>
</span></span></code></pre></div><hr>

<h2 class="relative group">Entrega: comparativa CPU vs GPU en tu hardware
    <div id="entrega-comparativa-cpu-vs-gpu-en-tu-hardware" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#entrega-comparativa-cpu-vs-gpu-en-tu-hardware" aria-label="Ancla">#</a>
    </span>
    
</h2>
<p><strong>Acción</strong>: ejecuta <code>elementwise_benchmark</code> con tus datos y crea una tabla:</p>
<table>
  <thead>
      <tr>
          <th>Operación</th>
          <th>Tamaño mínimo para speedup &gt;2×</th>
          <th>Speedup a 1M elementos</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>ADD</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td>MUL</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td>EXP</td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p><strong>Preguntas</strong>:</p>
<ol>
<li>¿Transferencias dominan en qué porcentaje de casos?</li>
<li>Si tus datos típicos son &lt;1000 elementos, ¿vale la pena GPU?</li>
</ol>
<hr>

<h2 class="relative group">Qué estudiar para escribir este artículo
    <div id="qué-estudiar-para-escribir-este-artículo" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#qu%c3%a9-estudiar-para-escribir-este-art%c3%adculo" aria-label="Ancla">#</a>
    </span>
    
</h2>

<h3 class="relative group">Fundamentos necesarios
    <div id="fundamentos-necesarios" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#fundamentos-necesarios" aria-label="Ancla">#</a>
    </span>
    
</h3>
<ol>
<li>
<p><strong>Modelo de memoria CPU vs GPU</strong></p>
<ul>
<li>Conceptos: memoria unificada vs separada, latencia PCIe, bandwidth</li>
<li>Recursos: CUDA Programming Guide (Cap. 3: Programming Interface)</li>
</ul>
</li>
<li>
<p><strong>Tipos de datos numéricos</strong></p>
<ul>
<li>Conceptos: FP32, FP16, BF16, INT8; precisión vs rango; denormalized numbers</li>
<li>Recursos: IEEE 754 standard (resumen), documentación PyTorch dtypes</li>
</ul>
</li>
<li>
<p><strong>Pinned memory y DMA</strong></p>
<ul>
<li>Conceptos: page-locked memory, Direct Memory Access, copy engines</li>
<li>Recursos: CUDA Best Practices Guide (sección Memory Optimizations)</li>
</ul>
</li>
<li>
<p><strong>Benchmarking correcto en GPU</strong></p>
<ul>
<li>Conceptos: async execution, synchronization, warmup, amortización de latencia</li>
<li>Recursos: PyTorch Performance Tuning Guide</li>
</ul>
</li>
</ol>

<h3 class="relative group">Lecturas recomendadas
    <div id="lecturas-recomendadas" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#lecturas-recomendadas" aria-label="Ancla">#</a>
    </span>
    
</h3>
<ul>
<li><strong>CUDA C Programming Guide</strong>: Capítulo 3 (Memory), Capítulo 5 (Performance)</li>
<li><strong>PyTorch Documentation</strong>:
<ul>
<li><a
  href="https://pytorch.org/docs/stable/tensor_attributes.html"
    target="_blank"
  >Tensor Attributes</a></li>
<li><a
  href="https://pytorch.org/docs/stable/notes/cuda.html"
    target="_blank"
  >CUDA Semantics</a></li>
</ul>
</li>
<li><strong>NVIDIA CUDA Best Practices</strong>: sección 9 (Memory Optimizations)</li>
</ul>

<h3 class="relative group">Práctica previa
    <div id="práctica-previa" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#pr%c3%a1ctica-previa" aria-label="Ancla">#</a>
    </span>
    
</h3>
<ol>
<li>
<p><strong>Benchmarking básico</strong></p>
<ul>
<li>Usar <code>time.perf_counter()</code>, <code>timeit</code>, entender varianza</li>
<li>Conocer warmup y cómo evitar optimizaciones del compilador</li>
</ul>
</li>
<li>
<p><strong>PyTorch fundamentals</strong></p>
<ul>
<li>Crear tensores, operaciones básicas (matmul, elementwise)</li>
<li>DataLoader y transforms</li>
</ul>
</li>
<li>
<p><strong>Análisis de transferencias</strong></p>
<ul>
<li>Calcular throughput (bytes/segundo)</li>
<li>Entender latencia vs bandwidth</li>
</ul>
</li>
</ol>

<h3 class="relative group">Experimentos previos necesarios
    <div id="experimentos-previos-necesarios" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#experimentos-previos-necesarios" aria-label="Ancla">#</a>
    </span>
    
</h3>
<p>Antes de escribir, debes haber:</p>
<ul>
<li>✅ Medido transferencia CPU→GPU en tu hardware con diferentes tamaños</li>
<li>✅ Comparado pinned vs normal memory</li>
<li>✅ Reproducido el &ldquo;break-even point&rdquo; donde GPU supera CPU (con transferencias)</li>
<li>✅ Validado que <code>torch.cuda.synchronize()</code> afecta tiempos medidos</li>
</ul>

<h3 class="relative group">Conceptos avanzados (opcional para mencionar)
    <div id="conceptos-avanzados-opcional-para-mencionar" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#conceptos-avanzados-opcional-para-mencionar" aria-label="Ancla">#</a>
    </span>
    
</h3>
<ul>
<li><strong>Unified Memory (CUDA Managed Memory)</strong>: ventajas/desventajas</li>
<li><strong>GPUDirect</strong>: transferencias GPU↔GPU sin pasar por CPU</li>
<li><strong>Streams</strong>: overlapping de compute y transferencias (tema del post 15)</li>
</ul>

<h3 class="relative group">Tiempo estimado de estudio
    <div id="tiempo-estimado-de-estudio" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#tiempo-estimado-de-estudio" aria-label="Ancla">#</a>
    </span>
    
</h3>
<ul>
<li><strong>Si eres nuevo en PyTorch</strong>: 8-10 horas (tutoriales + experimentar con tensores + benchmarks)</li>
<li><strong>Si usas PyTorch pero no optimizas</strong>: 4-6 horas (profundizar en memoria + pinned + medir overhead)</li>
<li><strong>Si ya optimizas modelos</strong>: 2-3 horas (documentar edge cases + crear ejemplos didácticos)</li>
</ul>

<h3 class="relative group">Recursos de validación
    <div id="recursos-de-validación" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#recursos-de-validaci%c3%b3n" aria-label="Ancla">#</a>
    </span>
    
</h3>
<ul>
<li>Reproducir benchmarks de documentación oficial PyTorch</li>
<li>Comparar tus mediciones con especificaciones de tu GPU (PCIe version, bandwidth teórico)</li>
<li>Validar que speedups observados coinciden con literatura (papers de optimización)</li>
</ul>
<hr>

<h2 class="relative group">Siguiente
    <div id="siguiente" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none">
        <a class="text-primary-300 dark:text-neutral-700 !no-underline" href="#siguiente" aria-label="Ancla">#</a>
    </span>
    
</h2>
<p>En el <strong>próximo artículo</strong> (4/22): escribirás tu primer kernel en <strong>Triton</strong> para suma de vectores. Aprenderás sobre bloques, grids y medirás speedup vs PyTorch nativo.</p>

          
          
          
        </div>
        
        

        

        

      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js"
          integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="
          data-oid="views_posts/hpc-gpu-03-tensores-gpu-pytorch.md"
          data-oid-likes="likes_posts/hpc-gpu-03-tensores-gpu-pytorch.md"></script>
      
    </section>

    
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span class="flex flex-col">
          
            <a
              class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/2025/01/22/hpc-con-gpu-2/22-setup-sin-dolor-drivers-cuda-toolkit-y-verificaci%C3%B3n/">
              <span class="leading-6">
                <span class="inline-block rtl:rotate-180">&larr;</span>&ensp;HPC con GPU (2/22): Setup sin dolor — drivers, CUDA Toolkit y verificación
              </span>
            </a>
            
              <span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="2025-01-22T00:00:00&#43;00:00">22 enero 2025</time>
              </span>
            
          
        </span>
        <span class="flex flex-col items-end">
          
            <a
              class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
              href="/2025/02/23/rese%C3%B1a-del-libro-deep-learning-de-mit-press-essential-knowledge-series/">
              <span class="leading-6">
                Reseña del libro 'Deep Learning' de MIT Press Essential Knowledge Series&ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
              </span>
            </a>
            
              <span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400">
                <time datetime="2025-02-23T00:00:00&#43;00:00">23 febrero 2025</time>
              </span>
            
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        





<div
  id="scroll-to-top"
  class="fixed bottom-24 end-6 z-50 transform translate-y-4 opacity-0 duration-200">
  <a
    href="#the-top"
    class="pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Ir arriba"
    title="Ir arriba">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Desarrollada con <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    
  </div>
  
    <script>
      mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
        margin: 24,
        background: "rgba(0,0,0,0.5)",
        scrollOffset: 0,
      });
    </script>
  
  
  
  <script
    type="text/javascript"
    src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500"
  data-url="http://localhost:1313/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          <span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span>
        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Buscar"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Cerrar (Esc)">
        <span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>
</span>
      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
